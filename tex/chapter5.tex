%# -*- coding: utf-8-unix -*-
%%==================================================
\chapter{计算智能}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[colback=white!50,colframe=orange!50,title=计算智能]
计算智能是信息科学、生命科学、认知科学等不同学科相互交叉的产物. 它主要借鉴仿生学的思想, 基于人们对生物体智能机理的认识, 采用数值计算的方法去模拟和实现人类的智能.
计算智能的主要研究领域包括: 神经计算、进化计算、模糊计算、免疫计算、DNA计算和人工生命等.
\hfill
\end{tcolorbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\centering
\includegraphics[width=0.74\textwidth]{CI2019121801.jpg}
\label{CI2019121801}
\end{figure}
教案二维码下载地址: \qrcode[height=1in]{https://github.com/zggl/AITeachingPlanDraft2020/blob/master/AIMaster2020.pdf},
第五章幻灯片 \qrcode[height=1in]{https://github.com/zggl/AITeachingPlanDraft2020/blob/master/5-\%E4\%BA\%BA\%E5\%B7\%A5\%E6\%99\%BA\%E8\%83\%BD\%E7\%AC\%AC\%E4\%BA\%94\%E7\%AB\%A0\%E7\%AC\%AC\%E4\%B8\%80\%E6\%AC\%A1\%20\%E8\%AE\%A1\%E7\%AE\%97\%E6\%99\%BA\%E8\%83\%BD.pdf}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{概述}
本章主要讨论神经计算、进化计算和模糊计算问题.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{什么是计算智能}
计算智能(Computational Intelligence, CI)目前还没有一个统一的定义, 使用较多的是美国科学家贝慈德克(J. C. Bezdek)从计算智能系统角度给出的定义:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{计算智能}{1}
    如果一个系统仅处理低层的数值数据, 含有模式识别部件, 没有使用人工智能意义上的知识, 且具有计算适应性、计算容错力、接近或高于人的计算速度和近似于人的误差率这4个特性, 则它是计算智能的.
\end{mydef}
\begin{remark}
    从学科范畴看, 计算智能是在神经网络(Neural Networks, NN)、进化计算(Evolutionary Computation, EC)以及模糊系统(Fuzzy System, FS)这3个领域发展相对成熟基础上形成的一个统一的学科概念.
\end{remark}

$\bullet$ 神经网络是一种对人类智能的结构模拟方法, 它是通过对大量人工神经元的广泛并行互联, 构造人工神经网络系统, 去模拟生物神经系统的机理.

$\bullet$ 进化计算是一种对人类智能的演化模拟方法, 它是通过对生物遗传和演化过程的认识, 用进化算法去模拟人类智能的进化规律.

$\bullet$ 模糊计算是一种对人类智能的逻辑模拟方法, 它是利用人类研究和处理模糊现象过程中积累的认知能力, 采用模糊逻辑去模拟人类智能行为的一种模拟方法.

\begin{remark}
从贝慈德克对计算智能的定义和对计算智能学科范畴的分析, 可以看出, 计算智能具有以下2个特点:

$\bullet$ 第一, 计算智能是借鉴仿生学的思想, 基于生物神经系统的结构、进化和认知, 对自然智能进行模拟.

$\bullet$ 第二, 计算智能是一种以模型(计算模型、数学模型)为基础, 以分布和并行计算为特征的自然智能模拟方法.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{计算智能与人工智能的关系}
目前, 计算智能与人工智能的关系存在2种不同观点, 一种观点认为计算智能是人工智能的一个子集, 另一种观点认为计算智能和人工智能是不同的范畴.

第一种观点的代表人物是贝慈德克. 他把智能(Intelligence, I)和神经网络(Neural Network, NN) 分为计算的 (Computational, C)、人工的 (Artificial, A) 和生物的 (Biological, B) 3个层次, 并以模式识别(PR)为例, 给出了图 \ref{AI32fig26000001} 所示的智能的层次结构.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{AI32C42019112601.PNG}
    \vspace{-0.4cm}
    \caption{贝慈德克的智能的3个层次}
    \label{AI32fig26000001}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
在图 \ref{AI32fig26000001} 中, 底层是计算智能 (CI), 它通过数值计算来实现, 其基础是 CNN; 中间层是人工智能 (AI), 它通过人造的符号系统实现, 其基础是 ANN; 顶层是生物智能 (BI), 它通过生物神经系统来实现, 其基础是 BNN.

按照贝慈德克的观点, CNN是指按生物激励模型构造的NN, ANN是指CNN+知识, BNN是指人脑, 即ANN包含了CNN, BNN又包含了ANN. 对智能也一样, 贝慈德克认为AI包含了CI, BI又包含了AI, 即计算智能是人工智能的一个子集.

第二种观点是大多数学者所持有的观点, 其代表人物是艾伯哈特(R. C. Eberhart). 他们认为: 虽然人工智能与计算智能之间有重合, 但计算智能是一个全新的学科领域, 无论是生物智能还是机器智能, 计算智能都是其最核心的部分, 而人工智能则是外层.
事实上, CI和传统的AI只是智能的两个不同层次, 各自都有自身的优势和局限性, 相互之间只应该互补, 而不能取代.
大量实践证明, 只有把AI和CI很好地有机结合, 才能更好地模拟人类智能, 才是智能科学技术发展的正确方向.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{计算智能的产生与发展}
1992年, 贝慈德克在《Approximate Reasoning》杂志上首次 提出了“计算智能”的概念.
%The International Journal of Approximate Reasoning is intended to serve as a forum for the treatment of imprecision and uncertainty in Artificial and Computational Intelligence, covering both the foundations of uncertainty theories, and the design of intelligent systems for scientific and engineering applications. It publishes high-quality research papers describing theoretical developments or innovative applications, as well as review articles on topics of general interest.
%
%Relevant topics include, but are not limited to, probabilistic reasoning and Bayesian networks, imprecise probabilities, random sets, belief functions (Dempster-Shafer theory), possibility theory, fuzzy sets, rough sets, decision theory, non-additive measures and integrals, qualitative reasoning about uncertainty, comparative probability orderings, game-theoretic probability, default reasoning, nonstandard logics, argumentation systems, inconsistency tolerant reasoning, elicitation techniques, philosophical foundations and psychological models of uncertain reasoning.
%
%Domains of application for uncertain reasoning systems include risk analysis and assessment, information retrieval and database design, information fusion, machine learning, data and web mining, computer vision, image and signal processing, intelligent data analysis, statistics, multi-agent systems, etc. \index{IJAR}
1994年6月29到7月1日, IEEE在美国佛罗里达州的奥兰多市召开了首届国际计算智能大会 (简称 WCCI'94). 会议第一次将神经网络、进化计算和模糊系统这三个领域合并在一起, 形成了“计算智能”这个统一的学科范畴.

在此之后, WCCI大会就成了IEEE的一个系列性学术会议, 每4年举办一次. 1998年5月, 在美国阿拉斯加州的安克雷奇市, 又召开了第2届计算智能国际会议 WCCI’98.
2002年5月, 在美国夏威夷州首府火奴鲁鲁市, 又召开了第3届计算智能国际会议WCCI'02.
此外, IEEE还出版了一些与计算智能有关的刊物.
目前, 计算智能的发展得到了国内外众多的学术组织和研究机构的高度重视, 并已成为智能科学技术一个重要的研究领域.

2020年的计算智能会议WCCI'20在英国的格拉斯哥(19-24th July 2020)举办. 会议包含三个分会, 2020神经网络国际联合会议 (IJCNN 2020), 2020 IEEE模糊系统国际大会 (FUZZ-IEEE 2020)和 2020 IEEE 进化计算代表大会 (IEEE CEC2020).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{浅层神经计算}
神经计算(也叫浅层神经网络)是计算智能的重要基础和核心, 浅层神经网络是相对于深度等深层网络的一种提法, 浅层神经网络是对深度网络出现以前的全体网络的一个称谓, 也是计算智能乃至智能科学技术的一个重要研究领域.

神经网络是受生物神经元启发而构建的计算系统. 神经网络由许多独立的单元组成, 每个单元接收来自前一层单元的输入, 并将输出发送到下个一单元(「单元」不一定是单独的物理存在;
它们可以被认为是计算机程序的不同组成部分). 单元的输出通常通过取输入的加权和, 然后再通过某种简单的非线性处理来进行转型, 神经网络的关键特性是基于经验修改单元间的相关权重.

\textcolor[rgb]{0,0,1}{常见误解---StuartRussell}

1) 「\uwave{神经网络是一种新型计算机}」. 在实践中, 几乎所有的神经网络都运行在普通的计算机架构上. 也有一些专用机器, 它们有时会被称作是「神经计算机」, 可以有效地运行神经网络, 但目前为止, 这类机器无法提供足够的计算优势, 值得花费大量时间去开发.

2) 「\uwave{神经网络像大脑一样工作}」. 事实上, 生物神经元的工作方式比神经网络复杂得多, 自然界存在很多种不同的神经元, 神经元的连接可以随时间进行改变, 大脑中也存在其他的机制, 可以影响动物的行为.

本节的主要内容包括: 神经计算基础、人工神经网络的互连结构介绍和神经网络的典型模型.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{神经计算基础}
生物神经系统是人工神经网络的基础. 人工神经网络是对人脑神经系统的简化、抽象和模拟, 具有人脑功能的许多基本特征.
为方便对神经网络进行讨论, 先介绍生物神经元的结构.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{生物神经元的结构}
神经元由细胞体(Soma)、轴突(Axon)和树突(Dendrite)三个主要部分组成 (图 \ref{AI32fig2602}):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{AI32C42019112602.PNG}
\caption{细胞体的生物神经元结构}
\label{AI32fig2602}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
细胞体由细胞核、细胞质和细胞膜等组成, 其直径大约为0.5-100 $\si{\mu m}$大小不等. 细胞体是神经元的主体, 用于处理由树突接受的其它神经元传来的信号, 其内部是细胞核, 外部是细胞膜, 细胞膜的外面是许多向外延伸出的纤维状物质.

轴突是由细胞体向外延伸出的所有纤维中最长的一条分枝, 用来向外传递神经元产生的输出电信号.
\begin{itemize}
\item 每个神经元都有一条轴突, 其最大长度可达1\si{m}\,以上. 在轴突的末端形成了许多很细的分枝, 这些分支叫\textbf{神经末梢}.
\item 每一条神经末梢可以与其它神经元形成功能性接触, 该接触部位称为突触. 所谓功能性接触, 是指非永久性的接触, 这正是神经元之间传递信息的奥秘之处.
\item 树突是指由细胞体向外延伸的所有分支(轴突除外). 树突的长度一般较短, 但数量很多, 它是神经元的输入端, 用于接受从其它神经元的突触传来的信号.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{生物神经元的功能}
根据神经生理学的研究, 生物神经元的主要功能有2个: 神经元的兴奋与抑制, 以及神经元内神经冲动的传导方式.
\begin{itemize}
\item \ding{172} 神经元的抑制与兴奋

\textbf{抑制状态}是指神经元在没有产生冲动时的工作状态.

\textbf{兴奋状态}是指神经元产生冲动时的工作状态.

通常情况下, 神经元膜电位约为-70毫伏, 膜内为负, 膜外为正, 处于抑制状态. 当神经元受到外部刺激时, 其膜电位随之发生变化, 即膜内电位上升、膜外电位下降, 当膜内外的电位差大于阈值电位(约+40毫伏)时, 神经元产生冲动而进入兴奋状态.

\begin{remark}
    神经元每次冲动的持续时间大约1毫秒左右, 在此期间, 即使刺激强度再增加, 也不会引起冲动强度的增加. 神经元每次冲动结束后, 都会重新回到抑制状态. 如果神经元受到的刺激作用不能使细胞膜内外的电位差大于阈值电位, 则神经元不会产生冲动, 将仍处于抑制状态.
\end{remark}

\item \ding{173} 神经元内神经冲动的传导

神经冲动在神经元内的传导是一种电传导过程, 神经冲动沿神经纤维传导的速度在3.2-320\si{km/s}之间, 且其传导速度与纤维的粗细、髓鞘的有无有一定关系. 一般来说, 有髓鞘的纤维的传导速度较快, 而无髓鞘的纤维的传导速度较慢.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{人脑神经系统的联结机制} 简单介绍神经系统的联结规模和神经系统功能分布.

\ding{172} 人脑神经系统的联结规模

人类大脑中由860-1012亿个神经元所组成, 其中每个神经元大约有$3\times 10^4$个突触. 小脑中的每个神经元大约有$10^5$个突触, 并且每个突触都可以与别的神经元的一个树突相连. 人脑神经系统就是由这些巨量的生物神经元经广泛并行互连, 形成了一个高度并行、结构复杂的神经网络系统.

\ding{173} 人脑神经系统的分布功能

人脑神经系统的记忆和处理功能是有机地结合在一起的, 每个神经元既具有存储功能, 同时又具有处理能力. 从结构上看, 人脑神经系统又是一种分布式系统.
人们通过对脑损坏病人所做的神经生理学研究, 没有发现大脑中的哪一部分可以决定其余所有各部分的活动, 也没有发现在大脑中存在有用于驱动和管理整个智能处理过程的任何中央控制部分.
即, 人类大脑的各个部分是协同工作且相互影响的. 在大脑中, 不仅知识的存储是分散的, 而且其控制和决策也是分散的.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{人工神经网络的互连结构}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{人工神经网络}{1}
    人工神经网络是由大量的人工神经元经广泛互联所形成的一种人工网络系统, 用以模拟人类神经系统的结构和功能.
\end{mydef}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{人工神经元的结构}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{生物神经元的结构}
人工神经元是对生物神经元的抽象与模拟, 图 \ref{AI32fig2603} 是一个MP神经元模型, 它由细胞体(Soma)、轴突(Axon)和树突(Dendrite)三个主要部分组成:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.6\textwidth]{AI32C42019112603.PNG}
%\caption{MP神经元模型}
%\label{AI32fig2603}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\draw(-3,2) circle (1.0cm) node[left, xshift=0.49cm] {\footnotesize $\theta$};
\draw[thick,->](-2,2)--(-0.5,2)node[right, xshift=0.24cm] {\footnotesize $y$};

\draw[thick,->](-6,3)node[left, xshift=-0.2cm] {\footnotesize $x_1$}--(-4,2.07)node[above, xshift=-1.4cm,yshift=1.04cm] {\footnotesize $W_1$};
\draw[thick,->](-6,1.98)node[left, xshift=-0.2cm] {\footnotesize $x_2$}--(-4,1.98)node[above, xshift=-1.4cm] {\footnotesize $W_2$};
\node[above, xshift=0.4cm](cdot) at (-6,1.3) {\footnotesize $\vdots$};
\draw[thick,->](-6,0.5)node[left, xshift=-0.2cm] {\footnotesize $x_n$}--(-4,1.92)node[above, xshift=-1.4cm,yshift=-1.55cm] {\footnotesize $W_n$};
\end{tikzpicture}
\caption{MP神经元模型}
\label{AI32fig2603}
\end{center}
\vspace{-0.4cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{常用的人工神经元模型}
根据功能函数的不同, 可得到不同的神经元模型. 常用模型包括以下四种 (图 \ref{AI32fig2604}):
\begin{itemize}
\item 阈值型(Threshold): 这种模型的神经元没有内部状态, 作用函数$f$是一个阶跃函数, 它表示激活值$\sigma$和输出之间的关系.
\item 分段线性强饱和型: 这种模型又称为\textbf{伪线性}, 其输入/输出之间在一定范围内满足线性关系, 一直延续到最大输出值1为止. 但当达到最大值后, 输出就不再增加.
\item S(Sigmoid)型: 是一种连续的神经元模型, 其输入输出特性常用指数、对数或双曲正切等S型函数表示. 它反映的是神经元的饱和特性.

\item 子阈累积型(Subthreshold Summation)是一个非线性函数, 当产生的激活值超过阈值$T$值时, 该神经元被激活, 并产生反响. 在线性范围内, 系统的反响是线性的.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.5\textwidth]{AI32C42019112604.PNG}
%\caption{MP神经元模型}
%\label{AI32fig2604}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\draw[->,color=black] (-5.5,0) -- (-0.2,0);
\draw[->,color=black] (-3.5,-1.05) -- (-3.5,4.03);
\foreach \y in {-1,1,2,3}
\draw[shift={(-3.5,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) ;
% x label and y label O
\node[below,xshift=-0.38cm](origin0) at (-3.5,0) {$O$};
\node[below](axisx) at (-0.3,0) {$x$};
\node[below](axistick11) at (-3.25,2) {$1$};
%fun 1
\draw[thick, -](-5.5,0.0) -- (-3.5,0.0);
\draw[thick, -](-3.5,2.0) -- (-0.25,2.0);
\node[left,xshift=-0.2cm,yshift=0.2cm](axisy) at (-3.5,3.5) {$f(\theta)$};
%% circle
%\draw(-3,2) circle (1.0cm);
%\draw(-3,2) circle (0.05cm) node[left, xshift=0.1cm] {\footnotesize $z_0$};
%\draw[->,color=black, dashed](-3,2)--(-3,1);
%\draw[fill=blue!30](-2.8,2.5) circle (0.05cm)coordinate (z) node[left, xshift=0.1cm] {\footnotesize $z$};
%% right part
%% right 1,2
\draw[->,color=black] (0,0) -- (7.4,0);
\draw[->,color=black] (2.5,-1.05) -- (2.5,4.03);
\foreach \y in {-1,1,2,3}
\draw[shift={(2.5,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) ;
% x label and y label O
\node[below,xshift=-0.38cm](origin0) at (2.5,0) {$O$};
\node[below](axistick12) at (2.25,2) {$1$};
%fun 2
\draw[thick, -](0.0,0.0) -- (2.5,0);
\draw[thick, -](2.5,0) -- (4.5,2);
\draw[thick, -](4.5,2)--(7,2);
\node[below](axisx) at (5.3,0) {$x$};
\node[left,xshift=0.4cm,yshift=0.2cm](axisy) at (2.05,3.5) {$f(\theta)$};
%\draw(3.5,2.4) circle (1.2cm);
%\draw[->,color=black, dashed](3.5,2.4) -- ((3.5,1.2);
%\draw(3.5,2.4)circle (0.05cm)node[right, xshift=0.1cm] {\footnotesize $A$};
%\draw[fill=blue!20](3.2,3.0) circle (0.05cm) coordinate (fz0) node[right, xshift=0.1cm] {\footnotesize $f(z)$};
%\draw[->,color=blue, dashed](z) to [in=120,out=45] (fz0);
%% right 2,1
\draw[->,color=black] (-5.5,-6) -- (-0.2,-6);
\draw[->,color=black] (-3.5,-7) -- (-3.5,-2);
\foreach \y in {-7,-6,-5,-4,-3}
\draw[shift={(-3.5,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) ;
% x label and y label O
\node[below,xshift=-0.38cm](origin0) at (-3.5,-6) {$O$};
\node[below](axisx) at (-0.3,-6) {$x$};
\node[below](axistick11) at (-3.25,-4) {$1$};
%fun 3
\draw[thick, -](-5.5,-6.01) -- (-3.5,-6);
\node[left,xshift=-0.2cm,yshift=0.2cm](axisy) at (-3.5,3.5) {$f(\theta)$};
\draw[scale=1,domain=-5.5:-0.5,smooth,variable=\x,blue] plot ({\x},{-6+2/(1+exp(-3*(\x+3)))});
%\draw[scale=1,domain=0:5,smooth,variable=\x,red] plot ({\x},{1/(1+exp(-3*(\x-1.5)))});
%% circle
%\draw(-3,2) circle (1.0cm);
%\draw(-3,2) circle (0.05cm) node[left, xshift=0.1cm] {\footnotesize $z_0$};
%\draw[->,color=black, dashed](-3,2)--(-3,1);
%\draw[fill=blue!30](-2.8,2.5) circle (0.05cm)coordinate (z) node[left, xshift=0.1cm] {\footnotesize $z$};
%% right part
%% right 2,2
\draw[->,color=black] (0,-6) -- (7.4,-6);
\draw[->,color=black] (2.5,-7.05) -- (2.5,-2.03);
\foreach \y in {-7,-6,-5,-4,-3}
\draw[shift={(2.5,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) ;
% x label and y label O
\node[below,xshift=-0.38cm](origin0) at (2.5,-6) {$O$};
\node[below](axistick12) at (2.25,-4) {$1$};
%fun 4
\draw[thick, -](0.0,-6) -- (4.5,-6);
\draw[thick, -](4.5,-6) -- (5.5,-4);
\draw[thick, -](5.5,-4)--(7,-4);
\draw[dashed,thick, -](2.5,-4)--(5.5,-4);
\node[below](axisx) at (5.3,-6) {$x$};
\node[left,xshift=0.4cm,yshift=0.2cm](axisy) at (2.05,-2.5) {$f(\theta)$};
\end{tikzpicture}
\caption{激活函数}
\label{AI32fig2604}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.6\textwidth]{AI32C42019112605.PNG}
%\caption{MP神经元模型}
%\label{AI32fig2605}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
人工神经网络的互连结构(或称拓扑结构)是指单个神经元之间的连接模式, 它是构造神经网络的基础, 也是神经网络诱发偏差的主要来源. 从互连结构的角度:
仅含输入层和输出层, 且只有输出层的神经元是可计算节点. 网络除拥有输入、输出层外, 还至少含有一个或更多个隐含层的前向网络.
\begin{align}
    {y}_{{j}}={f}\left(\sum_{{i}=1}^{{n}} w_{{ij}} {x}_{{i}}-\theta_{{j}}\right), {j}=1,2, \ldots, {m}.
\end{align}
其中, 输入向量为$X=(x_1,x_2,\cdots,x_n)$; 输出向量为$Y=(y_1,y_2,\cdots,y_m)$; 输入层各个输入到相应神经元的连接权值分别是$w_{ij}, i=1,2,..,n, j=1,2,\cdots, m$.

若假设各神经元的阈值分别是$\theta_j,\, j=1,2,\cdots,m$, 则各神经元的输出$y_j,\, j=1,2, \cdots, m$分别为:
\begin{align}
    {y}_{{j}}={f}\left(\sum_{{i}=1}^{{n}} w_{{ij}} {x}_{{i}}-\theta_{{j}}\right), {j}=1,2, \ldots, {m},
\end{align}
其中, 由所有连接权值$w_{ij}$构成的连接权值矩阵$W$为:
\begin{align}
    W=\left(
  \begin{array}{ccc}
  {w_{11}} & {w_{12}} & {\cdots w_{1 {m}}} \\
  {w_{21}} & {w_{22}} & {\cdots w_{2 {m}}} \\
  {\vdots} & {\vdots} & {\vdots} \\ {w_{{nl}}} & {w_{{n} 2}} & \cdots w_{{nm}}.
  \end{array}
  \right)
\end{align}
在实际应用中, 该矩阵是通过大量的训练示例学习而形成的.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{前馈网络}
指前向网络,包括单层前馈网络和多层前馈网络.

单层前馈网络是指那种只拥有单层计算节点的前向网络. 它仅含有输入层和输出层, 且只有输出层的神经元是可计算节点, 如图 \ref{AI32fig1202105} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[font={\sf \small}]
\def \smbwd{2cm}
\def \smbwe{4cm}
\thispagestyle{empty}
%定义流程图的具体形状
\node (qLPV) at (0,0) [draw, process,minimum width=\smbwd, minimum height=0.5cm] {前馈网络};      % 李刚
\node (qLPVtex1) at (0,-0.7) [minimum width=\smbwd, minimum height=0.5cm] {只包含前向链接};      %
\node (UniSampling1)[draw, process,align=center,right=3cm of qLPV,yshift=1cm] {单层前馈网络};%
\node (UniSampling2)[draw, process,align=center,right=3cm of qLPV,yshift=-1cm] {多层前馈网络};%
\draw[->] (qLPV.east)->(UniSampling1)node[left,yshift=0.25cm,rotate=-20,xshift=-1.05cm] {};
\draw[->] (qLPV.east)->(UniSampling2)node[left,yshift=0.25cm,rotate=-20,xshift=-1.05cm] {};
\end{tikzpicture}
\caption{单层前馈网络结构}
\label{AI32fig1202105}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{多层前馈网络}
多层前馈网络是指那种除拥有输入和输出层以外, 还至少含有一个或更多个隐含层的前馈网络.
\begin{remark}
    隐含层是指由那些既不属于输入层又不属于输出层的神经元所构成的处理层, 也被称为中间层. 隐含层的作用是通过对输入层信号的加权处理, 将其转移成更能被输出层接受的形式.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{多层前馈网络的结构}

多层前馈网络的输入层的输出向量是第一隐含层的输入信号, 而第一隐含层的输出则是第二隐含层的输入信号, 以此类推, 直到输出层.
\begin{remark}
    多层前馈网络的典型代表是BP网络.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{反馈网络}
反馈网络是指允许采用反馈联结方式的神经网络, 如图 \ref{AI32fig1202205}. 所谓反馈联结方式是指一个神经元的输出可以被反馈至同层或前层的神经元.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[font={\sf \small}]
\def \smbwd{2cm}
\def \smbwe{4cm}
\thispagestyle{empty}
%定义流程图的具体形状
\node (qLPV) at (0,0) [draw, process,minimum width=\smbwd, minimum height=0.5cm] {反馈网络};      % 李刚
\node (qLPVtex1) at (0,-0.65) [minimum width=\smbwd, minimum height=0.5cm] {可含有反馈链接};      %
\node (UniSampling1)[draw, process,align=center,right=3cm of qLPV,yshift=1cm] {单层反馈网络};%
\node (UniSampling2)[draw, process,align=center,right=3cm of qLPV,yshift=-1cm] {多层反馈网络};%
\draw[->] (qLPV.east)->(UniSampling1)node[left,yshift=0.25cm,rotate=-20,xshift=-1.05cm] {};
\draw[->] (qLPV.east)->(UniSampling2)node[left,yshift=0.25cm,rotate=-20,xshift=-1.05cm] {};
\end{tikzpicture}
\caption{单层前馈网络结构}
\label{AI32fig1202205}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{反馈网络和前向网络不同}
前向网络属于非循环连接模式, 它的每个神经元的输入都没有包含该神经元先前的输出, 因此不具有“短期记忆”的性质.
反馈网络则不同, 它的每个神经元的输入都有可能包含有该神经元先前输出的反馈信息, 即一个神经元的输出是由该神经元当前的输入和先前的输出这两者来决定的, 这就有点类似于人类的短期记忆的性质.
\begin{remark}
    反馈网络的典型例子是后面将要介绍的Hopfield网络.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{人工神经网络的典型模型}
人工神经网络的模型是指对网络结构、联结权值和学习能力的总括. 常用的网络模型有数十种. 例如:
\begin{itemize}
\item 传统的感知机模型,
\item 具有误差反向传播功能的反向传播网络模型,
\item 采用多变量插值的径向基函数网络模型,
\item 建立在统计学习理论基础上的支持向量机网络模型,
\item 采用反馈联接方式的反馈网络模型,
\item 基于模拟退火算法的随机网络模型.
\item 基于神经网络的连接学习机制放到第\ref{AI32PChapter7}章学习部分讨论.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{感知机(Perceptron)模型}
    感知器是美国学者罗森勃拉特(Rosenblatt)于1957年为研究大脑的存储、学习和认知过程而提出的一类具有自学习能力的神经网络模型, 其拓扑结构是一种分层前向网络.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{单层感知器}
单层感知器是一种只具有单层可调节连接权值神经元的前向网络, 这些神经元构成了单层感知器的输出层, 是感知器的可计算节点.

在单层感知器中, 每个可计算节点都是一个线性阈值神经元. 当输入信息的加权和大于或等于阈值时, 输出为1, 否则输出为0或$-1$.

单层感知器的输出层的每个神经元都只有一个输出, 且该输出仅与本神经元的输入及联接权值有关, 而与其他神经元无关.

若假设各神经元的阈值分别是$\theta_j, j=1,2,\cdots,m$, 则各神经元的输出$y_j, j=1,2,\cdots,m$分别为
\begin{align}
    {y}_{{j}}={f}\left(\sum_{{i}=1}^{{n}} w_{{ij}} {x}_{{i}}-\theta_{{j}}\right), {j}=1,2, \ldots, {m},
\end{align}
其中, 输入向量为$X=(x_1,x_2,\cdots,x_n)$; 输出向量为$Y=(y_1,y_2,\cdots,y_m)$;
输入层各个输入到相应神经元的连接权值分别是$w_{ij},i=1,2,\cdots,n, j=1,2,\cdots, m$. 由所有连接权值$w_{ji}$构成的连接权值矩阵$W$为
\begin{align}
W=\left(
\begin{array}{llll}
{w_{11}} & {w_{12}} & \cdots          &w_{1 {m}} \\
{w_{21}} & {w_{22}} & \cdots         &w_{2 {m}} \\
{\vdots} & {\vdots} &                &\vdots\\
{w_{{n} 1}} & {w_{{n} 2}} & {\cdots} & {w_{{nm}}}.
\end{array}
\right)
\end{align}
在实际应用中, 该权矩阵是通过大量的训练示例学习而形成的.

使用感知器的主要目的是为了对外部输入进行分类. 罗森勃拉特已经证明, 如果外部输入是线性可分的(指存在一个超平面可以将它们分开), 则单层感知器一定能够把它划分为两类. 其判别超平面由如下判别式确定:
\begin{align}
    \sum_{i=1}^{n} w_{i j} x_{i}-\theta_{j}=0,\,j=1,2, \ldots, m.
\end{align}

下面讨论用单个感知器实现逻辑运算的问题. 事实上, 单层感知器可以很好地实现“与”、“或”、“非”运算, 但却不能解决“异或”问题.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    “与”运算($x_1\wedge x_2$).
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.6\textwidth]{AI32C42019112606.PNG}
%\caption{}
%\label{AI32fig2606}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.6\textwidth]{AI32C42019112607.PNG}
%\caption{}
%\label{AI32fig201912012607}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\draw[->,color=black] (-4.1,0) -- (4.1,0);
\foreach \x in {-4,-3,-2,-1,0,1,2,3}
\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt);
%\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
\draw[->,color=black] (-2,-2.25) -- (-2,5.03);
\foreach \y in {-2,-1,1,2,3}
\draw[shift={(-2,\y)},color=black] (2pt,0pt) -- (-2pt,0pt);
%\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
% x label and y label O
\node[below,xshift=-0.3cm](origin0) at (-2,0) {$O$};
\node[below](axisx) at (3.5,0) {$x$};
\node[left,xshift=-0.2cm](axisy) at (-2,3.5) {$y$};

\draw[fill=blue](1,0) circle (0.1cm)node[below] at (1,0) {$(1,0)$};
\draw[fill=blue](-2,0) circle (0.1cm);
\draw[fill=blue](-2,3) circle (0.1cm) node[below] at (-2,3) {$(0,1)$};
\draw[fill=red](1,3) circle (0.1cm) node[below] at (1,3) {$(1,1)$};
\draw[thick,color=black] (2.85,-1.2) -- (-3,4.6);
% vector
%\draw[->,color=black] (0,0)coordinate (B) -- (3,1.5)coordinate (C)node[right] {\footnotesize $z_1$};
%%\coordinate (A) at (3, 0);
%%% \bar z_1
%\draw[->,color=black] (B) -- (3,-1.5)coordinate (C2)node[right] {\footnotesize $\bar z_1$};
\end{tikzpicture}
\caption{分类曲面}\label{AI32fig201912012607}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[H]
\caption{与运算}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
输入&输入	&输出&超平面&	阈值条件\\
\hline
$x_1$&$x_2$&$x_1\wedge x_2$	&$w_1\times x_1+ w_2\times  x_2-\theta=0$  &$\theta= 0$\\
0&	0&	0&$w_1\times 0+ w_2\times 0-\theta <  0$ 	&$\theta> 0$\\
0&	1&0	&$w_1\times0 + w_2\times1-\theta < 0$ 	&$\theta>w_2$\\
1&	0&	0&$w_1\times1+ w_2\times 0 -\theta <0$ 	    &$\theta >w_1$\\
1&	1&	1&$w_1\times0 + w_2\times 1-\theta \geq 0$ 	&$\theta\leq w_1+w_2$\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112900011}
\end{table}

可以证明此表有解, 例如取$w_1=1$, $w_2=1$, $\theta=1.5$, 其分类结果如图 \ref{AI32fig201912012607} 所示,
其中, 输出为1的用实心圆表示, 输出为0的用空心圆表示. 后面约定同此处.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    “非”运算($\neg x_1$).
\end{example}

表 \ref{AI_table2019112902} 有解, 例如取$w_1=-1$, $\theta=-0.5$, 其分类结果如表 \ref{AI_table2019112902} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[H]
\vspace{-0.4cm}
\caption{非运算}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
输入&	输出&	超平面&	阈值条件\\
\hline
$x_1$&	$\neg x_1$&	$w_1\times x_1-\theta=0$        &$\theta =0$\\
0&	1             &	$w_1\times 0 -\theta \geq 0$ 	&$\theta\leq 0$\\
1&	0             &	$w_1\times 1 -\theta <0$ 	    &$\theta >w_1$\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112902}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    “异或”运算($x_1$ {\footnotesize XOR} $x_2$).
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[H]
\vspace{-0.4cm}
\caption{异或运算情况表}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
输入&	输出&	超平面&	阈值条件\\
\hline
$x_1$	&$x_2$	&$x_1$ {\footnotesize XOR} $x_2$	&$w_1\times x_1+ w_2\times x_2-\theta=0$&$\theta=0$\\
0	&0	&0	&$w_1\times 0+ w_2\times 0 -\theta <0 $     &	$\theta >0$\\
0	&1	&1	&$w_1\times 0+ w_2\times 1 -\theta \geq 0$  &	$\theta \leq w_2$\\
1	&0	&1	&$w_1\times 1+ w_2\times 0 -\theta \geq 0$   &	$\theta \leq w_1$\\
1	&1	&0	&$w_1\times 1+ w_2\times 1-\theta <0 $	     &   $\theta > w_1+ w_2$\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112902}
\end{table}
表 \ref{AI_table2019112902} 无解, 即无法找到满足条件的$w_1,w_2$和$\theta$, 如图 \ref{Tu9120100511} 所示. 因为异或问题是一个非线性可分问题, 需要用多层感知器来解决.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.56\textwidth]{Tu9120100511.png}
%\caption{}
%\label{Tu9120100511}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\draw[->,color=black] (-4.1,0) -- (4.1,0);
\foreach \x in {-4,-3,-2,-1,0,1,2,3}
\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt);
%\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
\draw[->,color=black] (-2,-2.25) -- (-2,4.03);
\foreach \y in {-2,-1,1,2,3}
\draw[shift={(-2,\y)},color=black] (2pt,0pt) -- (-2pt,0pt);
%\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
% x label and y label O
\node[below,xshift=-0.3cm](origin0) at (-2,0) {$O$};
\node[below](axisx) at (3.5,0) {$x$};
\node[left,xshift=-0.2cm](axisy) at (-2,3.5) {$y$};

\draw[fill=blue](1,0) circle (0.1cm)node[below] at (1,0) {$(1,0)$};
\draw[fill=red](-2,0) circle (0.1cm);
\draw[fill=blue](-2,3) circle (0.1cm) node[below] at (-2,3) {$(0,1)$};
\draw[fill=red](1,3) circle (0.1cm) node[below] at (1,3) {$(1,1)$};
\draw[dashed,color=black] (2.85,-1.2) -- (-3,4.6);
\draw[dashed,color=black] (-1.5,-0.2) -- (2,3.6);
% vector
%\draw[->,color=black] (0,0)coordinate (B) -- (3,1.5)coordinate (C)node[right] {\footnotesize $z_1$};
%%\coordinate (A) at (3, 0);
%%% \bar z_1
%\draw[->,color=black] (B) -- (3,-1.5)coordinate (C2)node[right] {\footnotesize $\bar z_1$};
\end{tikzpicture}
\caption{分类曲面}\label{Tu9120100511}
\end{center}
\vspace{-0.4cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{多层感知器}
    多层感知器是通过在单层感知器的输入、输出层之间加入一层或多层处理单元所构成的. 其拓扑结构与图 \ref{Tu9120100509} 所示的多层前向网络相似, 差别在于其计算节点的连接权值是可变的.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.56\textwidth]{Tu9120100509.png}
%\caption{}
%\label{Tu9120100509}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]

%第一层
\draw[fill=blue](0,1) circle (0.15cm)node[xshift=-0.1cm] (x1n) at (0,1) {};
\node[xshift=-0.3cm] (t1n) at (x1n) {$x_1$};
\draw[fill=red](0,2.5) circle (0.15cm)node[xshift=-0.1cm] (x12) at (0,2.5) {};
\node[xshift=-0.3cm] (t12) at (x12) {$x_2$};
\node[yshift=-0.5cm,xshift=0.1cm] (t105) at (x12) {$\vdots$};
\draw[fill=blue](0,4) circle (0.15cm) node[xshift=-0.1cm] (x11)at (0,4) {};
\node[xshift=-0.3cm] (t11) at (x11) {$x_n$};

%第二层
\draw[fill=blue](3,3.5) circle (0.15cm)node[xshift=0.1cm](x2n) at (3,3.5) {$\quad\quad\cdots$};
\node[xshift=1.3cm] (y2n) at (x2n) {$y_n$};
\draw[fill=blue](3,1.5) circle (0.15cm) node[xshift=0.1cm](x21) at (3,1.5) {$\quad\quad\cdots$};
\node[xshift=1.3cm] (y21) at (x21) {$y_1$};
\node[yshift=1cm] (t105) at (x21) {$\vdots$};

\draw[->](x11)--(x2n);
\draw[->](x11)--(x21);

\draw[->](x12)--(x2n);
\draw[->](x12)--(x21);

\draw[->](x1n)--(x2n);
\draw[->](x1n)--(x21);

\draw[->](x21)--(y21);
\draw[->](x2n)--(y2n);

\node[yshift=-0.8cm,xshift=0cm] (text01) at (t1n) {输入层};
\node[yshift=-0.8cm,xshift=1.85cm] (text02) at (t1n) {权重$W_{ij}$};
\node[yshift=-0.8cm,xshift=4cm] (text02) at (t1n) {输出层};
\end{tikzpicture}
\caption{多层感知网络}\label{Tu9120100509}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
多层感知器的输入与输出之间是一种高度非线性的映射关系, 如图 \ref{Tu9120100509} 所示的多层前向网络, 若采用多层感知器模型, 则该网络就是一个从$n$维欧氏空间到$m$维欧氏空间的非线性映射.
因此, 多层感知器可以实现对非线性可分问题进行分类.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    对“异或”运算, 用图 \ref{Tu9120100514} 所示的多层感知器即可解决.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.6\textwidth]{Tu9120100514.png}
%\caption{}
%\label{Tu9120100514}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
%第一层
\draw[fill=blue](0,1) circle (0.15cm)node[xshift=-0.1cm] (x1n) at (0,1) {};
\node[xshift=-0.3cm] (t1n) at (x1n) {$x_2$};
\node[yshift=1.5cm,xshift=0.1cm] (t105) at (x1n) {$\vdots$};
\draw[fill=blue](0,4) circle (0.15cm) node[xshift=-0.1cm] (x11)at (0,4) {};
\node[xshift=-0.3cm] (t11) at (x11) {$x_1$};

%第二层
\draw[fill=blue](3,4) circle (0.15cm)node[xshift=0.1cm](x2n) at (3,4) {};
\node[yshift=-0.4cm] (text12) at (x2n) {$x_{11}$};
\node[yshift=0.6cm,xshift=0cm] (text23) at (x2n) {阈值0.5};
\draw[fill=blue](3,1.) circle (0.15cm) node[xshift=0.1cm](x21) at (3,1.0) {};
\node[yshift=1.5cm] (t105) at (x21) {$\vdots$};
\node[yshift=-0.4cm] (text22) at (x21) {$x_{12}$};
\node[yshift=-0.7cm,xshift=0cm] (text23) at (x21) {阈值-0.5};
%第三层
\draw[fill=blue](5,2.5) circle (0.15cm)node[xshift=0.1cm](x30) at (5,2.5) {};
\node[yshift=-0.6cm,xshift=0.3cm] (text23) at (x30) {阈值1.5};
\node[yshift=0.65cm,xshift=-0.6cm] (text23) at (x30) {1};
\node[yshift=-0.65cm,xshift=-0.6cm] (text23) at (x30) {1};

\draw[->](x11)--(x2n);
\node[yshift=0.4cm,xshift=-1.05cm] (text22) at (x2n) {1};
\node[yshift=-0.9cm,xshift=0cm] (text23) at (text22) {1};
\draw[->](x11)--(x21);
\node[yshift=0.6cm,xshift=-1.05cm] (text22) at (x21) {-1};
\node[yshift=-0.4cm,xshift=0cm] (text23) at (text22) {-1};
\draw[->](x1n)--(x2n);

\draw[->](x1n)--(x21);

\draw[->](x21)--(x30);

\draw[->](x2n)--(x30);

\node[xshift=1.3cm] (x30r) at (x30) {$y$};
\draw[->](x30)--(x30r);

\node[yshift=-1.28cm,xshift=0cm] (text01) at (t1n) {输入层};
\node[yshift=-1.28cm,xshift=1.85cm] (text02) at (t1n) {权值};
\node[yshift=-1.28cm,xshift=3.5cm] (text02) at (t1n) {隐层};
\node[yshift=-1.28cm,xshift=4.25cm] (text02) at (t1n) {权值};
\node[yshift=-1.28cm,xshift=6cm] (text02) at (t1n) {输出层};
\end{tikzpicture}
\caption{异或运算的多层感知网络}\label{Tu9120100514}
\end{center}
\vspace{-0.4cm}
\end{figure}
\vspace{-0.4cm}
\end{example}
在图 \ref{Tu9120100514} 中, 隐层神经元$x_{11}$所确定的直线方程为
\begin{align}
    1 \times  x_{1}+1 \times  x_{2}-0.5=0,
\end{align}
它可以识别一个半平面. 隐层神经元$x_{12}$所确定的直线方程为
\begin{align}
    1 \times  x_{1}+1 \times  x_{2}-1.5=0,
\end{align}
它也可以识别一个半平面.

输出层神经元所确定的直线方程为
\begin{align}
    1 \times  x_{11}+1 \times  x_{12}-1.5=0,
\end{align}
它相当于对隐层神经元$x_{11}$和$x_{12}$的输出作“逻辑与”运算, 因此可识别由隐层已识别的两个半平面的交集所构成的一个凸多边形, 如图 \ref{Tu9120100515} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.6\textwidth]{Tu9120100515.png}
%\caption{}
%\label{Tu9120100515}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\draw[->,color=black] (-4.1,0) -- (4.1,0);
\foreach \x in {-4,-3,-2,-1,0,1,2,3}
\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt);
%\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
\draw[->,color=black] (-2,-2.25) -- (-2,4.03);
\foreach \y in {-2,-1,1,2,3}
\draw[shift={(-2,\y)},color=black] (2pt,0pt) -- (-2pt,0pt);
%\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
% x label and y label O
\node[below,xshift=-0.3cm](origin0) at (-2,0) {$O$};
\node[below](axisx) at (3.5,0) {$x$};
\node[left,xshift=-0.2cm](axisy) at (-2,3.5) {$y$};

\draw[fill=blue](1,0) circle (0.1cm)node[below] at (1,0) {$(1,0)$};
\draw[fill=red](-2,0) circle (0.1cm);
\draw[fill=blue](-2,3) circle (0.1cm) node[below] at (-2,3) {$(0,1)$};
\draw[fill=red](1,3) circle (0.1cm) node[below] at (1,3) {$(1,1)$};
\draw[dashed,color=black] (3.85,-1.1) -- (-3,4.5);
\draw[dashed,color=black] (-4,2) -- (2,-2.7);
%\fill[pattern color=blue,pattern=north west lines] (-2.5,1) -- ++(3.5,2) --  ++(-2.5,-0.8)-- ++(-1.5,-2) -- cycle;%
\fill[color=blue,pattern=north west lines,opacity=0.8] (-2.7,1.0)--(-1,3)--(1.3,1.18)--(-0.56,-0.8)-- cycle;
% vector
%\draw[->,color=black] (0,0)coordinate (B) -- (3,1.5)coordinate (C)node[right] {\footnotesize $z_1$};
%%\coordinate (A) at (3, 0);
%%% \bar z_1
%\draw[->,color=black] (B) -- (3,-1.5)coordinate (C2)node[right] {\footnotesize $\bar z_1$};
\end{tikzpicture}
\caption{}\label{Tu9120100515}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{反向传播(BP)模型}
误差反向传播 (Error Back Propagation) 网络, 通常简称为BP (Back Propagation) 网络, 是由美国加州大学的鲁梅尔哈特和麦克莱兰在研究并行分布式信息处理所使用的方法, 在探索人类认知微结构的过程中, 于1985年提出的一种网络模型.
BP网络的网络拓扑结构是多层前向网络, 如图 \ref{Tu9120100516} 所示.
\begin{remark}
    在BP网络中, 同层节点之间不存在相互连接, 层与层之间多采用全互连方式, 且各层的连接权值可调. BP网络实现了明斯基的多层网络的设想, 是当今神经网络模型中使用最广泛的一种结构.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.56\textwidth]{Tu9120100516.png}
%\caption{多层BP网络的结构}
%\label{Tu9120100516}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]

%第一层
\draw[](0,1) circle (0.15cm)node[xshift=-0.1cm] (x1n) at (0,1) {};
\node[xshift=-0.3cm] (t1n) at (x1n) {$x_n$};
\draw[](0,2.5) circle (0.15cm)node[xshift=-0.1cm] (x12) at (0,2.5) {};
\node[xshift=-0.3cm] (t12) at (x12) {$x_2$};
\node[yshift=0.95cm,xshift=0.1cm] (t105) at (x1n) {$\vdots$};
\draw[](0,4) circle (0.15cm) node[xshift=-0.1cm] (x11)at (0,4) {};
\node[xshift=-0.3cm] (t11) at (x11) {$x_1$};

%第二层
\draw[fill=blue](3,4) circle (0.15cm)node[xshift=0.1cm](x2n) at (3,4) {};
\node[yshift=-0.4cm] (text12) at (x2n) {$x_{11}$};
\draw[fill=blue](3,2.5) circle (0.15cm)node[xshift=-0.1cm] (x22) at (3,2.5) {};
\node[yshift=-0.3cm] (t22) at (x22) {$x_{12}$};
\node[yshift=-0.5cm,xshift=0.1cm] (t105) at (t22) {$\vdots$};
\draw[fill=blue](3,1.) circle (0.15cm) node[xshift=0.1cm](x21) at (3,1.0) {};
\node[yshift=1.5cm] (t105) at (x21) {$\vdots$};
\node[yshift=-0.4cm] (text22) at (x21) {$x_{1n}$};
%第三层
\draw[fill=blue](6,4) circle (0.15cm)node[xshift=0.1cm](x3n) at (6,4) {};
\draw[fill=blue](6,1) circle (0.15cm)node[xshift=0.1cm](x31) at (6,1) {};
\draw[fill=blue](6,2.5) circle (0.15cm)node[xshift=0.1cm](x32) at (6,2.5) {};
\node[yshift=-0.5cm,xshift=-0.1cm] (t105) at (x32) {$\vdots$};

\draw[->](x11)--(x2n);
\draw[->](x11)--(x21);
\draw[->](x11)--(x22);

\draw[->](x12)--(x2n);
\draw[->](x12)--(x21);
\draw[->](x12)--(x22);

\draw[->](x1n)--(x2n);
\draw[->](x1n)--(x21);
\draw[->](x1n)--(x22);

\draw[->](x1n)--(x2n);
\draw[->](x1n)--(x21);

\draw[->](x21)--(x3n);
\draw[->](x21)--(x32);
\draw[->](x21)--(x31);

\draw[->](x22)--(x3n);
\draw[->](x22)--(x32);
\draw[->](x22)--(x31);

\draw[->](x2n)--(x3n);
\draw[->](x2n)--(x32);
\draw[->](x2n)--(x31);

\node[xshift=1.3cm] (x3nh1) at (x3n) {$y_1$};
\node[xshift=1.3cm] (x32h1) at (x32) {$y_2$};
\node[yshift=-0.5cm,xshift=-0.1cm] (t105) at (x32h1) {$\vdots$};
\node[xshift=1.3cm] (x3nh2) at (x31) {$y_m$};
\draw[->](x31)--(x3nh2);
\draw[->](x32)--(x32h1);
\draw[->](x3n)--(x3nh1);

\node[yshift=-1.28cm,xshift=0cm] (text01) at (t1n) {输入层};
\node[yshift=-1.28cm,xshift=1.85cm] (text02) at (t1n) {权可调};
\node[yshift=-1.28cm,xshift=3.5cm] (text02) at (t1n) {隐层};
\node[yshift=-1.28cm,xshift=5.25cm] (text02) at (t1n) {权可调};
\node[yshift=-1.28cm,xshift=6.6cm] (text02) at (t1n) {输出层};
\end{tikzpicture}
\caption{多层BP网络的结构}\label{Tu9120100516}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
对BP网络需说明以下两点:
\begin{itemize}
\item 第一, BP网络的每个处理单元均为非线性输入/输出映射关系, 其作用函数通常采用的是可微的Sigmoid函数, 如:
\begin{align}
    f(x)=\frac{1}{1+e^{-x}}.
\end{align}
\item 第二, BP网络的学习过程是由工作信号的正向传播和误差信号的反向传播组成的. 所谓正向传播, 是指输入模式经隐层到输出层, 最后形成输出模式;
\begin{remark}
    所谓误差反向传播, 是指从输出层开始逐层将误差传到输入层, 并修改各层联接权值, 使误差信号为最小的过程.
\end{remark}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{反馈网络(Hopfield)模型}
Hopfield网络是由美国加州工学院物理学家霍普菲尔特1982年提出来的一种单层全互连的对称反馈网络模型. 它可分为离散Hopfield网络和连续Hopfield网络, 限于篇幅, 本书重点讨论离散Hopfield网络.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{离散Hopfield网络的结构}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.6\textwidth]{AI32C42019112608.PNG}
%\caption{离散Hopfield网络}
%\label{AI32fig201912022608}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]

%第一层
\draw[](0,1) circle (0.15cm)node[xshift=-0.1cm] (x1n) at (0,1) {};
\node[xshift=-0.3cm,yshift=0.3cm] (t1n) at (x1n) {$x_n$};
\draw[](0,2.5) circle (0.15cm)node[xshift=-0.1cm] (x12) at (0,2.5) {};
\node[xshift=-0.3cm,yshift=0.3cm] (t12) at (x12) {$x_2$};
\node[yshift=0.95cm,xshift=0.1cm] (t105) at (x1n) {$\vdots$};
\draw[](0,4) circle (0.15cm) node[xshift=-0.1cm] (x11)at (0,4) {};
\node[xshift=-0.3cm,yshift=0.3cm] (t11) at (x11) {$x_1$};

%第二层
\draw[fill=blue](3,4) circle (0.15cm)node[xshift=0.1cm](x2n) at (3,4) {};
\node[yshift=-0.4cm] (text12) at (x2n) {$x_{11}$};
\draw[fill=blue](3,2.5) circle (0.15cm)node[xshift=-0.1cm] (x22) at (3,2.5) {};
\node[yshift=-0.3cm] (t22) at (x22) {$x_{12}$};
\node[yshift=-0.5cm,xshift=0.1cm] (t105) at (t22) {$\vdots$};
\draw[fill=blue](3,1.) circle (0.15cm) node[xshift=0.1cm](x21) at (3,1.0) {};
\node[yshift=1.5cm] (t105) at (x21) {$\vdots$};
\node[yshift=-0.4cm] (text22) at (x21) {$x_{1n}$};
%第三层
\node[xshift=2.4cm] (text31) at (x21) {$y_{m}$};
\node[xshift=2.4cm] (text32) at (x22) {$y_{2}$};
\node[xshift=2.4cm] (text3n) at (x2n) {$y_{1}$};

% 返回线

\node[xshift=1.4cm,yshift=0.15cm] (text01) at (x21) {};
\node[xshift=1.4cm,yshift=-0.15cm] (text02) at (x22) {};
\node[xshift=1.4cm,yshift=-0.15cm] (text0n) at (x2n) {};

\draw[->](text01)--++(0,-1)--++(-6,0)--++(0,0.9)--++(1.34,0); %y_n
\draw[->](text01)--++(0,-1)--++(-6,0)--++(0,2.3)--++(1.34,0);

\draw[->](text0n)--++(0,1)--++(-6.6,0)--++(0,-2.385)--++(1.95,0); %y_1
\draw[->](text0n)--++(0,1)--++(-6.6,0)--++(0,-3.81)--++(1.95,0); %y_1

\draw[blue,->](text02)--++(0,2)--++(-6,0)--++(0,-0.4)--++(1.55,0); %y_2
\draw[blue,->](text02)--++(0,2)--++(-6,0)--++(0,-3.3)--++(1.55,0); %y_2

\draw[->](x21)--(text31);
\draw[blue,->](x22)--(text32);
\draw[->](x2n)--(text3n);

\draw[->](x11)--(x2n);
\draw[->](x11)--(x21);
\draw[->](x11)--(x22);

\draw[->](x12)--(x2n);
\draw[->](x12)--(x21);
\draw[->](x12)--(x22);

\draw[->](x1n)--(x2n);
\draw[->](x1n)--(x21);
\draw[->](x1n)--(x22);

\draw[->](x1n)--(x2n);
\draw[->](x1n)--(x21);

\node[yshift=-2.28cm,xshift=0cm] (text01) at (t1n) {输入层};
\node[yshift=-2.28cm,xshift=3cm] (text02) at (t1n) {输出层};
\end{tikzpicture}
\caption{离散Hopfield网络}
\label{AI32fig201912022608}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
离散Hopfield网络是在非线性动力学的基础上, 由若干基本神经元构成的一种单层全互连网络, 其任意神经元之间均有连接, 并且是一种对称连接结构.

\begin{example}
一个典型的离散 Hopfidld网络结构如图 \ref{AI32fig201912022608}所示.
离散Hopfield网络模型是一个离散时间系统, 每个神经元只有0和1 (或-1和1) 两种状态, 任意神经元$i$和$j$之间的连接权值为$w_{ij}$.
由于神经元之间为对称连接, 且神经元自身无连接, 因此有
\begin{align}
  w_{i j}=\left\{
  \begin{array}{ll}
  {w_{j i}}, & i \neq j \\
    {0}, &  i=j
    \end{array}\right.
\end{align}
\end{example}
由该连接权值所构成的连接矩阵是一个零对角的对称矩阵. 在 Hopfidld网络中, 虽然神经元自身无连接, 但由于每个神经元都与其他神经元相连, 即每个神经元的输出都将通过突触连接权值传递给别的神经元, 同时每个神经元又都接受其他神经元传来的信息, 这样对每个神经元来说, 其输出经过其他神经元映射后又有可能反馈给自己, 因此Hopfield网络是一种反馈神经网络.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{神经网络的泛逼近性——可视化证明}
Neural Networks: The Universality Theorem, V 1.0.0 (655 KB) 作者： Mayank Jhamtani-Visual Proof of Universal Approximation Theorem for Neural Networks.

《Neural Networks\_The Universality Theorem》. The original proof for this principle was given by George Cybenko in 1989, and it used concepts from a branch of abstract math called functional analysis.
Michael Nielson gave a visual proof in his freely available online book \href{http://neuralnetworksanddeeplearning.com/chap4.html}{Nueral Nets and Deep Learning}.

示例代码
%%%%%%%%%%%%%%%%%%%%%%%%%% matlab 代码  %%%%%%%%%%%%%%%%%%%%%%%%%%%
\lstinputlisting[language=Matlab,caption={FNN逼近示例.}]{code/FNN3x5x125x1x3Isinxtest.m}
%%%%%%%%%%%%%%%%%%%%%%%%%%% matlab 代码  %%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{其它神经学习算法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
最优剪枝极值学习机(OP-ELM)如图 \ref{OPELM20161021fig2} 所示, 图 \ref{OPELM20161021fig2} 显示了OP-ELM的三个步骤 (\href{http://www.cis.hut.fi/projects/eiml/research/download}{OP-ELM工具箱下载链接}).
如\cite{Miche2008OP, Miche2008A, Miche2010OP}中的描述, 对于OP-ELM， 当给网络添加一个纯随机噪声变量 (添加的噪声变量没有在图上显示) 时, 拟合会变得松散和扩散(ELM的并没有针对这种情况进行专门的设计).
针对这一问题, OP-ELM提出了一种三步骤解决方法, 简言之, 为了解决含噪声的问题, 给ELM添加一个含修剪策略的封装器, 之后再增加两个步骤.
首先是通过最小角度回归对神经元进行排序(LARS{efronhaste2004-32498});
实际上, 实现了LARS算法的MRSR算法\cite{SimiläTikka2005-32497}也适用于这类多输出情况, 根据输出值的大小对节点进行排序.
然后, 用LOO的准则来确定在OP-ELM模型结构中需要保留的神经元 (其实现来自LASSO方法, 这里将不对\textbf{LARS 算法}做详细的叙述).
由于OP-ELM算法设计中的神经元和输出之间的关系是线性的, OP-ELM对隐藏神经元的排序方法提供了支持.

LOO优化参数方法通常是一种代价高昂的方法, 它需要在整个数据集(只除去一个样本)上训练模型, 并对数据集的所有样本重复求值.
在OP-ELM结构中, 隐藏层和输出层的结点是线性关系.
LOO误差有一个封闭的矩阵形式, 由Allen的预测平方和 (PreSS) 给出 (关于PreSS-LOO误差的计算细节见后).
通过使用LARS方法 (这里是对回归问题的$L_1$约束), 这种闭合形式允许快速计算均方误差, 从而计算输出权重$\bm b$, 使OP-ELM计算仍然有效,
并且比原始ELM对不相关/相关变量的鲁棒性更强, 因此, OP-ELM可以被看作是一个“正则化”的ELM.
同时, 回归问题使用 $L_1$ 和$L_2$ (也包括$L_1$ 和 $L_2$的联合组合形式) 构造惩罚项.
同时, 由于在PreSS公式中执行的矩阵运算的性质(这些计算见第4节), 决定保留的神经元的最终数量 (通过LOO 准则索引) 显示出潜在的数值不稳定性.
解决方案是在PreSS公式的计算中使用正则化.

下面将回顾用于执行正则化的最著名算法, 回归问题使用 $L_1$ 和$L_2$ (也包括$L_1$ 和 $L_2$的联合形式)惩罚项.
提出的方法结合了OP-ELM中$L_1$和$L_2$的约束, 使网络正规化.
\begin{figure}[t]
    \begin{center}
    \includegraphics[scale=0.7]{OPELM20161021.pdf}
    \end{center}
    \caption{OP-ELM的三部分框架结构: SLFN使用了ELM的随机初始化权重的方法 ;隐层节点排序使用了 LARS算法那; 最优隐节点的计算使用了含Leave-One-Out准则的OP-ELM模型.}
    \label{OPELM20161021fig2}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tikhonov 正则化}\index{Tikhonov Regularization}
Tikhonov正则化以Andrey Tikhonov的名字命名, 是不适定问题最常用的正则化方法之一.
在统计学上, 这种方法被称为岭回归, 有多个独立存在的版本, 它也被称为\textbf{Tikhonov–Miller 方法}、\textbf{Phillips–Twomey 方法}, 约束线性反演方法和线性正则化索引方法. \index{岭回归}
它与针对非线性回归最小二乘问题的Levenberg-Marquardt算法有关.
假设对于已知的矩阵$\bm A$和向量$\bm b$, 我们希望找到向量$\bm{x}$, 使得下式成立
\begin{align}
    \bm A\bm {x} =\bm {b}.
\end{align}
上述问题的标准解方法是最小二乘线性回归. 然而, 如果没有$\bm x$满足等式, 或者有多个$\displaystyle x$满足等式, 则解决方案不是唯一的, 该问题称为不适定问题.
在这种情况下,普通最小二乘估计会导致方程组的超定(过拟合), 或者更常见的是欠定(欠拟合).
大多数现实问题都具有前向的低通滤波器的效果, 其中$A$将$\bm{x}$映射到$\bm{b}$.
因此, 在求解逆问题时,逆映射用作高通滤波器, 具有放大噪声的不良倾向(特征值/奇异值在逆映射中最大, 而在正向映射中最小).
此外, 普通的最小二乘法隐式地将重构版本$\bm{x}$的每个元素(在$A$的空空间中)置空, 而不允许将模型用作$\bm{x}$的前置.
最小二乘法寻求最小化残差平方和, 其紧凑格式可写为
\begin{align}
    \|\bm A\bm {x} -\bm {b} \|^{2}.
\end{align}
其中$\left\|\cdot\right\|$是欧几里德范数. 为了得到具有期望性质的特定解, 对于适定的Tikhonov矩阵$\Gamma$, 可在最小化问题中包括一个正则化项：
\begin{align}
    \|\bm A\bm {x} -\bm {b} \|^{2}+\|\Gamma \bm {x} \|^{2},
\end{align}
许多情况下, 该矩阵被选为恒等矩阵$(\Gamma=\alpha I)$, 优先选择具有较小范数的解; 这是熟知的$L_2$正则化方法 \cite{Ng2004-32636}.
在其他情况下, 则可以使用\textbf{低通滤波算子}\index{lowpass operators}(例如, 差分运算符或\textbf{加权傅里叶算子})来强制平滑.
这种正则化改进了问题的解, 从而使直接求得数值解成为可能. 由$\hat{x}$表示的显式解由以下公式给出：
\begin{align}
    {\hat {x}}=(\bm A^{T}\bm A+\Gamma ^{T}\Gamma )^{-1}\bm A^{T}\bm {b}.
\end{align}
正则化的效果可以通过调整矩阵$\Gamma$的尺度，使之变化. 对于$\Gamma=0$, 如果存在$(A^TA)^{-1}$, 则问题退化为未正则化的最小二乘解.
除了线性回归之外, $L_2$正则化还用于许多上下文中, 例如使用logistic回归或支持向量机分类\cite{Fan2008LIBLINEAR}以及矩阵因式分解\cite{Guan2012Online}.
\begin{remark}
Tikhonov正则化是在许多不同的工作中独立发现的. 在Andrey Tikhonov和David L. Phillips的工作中, 它在积分方程中的应用广为人知.
有些作者使用术语“Tikhonov-Phillips正则化”. Arthur E. Hoerl和 Manus Foster 采用统计方法, 阐述了有限维情况, 并将此方法解释为一个 \textbf{Wiener-Kolmogorov(Kriging)滤波器}.
在Hoerl之后, 它在统计文献中被称为岭回归 (\textbf{Generalized Tikhonov regularization}).\index{岭回归}
\end{remark}

对于$\bm x$的一般多元正态分布和数据误差,可以应用变量转换来减少上述情况. 同样地, 可以寻找$\bm x$以最小化
\begin{align}
    \|\bm A\bm x-\bm b\|_{P}^{2}+\|\bm x-\bm x_{0}\|_{Q}^{2}\,
\end{align}
其中, 我们使用$\left\|\bm x\right\|_{\bm Q}^{2}$表示加权范数$\bm x^{T}\bm Q\bm x$ (与Mahalanobis距离比较).

\begin{remark}
贝叶斯方法将$\bm P$解释为$\bm b$的逆协方差矩阵, $\bm x_0$是$x$的期望值, 而$\bm Q$是$\bm x$的逆协方差矩阵.
然后, 将Tikhonov矩阵作为矩阵$\displaystyle\bm Q=\Gamma^{T}\Gamma$(例如\textbf{Cholesky 分解})的因子分解给出, 并将其视为一个\textbf{whitening 滤波器}.
\end{remark}

上述广义问题有一个最优解$x^{*}$, 可以用公式显式地求解
\begin{align}
    \bm x^{*}=(\bm A^{T}\bm P\bm A+\bm Q)^{-1}(\bm A^{T}\bm P\bm b+\bm Q\bm x_{0}).
\end{align}
或等价地写为
\begin{align}
    \bm x^{*}=\bm x_{0}+(\bm A^{T}\bm P\bm A+\bm Q)^{-1}(\bm A^{T}\bm P(\bm b-\bm A \bm x_{0})).
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{TROP-ELM算法}\index{TROP-ELM}
\textbf{TROP-ELM}: 基于LARS和Tikhonov正则化的双正则化ELM \cite{MichevanHeeswijk2011-30641}, 2011.
图 \ref{TROPEMLmodel20161021fig2} 显示了提出的正则化OP-ELM(TROP-ELM).
\begin{figure}[H]
    \begin{center}
    \includegraphics[scale=0.7]{TROPEMLmodel20161021.pdf}
    \end{center}
    \vspace{-0.2cm}
    \caption{图 \ref{OPELM20161021fig2} 的改进: TROP-ELM.}
    \label{TROPEMLmodel20161021fig2}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Allen's PreSS(prediction sum of squares)}
OP-ELM中使用的原始PreSS公式是由Allen在 \cite{MADavid1972} 中提出. 原始的PreSS公式可以表示为
\begin{align}\label{AIC5eqn519}
  \textup{MSE}^{\textup{TR-PreSS}}=\frac 1 n \sum_{i=1}^n \left( \frac{y_i-\bm x_i(\bm X^T \bm X)^{-1}\bm x^T y}
       {1-\bm x_i(\bm X^T \bm X)^{-1}\bm x_i^T}\right)^2,
\end{align}
从公式可以看出, 每个观测值都是用其他$n-1$观测值“预测”的, 残差最终被平方和相加. 算法通过矩阵运算, 有效地实现了该算法.

\textbf{算法1}

1: 计算矩阵的逆$\bm C= (\bm X^T X)^{-1}$;

2: 计算$\bm P = \bm X\bm C$;

3: 计算伪逆$\bm w = \bm C\bm X^T\bm y$;

4: 计算PRES的分子$D = \bm 1-\textup{diag}(\bm P\bm X^T)$;

5: 计算PreSS的误差$e = \frac{y-\bm X \bm w} {\bm D} $;

6: 优化误差$\textup{MSE}^{\textup{TR-PreSS}}=\frac 1 n \sum_{i=1}^n \epsilon_i^2$.

这种方法的主要缺点在于在计算中使用伪逆(在\textbf{Moore–Penrose}中), 如果数据集$\bm X$不是全秩, 则可能导致数值不稳定性. 不幸的是, 现实世界中的数据集经常出现这种情况. 下面的方法对原始的计算提出了两个改进方式: 正则化和快速矩阵计算.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Tikhonov正则化的PreSS (TR-PreSS)}
Golub等人\cite{GolubHeath1979-10292} 指出: 使用奇异值分解 (SVD) 方法计算PreSS的统计结果比传统的伪逆方法更好.
在同一篇论文中, 给出了提出了Allen's PreSS的一个推广, 即广义交叉验证 (GCV) 方法, 它在技术上优于原始的方法, 因为它可以处理数据定义非常糟糕的情况, 如果除对角线项外,所有的项都是0.

实际上, 从我们的实验来看, 虽然GCV具有无可替代的优越性, 和原始的PreSS和Tikhonov正则化的PreSS相比, 它会导致计算时间陡增. 算法1以矩阵形式给出了用于确定$\textup{MSE}^{\textup{TR-PreSS}}$的计算步骤
\begin{align}
    \textup{MSE}^{\textup{TR-PreSS}}=\frac 1 n \sum_{i=1}^n \left( \frac{y_i-\bm x_i(\bm X^T \bm X+\lambda \bm I)^{-1}\bm x^T y  }
       {1-\bm x_i(\bm X^T \bm X+\lambda \bm I)^{-1}\bm x_i^T}\right)^2,
\end{align}
这是公式 \eqref{AIC5eqn519} 的正规化结果. 符号$A\circ B$表示矩阵$\bm A$和$\bm B$ (Schur积) 逐元素相乘. 算法2在步骤4中使用它, 因为它比标准矩阵乘积快.

\textbf{算法2}

用SVD分解$\bm X$: $\bm X = \bm U\bm S\bm V^T$;

计算后面将要用到的乘积: $A = \bm X\bm V$ and $B = \bm U^T\bm y$;

重复如下步骤:

\quad 1) 对$\bm X$用SVD分解, 按如下方式计算矩阵$\bm C$:

\quad 2) 计算 $\bm P$: $\bm P = \bm C\bm B$;

\quad 3) 计算 $\bm D$: $\bm D = \bm 1-\textup{diag}(\bm C\bm U^T)$;

\quad 4) 计算 $e = \frac{y-\bm P} {\bm D}$和$\textup{MSE}^{\textup{TR-PreSS}}=\frac 1 n \sum_{i=1}^n \epsilon_i^2$.

\quad 5) 直到$\lambda$收敛.

\quad 6) 关联$\textup{MSE}^{\textup{TR-PreSS}}$ 和$\lambda$ 的最优值.

全局来说, 该算法使用$\bm X$的奇异值分解来避免计算问题,并在奇异值分解计算伪逆时引入Tikhonov正则化参数.
由于在优化$\lambda$之前预先计算了效用矩阵($\bm A$, $\bm B$和$\bm C$), 所以此实现碰巧运行得非常快.
在实际应用中,该算法中$\lambda$的优化是通过一种\textbf{Nelder–Mead最小化方法}来实现的, 这种方法恰好在这个问题上收敛得非常快(Matlab中的fminsearch函数).
通过使用这个修改后的PreSS, OP-ELM对回归权重(隐藏层和输出层之间的回归)有$L_2$的惩罚, 对于这些权重, 神经元已经使用$L_1$的惩罚进行了排序.
图 \ref{TROPEMLmodel20161021fig2} 是图 \ref{OPELM20161021fig2} 的修改版本 (图 \ref{OPELM20161021fig2} 是 \textbf{OP-ELM} 方法).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{区间二型RVFL}
现在我们使用新的学习范式来训练RVFL网络.
在这个新的训练集中, $\bm x_i\in X$是原始功能, 一般来说, 特权功能$\bm x_i\in \widetilde x$属于特权功能空间$\widetilde x$, 它不同于原始功能空间$x$.

RVFL的模型结构如此简单, 为什么RVFL能很好地完成大多数任务?

Giryes等人 \cite{Giryes2016-7439822} 对这个开放问题给出一个可能的理论解释.
一般来说, 不同类别之间的样本角度大于同一类别内的样本角度 \cite{LiorWolf2003}.
因此, 训练在学习算法中的作用是“不同类别的点之间的角度比同一类别的点之间的角度受到的惩罚更大” \cite{Giryes2016-7439822}.
而且, 在这个大数据时代, 由于模型结构高度复杂, 学习过程中很难对所有参数进行快速调整, 这就需要极高的计算成本.
为了解决这个问题, 随机化是一些学习算法的理想选择, 从而降低了计算成本. 训练前的算法需要随机初始化  \cite{Glorot2015, Zhang2015}.
现在我们再次访问RVFL. RVFL使用混合策略来训练整个网络.
在RVFL中, 输入层和增强节点之间的随机初始参数处理具有可分辨角度的良好输入样本, 旋转输出权重进一步处理剩余数据.
RVFL网络是一种经典的单层前向神经网络, 其结构如图 \ref{RVFLNN181102Sys} 所示. RVFL随机初始化输入层和增强节点之间的所有权重和偏差, 然后这些参数在训练阶段不需要调整.
图 \ref{RVFLNN181102Sys} 中红色实线代表的输出权重可以通过Moore-Penrose伪逆 (\cite{paopillips1995-6471,IgelnikPao1995-6470}) 或岭回归 (\cite{Bishop2012-6469}) 来计算.
此外, 输入层和输出层之间的直接连接是防止RVFL网络过度拟合的一种有效而简单的正则化技术.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\centering
  \includegraphics[width=0.85\textwidth]{RVFLStruc.pdf}
  \vspace{-0.4cm}
  \caption{RVFL的网络结构.}
  \label{RVFLNN181102Sys}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%

RVFL+是一种用于所有二元、多类别分类和回归问题的统一学习算法, 输出函数可以直接应用于所有三种情况.
\begin{enumerate}
    \item 二元分类: 测试样品的预测标签由
\begin{align}
    \hat y = \textup{sign}(f_{\textup{test}}(z)).
\end{align}
    \item 多类分类: 我们采用一对多(One vs. All)策略来确定多类分类中的预测标签. 设$f{\textup{test}}^k(z)$为第$k$个输出节点的输出函数. 测试样品的预期标签由
\begin{align}
    \hat y = \arg \max_{ k \in 1,\cdots,m} f_{\textup{test}}^k (z).
\end{align}
  \item  回归: 预测值等于RVFL的输出函数$f_{\textup{test}}(z)$
\begin{align}
    \hat y = f_{test}(z).
\end{align}
\end{enumerate}

按照 \cite{VAPNIK2009544} 中的SVM+公式, 我们可以将RVFL+写成
\begin{align}
    &\min\limits_{{\bm w},\tilde {\bm w}} \frac 1 2 \|{\bm w}\|_2^ 2 + \frac \gamma 2 \|\tilde {\bm w}\|_2^ 2 + C\sum_{i=1}^N \zeta_i (\tilde {\bm w}, {\tilde h}({\tilde {\bm x}}_i)),\notag\\
    &s.t.\,\, \bm h(\tilde x_i){\bm w} = \bm y_i - \zeta_i (\tilde {\bm w}; {\tilde h}({\tilde {\bm x}}_i)), \forall \leq i \leq  N.
    \label{RVFL20181102eq8}
\end{align}
其中$\gamma$是正则化系数. 与$\bm h(\bm x_i)$类似, ${\tilde{\bm h}}({\tilde{\bm x_r}}i)$也是与特权功能${\tilde{\bm x}_i}$相对应的增强层输出向量, 可以用相同的方式计算.
$\zeta_i(\tilde{\bm w},{\tilde h}(\tilde{\bm x}_i))$是特权功能空间中的更正函数(或可宽延函数), 而$\tilde{\bm w}$是更正函数的输出权重向量.
\begin{align}\label{RVFL20181102eq9}
    \zeta_i (\tilde {\bm w}, {\tilde h}({\tilde {\bm x}}_i)) = {\tilde h}({\tilde {\bm x}}_i){\bm w}.
\end{align}

将 \eqref{RVFL20181102eq9} 代入 \eqref{RVFL20181102eq8}, 得到RVFL+表达形式 \cite{Saunders1998}.
\begin{align}
    &\min\limits_{{\bm w},\tilde {\bm w}} \frac 1 2\|{\bm w}\|_2^ 2 + \frac \gamma 2 \|\tilde {\bm w}\|_2^ 2 + C\sum_{i=1}^N  {\tilde {\bm h}}({\bm x}_i)\tilde {\bm w},\notag\\
    &s.t.\,\,\bm h(\tilde x_i){\bm w} =\bm y_i - {\tilde {\bm h}}({\bm x}_i)\tilde {\bm w}, \forall \leq i \leq  N.
    \label{RVFL20181102eq10}
\end{align}
从 \eqref{RVFL20181102eq10}, 我们注意到RVFL+最小化了$w$和$\tilde w$上的目标函数. 因此, 训练阶段RVFL+的分离超平面不仅取决于原始特征, 还取决于优先考虑的信息.
此外, SVM \cite{Cortes1995Support}与SVM+的模型相比, RVFL+的修正函数是正的或负的.
换句话说, RVFL+不考虑约束组$\zeta_i(\tilde{\bm h}, b, \phi({\bm x}_i))\geq 0\,(i=1, \cdots, N)$.
对于二值分类, RVFL+中的约束数目至少比SVM+少N个, 这使得RVFL+的优化约束要比SVM+的小得多.
此外, 为了解决 \eqref{RVFL20181102eq10} 中的优化问题, 构造如下的Lagrangian函数$\mathscr L(\bm w, \tilde{\bm w}, \bm\lambda)$.
\begin{align}
    \min\limits_{\bm w, \tilde{\bm w},\bm \lambda} \frac 1 2\|{\bm w}\|_2^ 2 + \frac \gamma 2 \|\tilde {\bm w}\|_2^ 2 + C\sum_{i=1}^N  {\tilde {\bm h}}({\bm x}_i)\tilde {\bm w}-\sum_{i=1}^N  \bm \lambda_i(\bm h(\tilde {\bm x}_i){\bm w} -\bm y_i +{\tilde {\bm h}}({\bm x}_i)\tilde {\bm w}),
\end{align}
其中$\bm\lambda=[\bm\lambda_1, \cdots, \bm\lambda_N]^T$是拉格朗日乘子.

为了找到最小二乘解, 我们使用KKT条件来计算Lagrangian函数$\mathscr L(\bm w, \tilde{\bm w},\bm\lambda)$关于变量$\bm w,\tilde{\bm w}$和$\bm\lambda$的鞍点.
\begin{align}
 &\frac {\partial \mathscr L (\bm w, \tilde{\bm w},\bm \lambda)}{\partial \bm w}= 0 \Rightarrow  \bm w = \bm  H^T \bm \lambda, \label{RVFL20181102eq12}\\
 &\frac {\partial \mathscr L (\bm w, \tilde{\bm w},\bm \lambda)}{\partial \tilde{\bm w}} = 0 \Rightarrow  \tilde{\bm w} = \frac 1  \gamma  (\tilde{\bm  H}^T  \bm \lambda - \tilde{\bm  H}^T C \bm 1), \label{RVFL20181102eq13}\\
 &\frac {\partial \mathscr L (\bm w, \tilde{\bm w},\bm \lambda)}{\partial \bm \lambda_i} = 0 \Rightarrow  \bm h(\bm x_i)\bm w - \bm y_i + \tilde {\bm h}({\bm x}_i) \tilde{\bm w} = 0, 1 \leq  i \leq  N.\label{RVFL20181102eq14}
\end{align}
其中$\bm 1\in\mathbb R^{N\times m}$是单位矩阵. $\tilde{\bm H}$也是来自增强节点的连接输出矩阵, 它对应于RVFL的特性.
将 \eqref{RVFL20181102eq12} 和 \eqref{RVFL20181102eq13} 代入 \eqref{RVFL20181102eq14}, 得到
\begin{align}
    \bm H \bm H^T  \bm \lambda + \frac 1 \gamma\tilde{\bm H} \tilde{\bm H}^T  (\bm \lambda -C \bm 1) = \bm Y. \label{RVFL20181102eq15}
\end{align}

可以进一步将 \eqref{RVFL20181102eq15} 重写为
\begin{align}
    \left(\bm H \bm H^T  + \frac 1 \gamma \tilde{\bm H} \tilde{\bm H}^T \right )\bm \lambda  = \bm Y - \frac {C \bm 1} {\gamma}\tilde{\bm H} \tilde{\bm H}^T.  \label{RVFL20181102eq16}
\end{align}
再由 \eqref{RVFL20181102eq12} 和 \eqref{RVFL20181102eq16}, 得到RVFL+的最后闭式解
\begin{align}
    \bm  w =\bm H^T \left(\bm H \bm H^T  + \frac 1 \gamma \tilde{\bm H} \tilde{\bm H}^T \right )^{-1} \left(\bm Y - \frac {C \bm 1} {\gamma}\tilde{\bm H} \tilde{\bm H}^T \right ). \label{RVFL20181102eq17}
\end{align}
根据岭回归方法\cite{Bishop2012-6469}, 为了避免奇异性并保证RVFL+的稳定性, 我们还附加了一个$\frac{\bm I}C$项. 因此, 我们可以得到RVFL+的最终闭式解
\begin{align}
    \bm w = \bm H^T \left(\bm H \bm H^T  + \frac 1 \gamma \tilde{\bm H} \tilde{\bm H}^T +\frac{ \bm I} C \right)^{-1} \left(\bm Y -\frac {C \bm 1} {\gamma}\tilde{\bm H} \tilde{\bm H}^T \right ).
\label{RVFL20181102eq18}
\end{align}
RVFL+ 输出函数定义为
\begin{align}
    f(x) =\bm  h (\bm x)\bm w =\bm h(\bm  x)\bm H^ T \left(\bm H \bm H^T  +\frac 1 \gamma\tilde{\bm H} \tilde{\bm H}^T  +\frac{\bm  I} C \right)^{-1}\left(\bm Y - \frac {C \bm 1} {\gamma}\tilde{\bm H} \tilde{\bm H}^T \right).
\end{align}
此外, 在测试阶段,当使用测试数据$\bm z$而不是训练数据$\bm x$时, 我们可以直接获得输出函数$f_{test}(\bm z)=\bm h(\bm z)-\bm w$.
\subsubsection{区间二型模糊集}
本节将随机向量函数链接网络(RVFL) 推广到了张量结构,  增强节点使用了区间二型模糊集来扩展非线性激活函数, 主要目的是用二型模糊集来捕捉输入输出关系中蕴含的高阶信息.
提出了两种随机向量函数链接网络, 一种是区间二型随机向量函数链接网络, 另一种是基于张量的二型随机向量函数链接网络.
区间二型随机向量函数链接网络和随机向量函数链接网络类似, 不同之处在于 其输出增强节点的输出是不确定加权方法的解模糊结果.
张量的二型随机向量函数链接网络也使用了不确定加权方法, 同时还使用了下隶属函数值和上隶属函数值, 由此形成了一个 4阶张量.
基于这个结构, 基于Einstein积的偶数阶张量的Moore-Penrose逆被用来求解张量方程.
此外, 权重向量由一个平衡因子综合得到, 它是RVFL网络结果和张量方程结果的加权平均.
最后, 分别在三个非线性测试函数、非线性系统识别问题和四个回归问题上验证了给出的算法.

高斯区间二型模糊集有两种定义形式,  不确定均值或者是不确定方差. 本节使用的区间二型模糊集上隶属和下隶属的均值相同.
图 \ref{TFLN:fig1-2} 给出了均值相同、方差不同的区间二型模糊集. 隶属函数按如下的方式定义
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [!htp]
    \begin{center}
        \includegraphics[width =0.6\linewidth]{IntervalT2MF}
    \end{center}
    \caption{区间二型模糊集}
    \label{TFLN:fig1-2}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align}
    &\bar{\mu} _{i} (x_i)=\exp\left(- \frac{(x_i-m_{i} )^2} {\bar{\sigma} ^2_{i} }  \right),\label{IT2TSKNM_mu1} \\
    &\underline{\mu} _{i} (x_i)=\exp\left(- \frac{(x_i-m_{i} )^2} {\underline{\sigma} ^2_{i} }  \right),
    \label{IT2TSKNM_mu2}
\end{align}
其中 $m_{i} $, $\bar{\sigma} _{i} $ 和 $\underline{\sigma} _{i} $ $(1\leq i\leq L)$分别是二型模糊集的下隶属函数和上隶属函数的均值和方差 . 下隶属函数和上隶属函数之间的隶属度是1的二型模糊集就是区间二型模糊集.

图 \ref{TensorRVFL20180927-2} 给出了基于张量积的区间二型随机向量函数链接网络 (TT2-RVFL), 其中 $\tilde g_i(\cdot)\, (i=1, 2, \cdots, L)$ 是区间二型模糊激活函数.
不同于RVFL, TT2-RVFL使用区间二型模糊集, 扩展了RVFL的增强节点部分,
因此, 区间二型模糊集的输出可以看成是原来两个激活函数的输出结果. RVFL网络中的乘法需要修正, 对于高维结构的TT2-RVFL, 增强型节点使用了张量的Einstein积运算.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The functional link net and learning optimal control, 1995, \cite{PaoPhillips1995-36113}
\begin{figure} [!htbp]
\def\circlrad2{0.1}
\begin{center}
\begin{tikzpicture} [scale=0.8,>=stealth,rotate border/.style={shape border uses incircle, shape border rotate=#1} ]
% layer 1
 \node(layer01) at (2,1) {$x_{1} $} ;
 \node(layer02) at (3.8,1) {$x_{l} $} ;
 \node(layer03) at (5,1) {$x_{L} $} ;
 \node(layer04) at (7,1) {\footnotesize $\tilde g_1(\bm a_1 \bm x_1+b_1)$} ;
 \node(layer05) at (9.8,1) {\footnotesize $\tilde g_2(\bm a_l \bm x_l+b_l)$} ;
 \node(layer06) at (12.8,1) {\footnotesize $\tilde g_L(\bm a_L \bm x_L+b_L)$} ;
% layer 2
 \node(layer11) at (2,3) {} ;
 \node(layer12) at (3.8,3) {} ;
 \node(layer13) at (5,3) {} ;
 \node(layer14) at (7,3) {} ;
 \node(layer15) at (9.8,3) {} ;
 \node(layer16) at (12.8,3) {} ;
%  cdots
 \node(layer1cd) at (2.9,3) {$\cdots$} ;
 \node(layer1cd) at (4.45,3) {$\cdots$} ;

 \node(layer1cd) at (8.75,3) {$\cdots$} ;
 \node(layer1cd) at (11,3) {$\cdots$} ;

\draw[fill=blue!30] (layer11) circle (\circlrad2 cm) node[xshift=0.1cm,yshift=0.2cm,above] {$w_1$} ;
\draw[fill=blue!30] (layer12) circle (\circlrad2 cm) node[xshift=0.1cm,yshift=0.2cm,above] {$w_l$} ;
\draw[fill=blue!30] (layer13) circle (\circlrad2 cm) node[xshift=0.75cm,yshift=0.2cm,above] {$w_L$} ;
\draw[fill=blue!30] (layer14) circle (\circlrad2 cm) node[xshift=0.65cm,yshift=0.2cm,above] {$\bm w_{L+1} $} ;;
\draw[fill=blue!30] (layer15) circle (\circlrad2 cm) node[xshift=0.1cm,yshift=0.2cm,above] {$\bm w_{2l} $} ;
\draw[fill=blue!30] (layer16) circle (\circlrad2 cm) node[xshift=0.1cm,yshift=0.2cm,above] {$\bm w_{2L} $} ;

% layer 3
\node(layer21) at (7.5,6) {} ;
\draw[fill=blue!40] (layer21) circle (\circlrad2 cm);

% layer 4
\node(layer31) at (7.5,7) {} ;
\draw[->] ($(layer21)+(0,\circlrad2)$) -- (layer31);

% 0->1
%\draw[->] ($(layer01) +(0.2,0)$) -- ($(layer11)-(0.6,0.1)$);
\draw[->] (layer01) -- ($(layer11)-(0,\circlrad2)$);
\draw[->] (layer02) -- ($(layer12)-(0,\circlrad2)$);
\draw[->] (layer03) -- ($(layer13)-(0,\circlrad2)$);
\draw[->,thick=1.2] (layer04) -- ($(layer14)-(0,\circlrad2)$);
\draw[->,thick=1.2] (layer05) -- ($(layer15)-(0,\circlrad2)$);
\draw[->,thick=1.2] (layer06) -- ($(layer16)-(0,\circlrad2)$);
% 1->2
\draw[->] ($(layer11)+(0,\circlrad2)$)--($(layer21)-(0,\circlrad2)$);
\draw[->] ($(layer12)+(0,\circlrad2)$)--($(layer21)-(0,\circlrad2)$);
\draw[->] ($(layer13)+(0,\circlrad2)$)--($(layer21)-(0,\circlrad2)$);
\draw[->,thick=1.2] ($(layer14)+(0,\circlrad2)$)--($(layer21)-(0,\circlrad2)$);
\draw[->,thick=1.2] ($(layer15)+(0,\circlrad2)$)--($(layer21)-(0,\circlrad2)$);
\draw[->,thick=1.2] ($(layer16)+(0,\circlrad2)$)--($(layer21)-(0,\circlrad2)$);
% y
\node(layery) at (7.5,7.05) {$\bm Y$} ;
\end{tikzpicture}
\caption{TT2-RVFL的结构}
\label{TensorRVFL20180927-2}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
TT2-RVFL 的输出模型如图 \ref{TensorRVFL20180927-2}:
\begin{align}
    \bm Y=\mathcal A *_N \mathcal X+\bm H \bm \Omega,\label{TensorRVFLEq05}
\end{align}
其中 $\mathcal A *_N \mathcal X$ 为增强节点的张量乘积, $\bm H \bm \Omega$ 为线性部分的输出 , $\mathcal X$ 为增强节点部分的权重矩阵;
类似于RVFL, $\bm H$ 是输入数据矩阵, 单列输出的数据需要复制一次, 以便与张量和矩阵的运算结果相容, $\Omega$是线性部分的权矩阵, 且$\mathcal A$是一个张量, 它是由 叠加区间二型模糊集的映射结果得到的张量.
对于TT2-RVFL, $\bm Y$, $\mathcal X$ 和$\Omega$需要特别对待, 以便和后件形成的张量方程相容. 下一节将介绍由三个变量组成的张量结构.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{TT2-RVFL}{TT2-RVFL}
RVFL常使用 Radbas ($y = \exp(-s^2)$), Sinc ($y = \textup{sinc} (s)$), Sigmoid ($y = \frac 1 {1+e^{-s} } $)和Tribas($y = \max(1 - |s|, 0)$)这几类激活函数, 其中 $s$ 和 $y$分别表示输入和输出变量.
对于TT2-RVFL, 增强节点使用了区间二型模糊集. RVFL的 Radbas激活函数被 推广成区间二型模糊集, 扩展后的 RVFL 使用了区间二型模糊集, 扩展后的激活函数简记为 IT2Radbas, 它是 \eqref{IT2TSKNM_mu1} 和 \eqref{IT2TSKNM_mu2} 的变种,
记为$\tilde g_i(\cdot)$, 可以用下面的式子表示为
 \begin{align}
  &\bar{g} _{i} (x_i)=\exp(-k_1 s^2),\label{IT2TSKNM_g1} \\
  &\underline{g} _{i} (x_i)=\exp(-k_2 s^2),\label{IT2TSKNM_g2}
\end{align}
其中 $s=x_i-m_{i}, k_1=\frac{1} {\bar{\sigma} ^2_{i} } $, $k_2=\frac{1} {\underline{\sigma} ^2_{i} },\, (i=1,2,\cdots, L)$.

对于测试集$\{D_t\} _{t=1} ^{N} $, 其中 $D_t=(\bm x_t,y_t)$, $\bm x_t=(x_{t1}, x_{t2}, \cdots, x_{tN} )\in \mathbb R^{N}$, $y_t\in \mathbb R$ 且$\bm y=[y_1,\cdots, y_N]^T$.
下隶属函数矩阵 $\underline\Phi\in \mathbb R^{N\times 2\times L \times 1}$可用下式构建
\begin{align*}
\begin{array}{cc}
 \underline\Phi_{:,:,1,1}  =\begin{bmatrix}
  \underline g_1(\bm {a} _{11}  \bm x_1+b_{11} )&\underline g_1(\bm {a} _{12}  \bm x_1+b_{12} )\\
   \vdots&\vdots\\
  \underline g_1(\bm {a} _{11}  \bm x_N+b_{11} )&\underline g_1(\bm {a} _{12}  \bm x_N+b_{12} )
\end{bmatrix},\\
\vdots\\
   \underline\Phi_{:,:,L,1}  =\begin{bmatrix}
  \underline g_L(\bm {a} _{L1}  \bm x_1+b_{L1} )&\underline g_L(\bm {a} _{L2}  \bm x_1+b_{L2} )\\
   \vdots&\vdots\\
  \underline g_L(\bm {a} _{L1}  \bm x_N+b_{L1} )&\underline g_L(\bm {a} _{L2}  \bm x_N+b_{L2} )
\end{bmatrix},
\end{array}
\end{align*}
其中 $b_{il} $和$\bm {a} _{il} =[w_{i1} ,w_{i2} 、,\cdots,w_{iK} ]$ ($i=1, 2, \cdots, L$; $l=1,2$) 是随机生成的偏置和输入权重. 下隶属函数矩阵使用下隶属函数来逼近输入$\bm x_t$ 和期望输出$y_t$的关系.
上隶属函数矩阵$\bar \Phi\in \mathbb R^{N\times 2\times L \times 1}$ 可以用类似的方式建立, 具有如下形式
\begin{align*}
 \begin{array} {cc}
 \bar\Phi_{:,:,1,2}  =\begin{bmatrix}
  \bar g_1(\bm {a} _{11}  \bm x_1+b_{11} )&\bar g_1(\bm {a} _{12}  \bm x_1+b_{12} )\\
   \vdots&\vdots\\
  \bar g_1(\bm {a} _{11}  \bm x_N+b_{11} )&\bar g_1(\bm {a} _{12}  \bm x_N+b_{12} )
\end{bmatrix},\\
\vdots\\
   \bar\Phi_{:,:,L,2}  =\begin{bmatrix}
  \bar g_L(\bm {a} _{L1}  \bm x_1+b_{L1} )&\bar g_L(\bm {a} _{L2}  \bm x_1+b_{L2} )\\
   \vdots&\vdots\\
  \bar g_L(\bm {a} _{L1}  \bm x_N+b_{L1} )&\bar g_L(\bm {a} _{L2}  \bm x_N+b_{L2} )
\end{bmatrix},
\end{array}
\end{align*}
其中 $\bm {a} _{ij} \,(i=1,2\cdots,L; j=1,2)$是构建张量所用的随机生成的权重向量.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
不确定加权方法\cite{RunklerCoupland2018-6356} 被用于区间二型模糊激活函数的解模糊化计算,
它代表了隶属函数隶属度对于整体解模糊化结果的影响程度, 映射结果由下式计算得到
\begin{align} \label{TensorRVFLEq08}
    u_{UW} (x) =\frac 1 2 (\underline u(x) + \bar u(x))\cdot (1+\underline u(x)-\bar u(x))^\gamma,
\end{align}
其中 $\gamma > 0$ 用于调整由$(1+\underline u(x)-\bar u(x))^\gamma$给出解模糊化结果.
不确定加权方法推广了简单的平均下隶属和上隶属度方法, 清晰输出结果由下式给出
\begin{align} \label{TensorRVFLEq09}
  (1+\underline u(x)-\bar u(x))^\gamma=
  \left\{
  \begin{array} {ll}
    0,& \underline u(x)=0, \bar u(x)=1,\\
    1,&\underline u(x)=\bar u(x).\\
  \end{array}
  \right.
\end{align}
方程 \eqref{TensorRVFLEq09} 显示出: 当$\gamma =1$时, 解模糊结构呈线性, 对于所有的 $\gamma<1$ 线性增加, 且对于所有 $\gamma>1$, 线性性将会减少.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
当式 \eqref{TensorRVFLEq09} 被用于解模糊化, 只用区间二型模糊集拓展随机向量函数链接网络, 也即, 只是扩展了原网络的增强节点为区间二型模糊集, 这种类型的网络记为IT2-RVFL.
\end{remark}

最后得到的 4阶张量$\Upphi \in \mathbb R^{N\times 2\times L \times 2}$ 是由前面提到的张量$\underline\Phi_{:,:,:,1} $和$\bar \Phi_{:,:,:,2} $构建出来的.
接下来, 张量$\Upphi$用$\mathcal A$来记,  这是由于经典的张量方程中经常使用$\mathcal A$表示张量方程的系数.

基于张量的乘积运算, 得到的增强节点的权重矩阵 维数是$L \times 2$, 将TT2-RVFL的增强节点部分的权重矩阵记为
\begin{align}
  \mathcal X=
  \begin{bmatrix}
    \bm {w} _{L+1\, 1} &\bm {w} _{L+1\,  2} \\
    \bm {w} _{L+2\, 1} &\bm {w} _{L+2\,  2} \\
    \vdots &\vdots\\
    \bm {w} _{2L\, 1} &\bm {w} _{2L\,  2}
  \end{bmatrix} .
\end{align}
TT2-RVFL 的输出可用下式表示
\begin{align}
    \bm Y=\alpha\mathcal A *_N \mathcal X+(1-\alpha)\bm H \bm \Omega, 0\leq \alpha\leq 1,\label{TensorRVFLEq08}
\end{align}
其中 $\alpha$是TT2-RVFL的平衡因子, $\mathcal A$是区间二型模糊激活函数的映射结果, $\mathcal X$是权重矩阵, $\bm Y=[\bm y\, \bm y]$, 从输入到增强节点的权重向量$\bm {a} _{i} \,(i=1,2,\cdots, L)$是随机生成的,
使得 $\underline\Phi$, $\bar\Phi$不饱和; $\bm\Omega=[\bm \omega\, \bm \omega]$是待求的输入权重矩阵, $\bm H$ 是由输入样本构建的输入矩阵.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
    当下三角隶属函数和上三角隶属函数一样时, TT2-RVFL 退化为RVFL, 也即, 激活函数是一型模糊集.
    当不适用直接连接方式, TT2-RVFL 将会退化为张量型极限学习机 \cite{HuangZhao2019-9577}. 当同时使用RVFL和张量结构时,  TT2-RVFL是这两种结构的混合结果.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
  和RVFL相比, 基于张量的二型 RVFL模型 \eqref{TensorRVFLEq08} 做了三方面的扩展:
  1) 增强节点使用区间二型模糊激活函数来得到非线性映射结果, 增强节点部分由张量和张量乘法表示, 且使用了张量方程的求解算法 .
  2) 为了和增强节点的运算相容, TT2-RVFL的线性部分需要 复制权重向量一次, 权重矩阵 $\bm\Omega$等于 $[\bm \omega\, \bm \omega]$, 其中 $\bm \omega$为权重列向量.
  3) 对于输出$\bm Y$, 和以前的输出向量 $\bm y$不同, TT2-RVFL需要重复该向量一次, 以便和张量运算相容.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{algorithm}[!ht]
%\caption{偶数阶张量$\mathcal A$的M-P逆 \cite{HuangZhao2019-9577}}
%\label{T2RVFL3:alg1}
%\begin{algorithmic}[1] %
%\REQUIRE 输入: 张量$\mathcal A$, $I_{1 \cdots N} $ 和指标 $J_{1 \cdots N} $.
%\ENSURE $N$ 为偶数, $I=\prod_{i=1} ^N I_i$, $J=\prod_{i=1} ^N J_i$.
%\STATE 变换张量$\mathcal A\in \mathbb R^{I_{1 \cdots N} \times J_{1 \cdots N} } $ 为矩阵 $A\in \mathbb R^{I\times J} $.
%\STATE 对矩阵$A$做奇异值分解, $A$ 可以分解为 $A=U S V^*$, 其中 $V^*$ 是矩阵$V$的共轭转置.
%\STATE 分别将矩阵$U, S$和$V^*$ 转换为相应阶次的张量$\mathcal U, \mathcal S$ 和$\mathcal V^*$.
%\STATE 输出: 通过$\mathcal A^+ = \mathcal V *_N\mathcal S^+ *_N \mathcal U^*$计算张量$\mathcal A$的 M-P逆.
%\end{algorithmic}
%\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%table 1
\begin{table} [!ht]
\caption{算法1: 偶数阶张量$\mathcal A$的M-P逆 \cite{HuangZhao2019-9577}}
\label{T2RVFL3:alg1}
\vspace{-0.5cm}
\begin{center}
\begin{tabular} {p{1.05cm}p{9.25cm}llcccccc}
\hline
输入:& 张量$\mathcal A$, $I_{1 \cdots N} $ 和指标 $J_{1 \cdots N} $\\
&$N$ 为偶数, $I=\prod_{i=1} ^N I_i$, $J=\prod_{i=1} ^N J_i$\\
&变换张量$\mathcal A\in \mathbb R^{I_{1 \cdots N} \times J_{1 \cdots N} } $ 为矩阵 $A\in \mathbb R^{I\times J} $\\
&对矩阵$A$做奇异值分解, $A$ 可以分解为 $A=U S V^*$, 其中 $V^*$ 是矩阵$V$的共轭转置\\
&分别将矩阵$U, S$和$V^*$ 转换为相应阶次的张量$\mathcal U, \mathcal S$ 和$\mathcal V^*$\\
输出:& 通过$\mathcal A^+ = \mathcal V *_N\mathcal S^+ *_N \mathcal U^*$计算张量$\mathcal A$的 M-P逆\\
\hline
\end{tabular}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{TT2-RVFL中的张量逆} \label{Trapezoidsection5-1}
张量方程 $\mathcal A *_N \mathcal X = \mathcal Y$的张量运算被用于求解 TT2-RVFL网络的权重向量,  该运算非常适合计算TT2-RVFL增强节点部分的权重向量.
求解方程使用的偶数阶张量的 Moore-Penrose (M-P) 逆可由表 \ref{T2RVFL3:alg1} 中的算法中给出, 在得到M-P逆的基础上, 可以很容易算出权重矩阵.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure} [!htp]
%    \begin{center}
%        \includegraphics[width =0.76\linewidth]{NCAATTELML.PNG}
%    \end{center}
%    \caption{张量的Moore-Penrose逆算法\cite{HuangZhao2018NCAA-5838}}
%    \label{NCAATTELML}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
    方程$\mathcal A *_N \mathcal X = \mathcal Y$的解也是如下张量方程的解
\begin{align}
    \mathcal A^T *_N\mathcal  A *_N \mathcal X = \mathcal A^T *_N \mathcal Y,
\end{align}
其中张量$\mathcal A \in \mathbb R^{I_1\times \cdots\times I_N\times J_1\times \cdots\times J_N} $, $\mathcal X \in \mathbb R^{J_1\times \cdots\times J_N\times K_1\times \cdots\times K_M} $
且 $\mathcal Y \in \mathbb R^{I_1\times \cdots\times I_N\times K_1\times \cdots\times K_M}$,
不论方程是否相容, 都可以应用表\ref{T2RVFL3:alg1} 的算法来求解方程 $\mathcal  A *_N \mathcal X = \mathcal Y$ \cite{HuangZhao2019-9577}.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{求解TT2-RVFL的张量逆算法} \label{Trapezoidsection5-2}
为了得到TT2-RVFL的解,  模型 \eqref{TensorRVFLEq08} 的最小范目标函数定义为
\begin{align} \label{TensorRVFLEq13}
    \min_{\bm \Omega,\mathcal X}  \mathcal E_1(\mathcal X)+ \mathcal E_2( \Omega),
\end{align}
其中 $\mathcal E_1(\mathcal X)=\| \bm Y -\mathcal A \mathcal X \|_F^2+\|\mathcal X \|_F^2$, $\mathcal E_2(\Omega)=\|\bm y-\bm H \bm \Omega\|_2^2+\|\bm \Omega\|_2^2$.
模型 \eqref{TensorRVFLEq08} 的极小化问题可以划分成两个子问题来求解, 由下面的定理给出：
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mythm}{极小化模型的解}{1}
 对于极小化模型 \eqref{TensorRVFLEq08}, 令$\Omega^*$ 是$\bm y= \bm H \bm \Omega$的解集, $\mathcal X^*$ 是$\mathcal Y=\mathcal A *_N \mathcal X$的解.
 对于模型 \eqref{TensorRVFLEq13} 的两个子问题$\mathcal E_1(\mathcal X)$ 和 $\mathcal E_2(\mathcal X)$, $\mathcal Y$ 退化为一个矩阵, 即$\mathcal Y=[\bm y\, \bm y]$,
 且$\bm y$是训练结果列向量. 式 \eqref{TensorRVFLEq08} 的充分条件就是寻找如下子问题的解
\begin{align}
    \min_{\mathcal X}  \mathcal E_1(\mathcal X)\, \textup{和} \, \min_{\bm \Omega} \mathcal E_2(\Omega),\label{TensorRVFLEq11}
\end{align}
则方程的解 $\begin{bmatrix}  \mathcal X & \bm \Omega \end{bmatrix} ^T$ 满足下式
\begin{align}
  \bm Y=\mathcal A *_N \mathcal X+\bm H \bm \Omega=
  \begin{bmatrix}
    \mathcal A& \bm H
  \end{bmatrix}
  *
  \begin{bmatrix}
    \mathcal X\\
    \bm \Omega
  \end{bmatrix} ,\\
    \begin{bmatrix}
    \mathcal X\\
    \bm \Omega
  \end{bmatrix}
  =\begin{bmatrix}
    \alpha \mathcal X^*\\
    (1-\alpha)\bm \Omega^*
  \end{bmatrix} ,
\end{align}
其中 $\alpha$是 TT2-RVFL ($0\leq \alpha\leq 1$)的平衡因子, $*_N$是张量$\mathcal A$和$\mathcal X$的Einstein乘积.
\end{mythm}
\begin{proof}
  $\bm H \bm \Omega=\bm Y$的解是$\Omega^*$, $\bm H \bm \Omega^*=\bm Y$成立.
  $\mathcal X^*$ 是$\mathcal A *_N \mathcal X=\mathcal Y$的解, 则$\mathcal A *_N \mathcal X^*=\mathcal Y$.
  给定平衡因子$\alpha$, $\mathcal A *_N  \alpha \mathcal X^*+\bm H (1-\alpha)\bm \Omega^*=\alpha \bm Y+(1-\alpha)\bm Y=\bm Y$成立.
  对于$\bm Y\equiv \mathcal Y$的情况, 张量$\mathcal Y$ 退化为矩阵$\bm Y$, 且$\bm Y\in \mathbb R^{L\times 2}$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
    对于TT2-RVFL, 权重向量是RVFL的输出结果和 张量方程解结果的综合, RVFL的权重向量记为$\bm \omega=(\bm H^T \bm H+\frac {\bm I}  C)^{-1} \bm H^T \bm Y$, 其中 $\bm I$ 是单位矩阵, 且 $C$ 是平衡参数.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{仿真实验} \label{TensorFLNsection4}
提出的算法使用了Frobenius 范数来度量训练误差和测试误差, 所有结果都由1000次运行结果求得. 为了说明IT2-RVFl 和 TT2-RVFL的学习能力, 隐藏节点数选为$L=[30,35,40]$.
IT2-RVFL 和 TT2-RVFL使用了均值一样, 方差不确定这一形式, 下隶属函数从区间$[0.5,0.9]$随机选取, 上隶属函数通过给下隶属函数值加 0.1 得到;
则$k_1, k_2$ 可由关系式$k_1=\frac{1} {\bar{\sigma} ^2_{i}}, k_2=\frac{1} {\underline{\sigma} ^2_{i}} \,(i=1,2,\cdots, L)$ 求得.
$\bar{q} $是$q_1, \cdots, q_N$的平均结果.
\subsubsection{非线性函数}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%例-1
\begin{example}\label{AIC5exam4.1}
单输入 \textup{Sinc} 函数的逼近问题如下
\begin{align}
  y =\left\{
  \begin{array} {ll}
    1, &x = 0 \\
    \frac{\sin\,x} {x} ,&x\neq 0
  \end{array} \right. x\in[-10,10].
\end{align}
\vspace{-0.1cm}
\end{example}
对于 IT2RVFL和 TT2-RVFL, 平衡参数设置为$C=2^{10}$, $\gamma =0.95$.
表 \ref{TT2-RVFL181123:Sec4-1} 给出了例 4.1在不同 $L$下的训练误差和测试误差.
对于IT2-RVFL, 当$L=30,35$, 它的训练误差要小于TT2-RVFL的训练误差. 然而, TT2-RVFL 的测试误差要小于IT2-RVFL的测试误差, 这意味着TT2-RVFL中的平衡因子对网络的学习能力有一定的影响.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%table 1
\begin{table} [!ht]
\caption{例 4.1中不同$L$下的训练和测试误差 } \label{TT2-RVFL181123:Sec4-1}
\vspace{-0.4cm}
\begin{center}
\begin{tabular} {p{0.25cm} p{2.75cm} ccccccc}
\hline
\multirow{2} {*} {$L$} &\multirow{2} {*} {算法} &\multicolumn{2} {c} {训练误差} &\multicolumn{2} {c} {预测误差} \\
\cline{3-4} \cline{5-6}
&&  均值   &  方差 &  均值   &  方差 \\
\hline
\multirow{2} {*} {30}   &IT2-RVFL&2.53e-02  & 4.51e-05 &  4.82e-02  & 7.22e-04\\
     &TT2-RVFL&   \textbf{2.36e-04}  &  \textbf{1.71e-05}  &  \textbf{2.32e-04}  &  \textbf{1.70e-05} \\
\multirow{2} {*} {35}   &IT2-RVFL&   2.34e-02 &  9.15e-05 &  4.66e-02 &  3.17e-04\\
     &TT2-RVFL&   \textbf{1.57e-02}  &  \textbf{7.44e-05}  &  \textbf{1.54e-02}   & \textbf{7.61e-05} \\
\multirow{2} {*} {40}   &IT2-RVFL&   \textbf{2.18e-02}  &  \textbf{1.38e-04}   & 4.57e-02 &  7.92e-04\\
     &TT2-RVFL&   3.86e-02 &  4.25e-04  & \textbf{3.76e-02}   & \textbf{4.25e-04} \\
\hline
\end{tabular}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%例-2
\begin{example}\label{AIC5exam4.2}
含噪的非线性函数具有如下形式
\begin{align} \label{nonlinearfunctioneq17}
    y=\frac{0.9x} {1.2x^3+x+0.3} +\eta,x\in[0,1],
\end{align}
其中 $\eta$ 是均匀分布的白噪声,  $\eta$的幅值设置为 0.2.
\end{example}

例 \ref{AIC5exam4.2} 用来测试给出算法的野值拒绝能力.
对于IT2RVFL和 TT2-RVFL, 平衡参数设置为$C=2^{10}$, $\gamma =1$.
表 \ref{TT2-RVFL181121:Sec4-2} 给出了例4.2在不同$L$值下的训练误差和测试误差.
结果表明, 当$L=30,35,40$时, TT2-RVFL 性能要好于 IT2-RVFL, 其平均训练误差和测试误差要小于IT2-RVFL的结果.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%table 2
\begin{table} [!ht]
\caption{例 4.2中不同$L$下的训练误差和预测误差}
\vspace{-0.4cm}
\begin{center}
 \begin{tabular} {p{0.25cm} p{2.75cm} cccc}
\hline
\multirow{2} {*} {$L$} &\multirow{2} {*} {算法} &\multicolumn{2} {c} {训练误差} &\multicolumn{2} {c} {预测误差} \\
\cline{3-4} \cline{5-6}
&&  均值   &  方差 &  均值   &  方差 \\
\hline
\multirow{2} {*} {30}   &IT2-RVFL&2.85e-01 &  \textbf{7.30e-06}  &  4.13e-01 &  5.58e-03\\
     &TT2-RVFL&\textbf{2.07e-01}   & 8.43e-04  & \textbf{3.07e-01}  &  \textbf{2.19e-03} \\
\multirow{2} {*} {35}   &IT2-RVFL&2.85e-01 &  \textbf{2.94e-06}   & 4.15e-01 &  2.45e-03\\
     &TT2-RVFL&\textbf{2.07e-01}   & 1.75e-03  & \textbf{3.06e-01}  &  \textbf{1.96e-03} \\
\multirow{2} {*} {40}   &IT2-RVFL& 2.85e-01 &  \textbf{5.14e-06}   & 4.11e-01 &  5.10e-03\\
     &TT2-RVFL&\textbf{2.06e-01}   & 4.54e-04  & \textbf{3.04e-01}  & \textbf{1.13e-03} \\
\hline
\end{tabular}
\end{center}
\label{TT2-RVFL181121:Sec4-2}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%例-3
\textbf{例 4.3}: 双输入单输出 Sinc函数的逼近问题.
\begin{align} \label{two-inputSincfun}
    z =\left|\frac{\sin x\cdot\sin y} {x y} \right|, (x,y)\in [-\pi,\pi]^2.
\end{align}
采用等距采样方式, $41\times 41$ 训练数据对用于训练, $30\times 30$等距采样结果用来检验模型的泛化能力.
对于IT2-RVFL和TT2-RVFL, 平衡因子设置为 $C=2^{10}$, $\gamma =1$.
表 \ref{TT2-RVFL181121:Sec4-3} 给出了例4.3在不同$L$下的训练误差和测试误差.
\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{nonlinearfunTstRslutsT2RVFL.pdf}
   \caption{TT2-RVFL对非线性系统识别问题的预测结果}
  \label{nonlinearfunTstRslutsT2RVFL-fig4}
\end{figure}
TT2-RVFL的结果要好于 IT2-RVFL的结果, 训练误差和测试误差都要小于IT2-RVFL的结果.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%table 3
\begin{table} [!ht]
\caption{例 4.3中不同$L$下的训练误差和预测误差}
\vspace{-0.4cm}
\begin{center}
 \begin{tabular} {p{0.25cm} p{2.75cm} cccc}
\hline
\multirow{2} {*} {$L$} &\multirow{2} {*} {算法} &\multicolumn{2} {c} {训练误差} &\multicolumn{2} {c} {预测误差} \\
\cline{3-4} \cline{5-6}
&&  均值   &  方差 &  均值   &  方差 \\
\hline
\multirow{2} {*} {30}   &IT2-RVFL& 4.65e-02  & \textbf{3.09e-06}   & 7.06e-02  & \textbf{1.43e-09} \\
     &TT2-RVFL&   \textbf{4.36e-02}   & 1.30e-05 &  \textbf{6.77e-02}  &  6.05e-06\\
\multirow{2} {*} {35}   &IT2-RVFL&   4.63e-02 &  \textbf{8.69e-06}  &  7.06e-02  & \textbf{2.40e-08} \\
     &TT2-RVFL&   \textbf{4.07e-02}   & 3.38e-05 &  \textbf{6.73e-02}   & 9.21e-06\\
\multirow{2} {*} {40}   &IT2-RVFL&   \textbf{4.63e-02}  &  \textbf{2.95e-06}  &  7.06e-02  & \textbf{1.56e-09} \\
     &TT2-RVFL&   \textbf{3.98e-02}   & 4.72e-05  & \textbf{6.75e-02}  &  1.09e-05\\
\hline
\end{tabular}
\end{center}
\label{TT2-RVFL181121:Sec4-3}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{非线性系统参数识别}
对于如下的非线性系统 \cite{Rong2009-4804697}
\begin{align} \label{TT2FS:Nonlinearsys18}
     y_p(k) =\frac{ y_p(k - 1)y_p(k - 2)(y_p(k - 1) + 2.5)} {1 + (y_p(k - 1))^2 + (y_p(k - 2))^2} + u(k - 1).
\end{align}
系统的平衡态为 (0,0),  输入 $u(k) \in \{-2, 2\}$, 操作区间规定为 $[-2, 2]$. 均匀分布的随机变量从$\{-2, 2\}$ 中产生, 测试使用的输入 $u(k) = \sin(2\pi k/25)$.
通过选取$[y_p(k - 1), y_p(k - 2), u(k - 1)]$作为输入变量, $y_p(k)$作为输出变量. 系统可以写为
\begin{align}
    \hat y _p(k) = \hat f(y_p(k - 1), y_p(k - 2), u(k - 1)).
\end{align}
随机选用了800组数据, 600 组用于训练, 200 组用于测试.
对于IT2RVFL 和 TT2-RVFL, 平衡参数设置为$C=2^{10}$.
对于仿真,  TT2-RVFL 的预测结果见图 \ref{nonlinearfunTstRslutsT2RVFL-fig4}.
训练误差均值的Frobenius 范数 是1.87e-01, 测试误差均值的Frobenius 范数 是3.94e-01,  结果显示,  TT2-RVFL能够得到满意的精度.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{回归问题}
测试使用了四个回归数据集 (Auto-Mpg, Bank, Diabetes 和  Triazines).
Auto-MPG 数据集有392 个样本, 每一个样本有 8 个属性. Bank数据集有8192个样本, 每一个样本有 8 个属性.
Diabetes 数据集有768个样本, 每一个样本有4个属性. Triazines 数据集有186个样本, 每一个样本有5个属性.
对于Auto-MPG 和 Bank, 使用了mapminmax 方法来计算映射到区间 $[-1,1]$ 的数据.
而对于Diabetes和Triazines, 则直接使用原来的数据样本.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{数据集 Auto-Mpg, Bank, Diabetes 和 Triazines上的训练和测试误差}
\vspace{-0.4cm}
\begin{center}
 \begin{tabular} {llccccccccc}
\hline
\multirow{2} {*} {数据集} &\multirow{2} {*} {算法} &\multicolumn{2} {c} {训练误差} &\multicolumn{2} {c} {预测误差} \\
\cline{3-4} \cline{5-6}
&&  均值   &  方差  &  均值   &  方差   \\
\hline
\multirow{5} {*} {Auto-Mpg} &ELM   &4.3511e-01   &2.4125e+0   &4.3423e-01   &2.5502e+0   \\
&RELM   &1.3514e+0   &1.6125e+0   &1.3132e+0   &1.6101e+0   \\
&RVFL & 9.9152e-01 &  6.3012e-03   &9.2356e-01  & 5.1511e-03\\
&IT2-RVFL&\textbf{2.2161e-02}   & \textbf{1.5915e-04}   & 4.5358e-02  & \textbf{1.6692e-04}  \\
&TT2-RVFL&4.4055e-02  & 8.1940e-04  & \textbf{4.3847e-02}   & 8.1024e-04\\
\cline{3-6}
%%%%%%%%--------------------------------------
\multirow{5} {*} {Bank} &ELM    &3.2400e-02   &5.8710e-02  &3.2103e-02   &5.8425e-02\\
&RELM    &3.3214e-02   &5.8321e-02      &\textbf{3.0936e-02}    &5.8631e-02\\
&RVFL &1.1784e-01 &  6.3033e-03  & 5.0163e-02 &  1.3696e-04\\
&IT2-RVFL&\textbf{2.9035e-02}   & \textbf{3.4437e-05}   & 7.0701e-02 &  9.9726e-05\\
&TT2-RVFL&4.3582e-02 &  1.1435e-04 &  5.1382e-02 &  \textbf{8.3401e-05} \\
\cline{3-6}
%%%%%%%%--------------------------------------
\multirow{5} {*} {Diabetes} &ELM  &1.5199e-01   &1.5535e-01   &1.5724e-01   &1.3706e-01  \\
&RELM   &1.5046e-01   &1.3448e-01   &1.5741e-01   &1.4002e-01 \\
&RVFL &3.3931e-01  & 6.2734e-02   &\textbf{8.6827e-02}  &  1.0573e-02\\
&IT2-RVFL&1.4838e-01  & 2.6181e-05  & 1.5938e-01  & 4.3915e-04\\
&TT2-RVFL&\textbf{1.4548e-01}  &  \textbf{2.1532e-06}   & 1.4522e-01  & \textbf{8.8627e-05} \\
\cline{3-6}
%%%%%%%%--------------------------------------
\multirow{5} {*} {Triazines} &ELM &9.8322e-02   &2.4095e-01   &1.0947e-01   &1.6613e-01   \\
&RELM    &9.9402e-02   &2.1026e-01   &1.0739e-01   &1.6718e-01\\
&RVFL &3.0201e-01 &  2.8114e-01  & \textbf{2.2935e-02}   & 1.3741e-02\\
&IT2-RVFL&\textbf{8.2873e-02}  &  \textbf{6.5574e-05}  &  4.0105e-01  & 3.9270e-03\\
&TT2-RVFL&1.0360e-01  & 3.1127e-04 &  2.6182e-01 &  \textbf{1.0300e-03} \\
\hline
\end{tabular}
\end{center}
\label{TT2-RVFL181116:Sec4-1}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
   利用表 \ref{TT2-RVFL181116:Sec4-1} 的四个数据集和神经网络工具线或者神经网络APP, 编写一个表 \ref{TT2-RVFL181116:Sec4-1} 的实现代码, 并对结果进行分析比较.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
对于IT2-RVFL和 TT2-RVFL, 平衡参数设置为 $C=2^{10}$, $\gamma =1$.
RVFL的代码可以从Matlab的File Exchange下载,  ELM 和RELM 的代码可以从\href{https://www.ntu.edu.sg/home/egbhuang/elm_random_hidden_nodes.html}{主页下载}.
表 \ref{TT2-RVFL181116:Sec4-1} 给出了ELM、RELM、RVFL、IT2-RVFL和TT2-RVFL算法的训练误差和测试误差.
对于Auto-MPG, 扩展后的RVFL的训练误差和测试误差都得到了很好的结果.
IT2-RVFL在Bank上的训练误差最小, RELM在Bank上的测试误差最小, 这说明正则化项在ELM的训练过程中是发挥作用的.
TT2-RVFL 在Diabetes上的训练误差最小, 它的测试误差要大于 RVFL, 均值的误差大约是0.0584.
IT2-RVFL 在Triazines 的训练误差最小, 而RVFL的测试误差则总体较小.
对于四个数据集的回归结果, 可以得出 IT2-RVFL和 TT2-RVFL是可以作为回归问题的求解器使用的.
如表 \ref{TT2-RVFL181121:Sec4-2} 的结果所示, TT2-RVFL更适合于数据含噪的情形.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{进化计算}
进化计算(Evolutionary Computation,EC)是在达尔文(Darwin)的进化论和孟德尔(Mendel)的遗传变异理论的基础上产生的一种在基因和种群层次上模拟自然界生物进化过程与机制的问题求解技术.
\begin{mydef}{神经进化}{1}
    神经进化是人工智能和机器学习领域的一个分支, 是一种机器学习技术, 它能够改进人工神经网络, 并且试图通过进化算法去寻找最优神经网络.
    具体而言, 就像大自然通过突变、交叉或移除一些潜在的遗传密码来提高大脑的功能一样, 人工神经网络通过进化算法能够产生越来越好的拓扑、权重和超参数.
\end{mydef}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{NetEvolving2020020401.png}
\caption{神经进化演化过程}
\label{NetEvolving2020020401}
\end{figure}
简单的说就是将进化的思想使用到神经网络参数优化的更迭中.
它使用基于群体的优化方法, 能够不断提高群体中每个神经网节点的质量, 从而根据其要解决的问题生成越来越好的神经网络.
种群中的每个个体的存储方式并不是复杂的神经网络, 而是存储为基因组. 基因组是一种简化的遗传表示, 可以映射到神经网络.

进化计算主要包括一下四个分支:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item 遗传算法(Genetic Algorithm, GA)
\item 进化策略(Evolutionary Strategy, ES)
\item 进化规划(Evolutionary Programming, EP)
\item 遗传规划(Genetic Programming,GP)四大分支.
\end{itemize}
其中, 第一个分支是进化计算中最初形成的, 是一种具有普遍影响的模拟进化优化算法. 因此主要讨论遗传算法.

遗传算法是一种模拟自然界生物进化过程与机制进行问题求解的自组织、自适应的随机搜索技术. 它以达尔文进化论的“物竟天择、适者生存”作为算法的进化规则, 并结合孟德尔的遗传变异理论, 将生物进化过程中的
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item 繁殖(Reproduction)
\item 变异(Mutation)
\item 竞争(Competition)
\item 选择(Selection)
\end{itemize}
引入到了算法中.

  (2) 进化计算的生物学基础

  自然界生物进化过程是进化计算的生物学基础, 它主要包括遗传(Heredity)、变异(Mutation)和进化(Evolution)理论.

  \ding{172} 遗传理论
\begin{mydef}{遗传}{1}
    遗传是指父代(或亲代)利用遗传基因将自身的基因信息传递给下一代(或子代), 使子代能够继承其父代的特征或性状的一种生命现象.
\end{mydef}
  正是由于遗传的作用, 自然界才能有稳定的物种.

  在自然界, 构成生物基本结构与功能的单位是细胞(Cell). 细胞中含有一种包含着所有遗传信息的复杂而又微小的丝状化合物, 人们称其为染色体(Chromosome).
  在染色体中, 遗传信息由基因(Gene)所组成, 基因决定着生物的性状, 是遗传的基本单位.
  染色体的形状是一种双螺旋结构, 构成染色体的主要物质叫做脱氧核糖核酸(DNA), 每个基因都在DNA长链中占有一定的位置.
    \begin{mydef}{基因组}{1}
        一个细胞中的所有染色体所携带的遗传信息的全体称为一个基因组(Genome).
    \end{mydef}
  细胞在分裂过程中, 其遗传物质DNA通过复制转移到新生细胞中, 从而实现了生物的遗传功能.

   \ding{173} 变异理论

   \begin{mydef}{变异}{1}
    变异是指子代和父代之间, 以及子代的各个不同个体之间产生差异的现象.
    \end{mydef}
    \begin{mydef}{变异}{1}
        变异是生物进化过程中发生的一种随机现象, 是一种不可逆过程, 在生物多样性方面具有不可替代的作用.
    \end{mydef}
    引起变异的主要原因有以下两种:

    \qquad 1) 杂交, 是指有性生殖生物在繁殖下一代时, 两个同源染色体之间的交配重组, 即两个染色体在某一相同处的DNA被切断后再进行交配重组, 形成两个新的染色体.

   \qquad 2) 复制差错, 是指在细胞复制过程中因DNA上某些基因结构的随机改变而产生出新的染色体.

    \ding{174}  进化理论
    \begin{mydef}{进化}{1}
        进化是指在生物延续生存过程中, 逐渐适应其生存环境, 使得其品质不断得到改良的一种生命现象.
    \end{mydef}

    遗传和变异是生物进化的两种基本现象, 优胜劣汰、适者生存是生物进化的基本规律.

    \begin{mydef}{达尔文的进化主义}{1}
         达尔文的自然选择学说认为: 在生物进化中, 一种基因有可能发生变异而产生出另一种新的生物基因. 这种新基因将依据其与生存环境的适应性而决定其增殖能力.
    \end{mydef}

\begin{remark}
    一般情况下, 适应性强的基因会不断增多, 而适应性差的基因则会逐渐减少.
    通过这种自然选择, 物种将逐渐向适应于生存环境的方向进化, 甚至会演变成为另一个新的物种, 而那些不适应于环境的物种将会逐渐被淘汰.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{进化计算的产生与发展}
 进化计算自20世纪50年代以来, 其发展过程大致可分为三个阶段.

     \ding{172}  萌芽阶段

     这一阶段是从20世纪50年代后期到70年代中期. 20世纪50年代后期, 一些生物学家在研究如何用计算机模拟生物遗传系统中, 产生了遗传算法的基本思想, 并于1962年由美国密执安(Michigan)大学霍兰德(Holland)提出.
     1965年德国数学家雷切伯格(Rechenberg)等人提出了一种只有单个个体参与进化, 并且仅有变异这一种进化操作的进化策略.
     同年, 美国学者弗格尔(Fogel)提出了一种具有多个个体和仅有变异一种进化操作的进化规划.
     1969年美国密执安(Michigan)大学的霍兰德(Holland)提出了系统本身和外部环境相互协调的遗传算法. 至此, 进化计算的三大分支基本形成.

    \ding{173}  成长阶段

     这一阶段是从20世纪70年代中期到80年代后期. 1975年, 霍兰德出版专著《自然和人工系统的适应性(Adaptation in Natural and Artificial System)》, 全面介绍了遗传算法.
     同年, 德国学者施韦费尔(Schwefel)在其博士论文中提出了一种由多个个体组成的群体参与进化的, 并且包括了变异和重组这两种进化操作的进化策略.
     1989年, 霍兰德的学生戈尔德伯格(Goldberg)博士出版专著《遗传算法----搜索、优化及机器学习(Genetic Algorithm----in Search Optimization and Machine Learning)》, 使遗传算法得到了普及与推广.

   \ding{174}  发展阶段

     这一阶段是从20世纪90年代至今. 1989年, 美国斯坦福(Stanford)大学的科扎(Koza)提出了遗传规划的新概念, 并于1992年出版了专著《遗传规划----应用自然选择法则的计算机程序设计(Genetic Programming:on the Programming of Computer by Means of Natural Selection)》该书全面介绍了遗传规划的基本原理及应用实例, 标志着遗传规划作为计算智能的一个分支已基本形成.

    进入20世纪90年代以来, 进化计算得到了众多研究机构和学者的高度重视, 新的研究成果不断出现、应用领域不断扩大.
    目前, 进化计算已成为人工智能领域的又一个研究热点.

    国内的研究生参考教材：《遗传算法的数学基础》,  张文修，梁怡, 2003年5月, 西安: 西安交通大学出版社。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{进化计算的基本结构}
进化计算尽管有多个重要分支, 并且不同分支的编码方案、选择策略和进化操作也有可能不同, 但它们却有着共同的进化框架.
若假设$P$为种群(Population, 或称为群体), $t$为进化代数, $P(t)$为第$t$代种群 , 进化计算基本结构的粗略描述如下:
\begin{Verbatim}
{确定编码形式并生成搜索空间;
   初始化各个进化参数, 并设置进化代数$t=0$;
   初始化种群$P(0)$;
   对初始种群进行评价(即适应度计算);
   while(不满足终止条件)do
       {
       $t=t+1$;
       利用选择操作从$P(t-1)$代中选出$P(t)$代群体;
       对$P(t)$代种群执行进化操作;
       对执行完进化操作后的种群进行评价(即适应度计算);
       }
}
\end{Verbatim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
    可以看出, 上述基本结构包含了生物进化中所必需的选择操作、进化操作和适应度评价等过程.
\end{remark}

遗传算法的基本思想是从初始种群出发, 采用优胜劣汰、适者生存的自然法则选择个体, 并通过杂交、变异来产生新一代种群, 如此逐代进化, 直到满足目标为止.
遗传算法所涉及到的基本概念主要有以下几个:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item 种群(Population): 种群是指用遗传算法求解问题时, 初始给定的多个解的集合. 遗传算法的求解过程是从这个子集开始的.
\item 个体(Individual): 个体是指种群中的单个元素, 它通常由一个用于描述其基本遗传结构的数据结构来表示.
        \begin{example}
            可以用0、1组成的长度为l的串来表示个体.
        \end{example}
\item 染色体(Chromos): 染色体是指对个体进行编码后所得到的编码串. 染色体中的每1位称为基因, 染色体上由若干个基因构成的一个有效信息段称为基因组.
\item 适应度(Fitness)函数: 适应度函数是一种用来对种群中各个个体的环境适应性进行度量的函数. 其函数值是遗传算法实现优胜劣汰的主要依据.
\item 遗传操作(Genetic Operator): 遗传操作是指作用于种群而产生新的种群的操作. 标准的遗传操作包括以下3种基本形式:
\begin{itemize}
    \item 选择(Selection),
    \item 交叉(Crosssover),
    \item 变异(Mutation).
\end{itemize}
\end{itemize}

遗传算法主要由染色体编码、初始种群设定、适应度函数设定、遗传操作设计等几大部分所组成, 其算法主要内容和基本步骤可描述如下:

(1) 选择编码策略, 将问题搜索空间中每个可能的点用相应的编码策略表示出来, 即形成染色体;

(2) 定义遗传策略, 包括种群规模$N$, 交叉、变异方法, 以及选择概率$P_r$、交叉概率$P_c$、变异概率$P_m$等遗传参数;

(3) 令$t=0$, 随机选择$N$个染色体初始化种群$P(0)$;

(4) 定义适应度函数$f(f>0)$;

(5) 计算$P(t)$中每个染色体的适应值;

(6) $t=t+1$;

(7) 运用选择算子, 从$P(t-1)$中得到$P(t)$;

(8) 对$P(t)$中的每个染色体, 按概率$P_c$参与交叉;

(9) 对染色体中的基因, 以概率$P_m$参与变异运算;

(10) 判断群体性能是否满足预先设定的终止标准, 若不满足则返回(5).

基本遗传算法流程如图 \ref{AI32fig2019120226518} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.76\textwidth]{AI32C420191126518.PNG}
%\caption{}
%\label{AI32fig2019120226518}
%\end{figure}
%%%%%%%----------------------------------------
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[decision/.style={diamond, draw, fill=blue!20, text width=3.5cm, aspect=5,align=flush center, inner sep=0pt},font={\sf \small},scale=0.78]
\def \smbwd{2cm}
\def \smbwe{4cm}
\thispagestyle{empty}
%定义流程图的具体形状
\node (ABC0) at (-1,0) [draw, process,minimum width=\smbwd, minimum height=0.5cm,xshift=-1cm] {\textcolor[rgb]{1,0,1}{生成初始种群并编码}};   % PQR0
\node (nCz)[draw, process,align=center,below=0.5cm of ABC0,yshift=0cm] {计算个体适应值并评价};                                                %R1
\node (nCzleft0)[align=center,right=4cm of nCz,yshift=0.75cm] {\textcolor[rgb]{0,0,1}{}};                                                 %R1
\draw[thick,thick,->](ABC0)--(nCz);
\node (AB0)[draw, decision,align=center,below=0.5cm of nCz,yshift=0cm,xshift=0.0cm] {满足终止条件吗?};                  %20
\draw[thick,thick,->](nCz)--(AB0);
\node (Choose)[draw, process,align=center,below=1cm of AB0,yshift=0cm] {\textcolor[rgb]{0,0,1}{选择}};
\draw[thick,thick,->](AB0)--node[left,xshift=0.1cm]{$Y$}(Choose);
\node (Cross)[draw, process,align=center,below=0.5cm of Choose,yshift=0cm] {\textcolor[rgb]{0,0,1}{交叉}};
\draw[thick,thick,->](Choose)--(Cross);
\node (Mutation)[draw, process,align=center,below=0.5cm of Cross,yshift=0cm] {\textcolor[rgb]{0,0,1}{变异}};
\draw[thick,thick,->](Cross)--(Mutation);

\draw[thick,->](Mutation)-|(nCzleft0.center)-|(nCz.north);
\node (nil0)[draw, process,align=center,right=3cm of Choose] {\textcolor[rgb]{1,0,1}{中止}};
\draw[thick,->](AB0)node[left,xshift=3.2cm,yshift=-0.2cm] {$N$}-| (nil0);                                              %30
\end{tikzpicture}
\end{center}
\caption{简单GA算法的流程图}
\label{AI32fig2019120226518}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{遗传编码}

常用的遗传编码算法有霍兰德二进制码、格雷码 (Gray Code)、实数编码和字符编码等.

(1)二进制编码

二进制编码是将原问题的结构变换为染色体的位串结构. 在二进制编码中, 首先要确定二进制字符串的长度l, 该长度与变量的定义域和所求问题的计算精度有关.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
假设变量$x\in [5,10]$, 要求的计算精度为$10^{-5}$, 则需要将[5, 10]至少分为600000个等长小区间, 每个小区间用一个二进制串表示. 于是, 串长至少等于20, 原因是:
\begin{align*}
  524288=2^{19}<600000<2^{20}=1048576,
\end{align*}
这样, 对应于区间$[5,10]$内满足精度要求的每个值$x$, 都可用一个20位编码的二进制串$<b_{19},b_{18},\cdots,b_0>$来表示.
\end{example}

二进制编码存在的主要缺点是汉明(Hamming)悬崖.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
  7和8的二进制数分别为0111和1000, 当算法从7改进到8时, 就必须改变所有的位.
\end{example}

 (2) 格雷编码(Gray encoding)

格雷编码是对二进制编码进行变换后所得到的一种编码方法. 这种编码方法\uwave{要求两个连续整数的编码之间只能有一个码位不同, 其余码位都是完全相同的}.

它有效地解决了汉明悬崖问题, 格雷编码的基本原理如下:
设有二进制串$b_1,b_2,\cdots,b_n$, 对应的格雷串为$a_1,a_2,\cdots,a_n$, 则从二进制编码到格雷编码的变换为:
\begin{align}
    a_{i}=\left\{\begin{array}{ll}{b_{1},} & {i=1} \\
    {b_{i-1} \oplus b_{i}}, & {i>1}\end{array}\right.,
\end{align}
其中, ⊕表示模2加法. 而从一个格雷串到二进制串的变换为:
\begin{align}
    b_{i}=\sum_{j=1}^{i} a_{i}\,(\bmod 2).
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    十进制数7和8的二进制编码分别为0111和1000, 而其格雷编码分别为0100和1100.
\end{example}

 (3) 实数编码(Real encoding)


实数编码是将每个个体的染色体都用某一范围的一个实数(浮点数)来表示, 其编码长度等于该问题变量的个数.

实数编码方法是将问题的解空间映射到实数空间上, 然后在实数空间上进行遗传操作.
由于实数编码使用的是变量的真实值, 因此这种编码方法也叫做真值编码方法.
实数编码适应于那种多维、高精度要求的连续函数优化问题.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{适应度函数}
适应度函数是一个用于对个体的适应性进行度量的函数. 通常, 一个个体的适应度值越大, 它被遗传到下一代种群中的概率也就越大.

(1) 常用的适应度函数

在遗传算法中, 有许多计算适应度的方法, 其中最常用的适应度函数有以下两种:

\ding{172} 原始适应度函数

直接将待求解问题的目标函数$f(x)$定义为遗传算法的适应度函数.

\begin{example}
    对于极值求解问题
\begin{align}
    \max _{x \in[a, b]} f(x),
\end{align}
$f(x)$即为$x$的原始适应度函数.
\end{example}

采用原始适应度函数的优点是能够直接反映出待求解问题的最初求解目标, 其缺点是有可能出现适应度值为负的情况.

\ding{173} 标准适应度函数

在遗传算法中, 一般要求适应度函数非负, 并其适应度值越大越好. 这就往往需要对原始适应函数进行某种变换, 将其转换为标准的度量方式, 以满足进化操作的要求, 这样所得到的适应度函数被称为标准适应度函数$f_{\textup{Normal}}(x)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{极小化问题}
对极小化问题, 其标准适应度函数可定义为
\begin{align}
f_{\textup{Normal}}(x)=
\left\{\begin{array}{ll}
{f_{\max }(x)-f(x)}, &f(x)<f_{\max}(x) \\
{0}, & f(x)\geq f_{\max }(x)
\end{array}
\right.,
\end{align}
其中, $f_{\max}(x)$是原始适应函数$f(x)$的一个上界. 如果$f_{\max}(x)$未知, 则可用当前代或到目前为止各演化代中的$f(x)$的最大值来代替. 可见, $f_{\max}(x)$是会随着进化代数的增加而不断变化的.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{极大化问题}
对极大化问题, 其标准适应度函数可定义为
\begin{align}
  f(x)=\left\{
  \begin{array}{ll}
  f(x)-f_{\min }(x), &f(x)>f_{\min}(x)\\
 0, & f(x)\leq f_{\min}(x)
  \end{array}
  \right.
\end{align}
其中, $f_{\min}(x)$是原始适应函数$f(x)$的一个下界. 如果$f_{\min}(x)$未知, 则可用当前代或到目前为止各演化代中的$f(x)$的最小值来代替.

(2) 适应度函数的加速变换

在某些情况下, 适应度函数在极值附近的变化可能会非常小, 以至于不同个体的适应值非常接近, 使得难以区分出哪个染色体更占优势.
对此, 最好能定义新的适应度函数, 使该适应度函数既与问题的目标函数具有相同的变化趋势, 又有更快的变化速度.

适应度函数的加速变换有两种基本方法

\ding{172} 线性加速

适应度函数的定义如下:
\begin{align}
    f'(x)=\alpha f(x)+\beta,
\end{align}
其中, $f(x)$是加速转换前的适应度函数; $f'(x)$是加速转换后的适应度函数;  $\alpha$和$\beta$是转换系数.

\ding{173}  非线性加速
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item 幂函数变换方法
   \begin{align}
        f'(x)=k f(x).
   \end{align}
\item 指数变换方法
         \begin{align}
            f'(x)=\exp(-\beta f(x)).
         \end{align}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{遗传算法中的基本操作}
遗传算法采用元启发式自然选择的过程, 属于进化算法(EA)大类. 基本操作包括选择、交叉和变异3种, 而每种操作又包括多种不同的方法, 下面分别对它们进行介绍.

(1) 选择操作: 选择操作是指根据选择概率, 按某种策略, 从当前种群中挑选出一定数目的个体, 使它们能够有更多的机会被遗传到下一代中.

常用的选择策略可分为比例选择、排序选择和竞技选择三种类型.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item 比例选择: 比例选择方法的基本思想是: 各个个体被选中的概率与其适应度大小成正比.

    \ding{171} 常用的比例选择策略包括: (1) 轮盘赌选择. (2) 繁殖池选择.

\item 轮盘赌选择: 轮盘赌选择法又被称为转盘赌选择法或轮盘选择法. 在这种方法中, 个体被选中的概率取决于该个体的相对适应度. 而相对适应度的定义为:
\begin{align}
    P\left(x_{i}\right)=\frac{f\left(x_{i}\right)}{\sum_{j=1}^{N} f\left(x_{j}\right)},
\end{align}
其中, $P(x_i)$是个体$x_i$的相对适应度, 即个体$x_i$被选中的概率; $f(x_i)$是个体$x_i$的原始适应度; 是种群的累加适应度.
\end{itemize}

    \qquad \ding{171} 轮盘赌选择算法的基本思想是: 根据每个个体的选择概率$P(x_i)$, 将一个圆盘分成$N$个扇区, 其中第$i$个扇区的中心角为:
\begin{align}
    2 \pi \frac{f\left(x_{i}\right)}{\sum_{j=1}^{N} f_{i}\left(x_{j}\right)}=2 \pi p\left(x_{i}\right).
\end{align}
并再设立一个固定指针. 当进行选择时, 可以假想转动圆盘, 若圆盘静止时指针指向第$i$个扇区, 则选择个体$i$.

\ding{171} 从统计角度看, 个体的适应度值越大, 其对应的扇区的面积越大, 被选中的可能性也越大. 这种方法有点类似于发放奖品使用的轮盘, 并带有某种赌博的意思, 因此亦被称为\textbf{轮盘赌选择}.

(2) 交叉操作: 交叉操作是指按照某种方式对选择的父代个体的染色体的部分基因进行交配重组, 从而形成新个体.
\begin{remark}
    交配重组是自然界中生物遗传进化的一个主要环节, 也是遗传算法中产生新个体的最主要方法. 根据个体编码方法的不同, 遗传算法中的交叉操作可分为\uwave{二进制交叉和实值交叉}两种类型.
\end{remark}


    \ding{172} 二进制交叉: 二进制交叉是指二进制编码情况下所采用的交叉操作, 它主要包括\uwave{单点交叉、两点交叉、多点交叉和均匀交叉}等方法.

    \ding{173} 单点交叉: 单点交叉也称简单交叉, 它是先在两个父代个体的编码串中随机设定一个交叉点, 然后对这两个父代个体交叉点前面或后面部分的基因进行交换, 并生成子代中的两个新的个体. 假设两个父代的个体串分别是:
\begin{align*}
    X&=x_1 x_2 \cdots x_k\, x_{k+1} \cdots x_n\\
    Y&=y_1 y_2 \cdots y_k\,  y_{k+1} \cdots y_n.
\end{align*}

    随机选择第$k$位为交叉点, 若采用对交叉点后面的基因进行交换的方法, 但点交叉是将$X$中的$x_{k+1}$到$x_n$部分与$Y$中的$y_{k+1}$到$y_n$部分进行交叉, 交叉后生成的两个新的个体是:
\begin{align*}
    X'&= x_1 x_2 \cdots x_k y_{k+1} \cdots y_n\\
    Y'&= y_1 y_2 \cdots y_k x_{k+1} \cdots x_n.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    设有两个父代的个体串$A=0\, 0\,  1\,  1\,  0\,  1$和$B=1\,  1\,  0\,  0\,  1\,  0$, 若随机交叉点为4, 则交叉后生成的两个新的个体是:
\begin{align*}
    A'= 0\, 0\, 1\, 1\,\,\, 1\, 0\\
    B'= 1\, 1\, 0\, 0\,\,\, 0\, 1.
\end{align*}
\vspace{-0.5cm}
\end{example}

    \ding{174} 两点交叉: 两点交叉是指先在两个父代个体的编码串中随机设定两个交叉点, 然后再按这两个交叉点进行部分基因交换, 生成子代中的两个新的个体.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
假设两个父代的个体串分别是:
\begin{align*}
    X&=x_1\, x_2 \cdots x_i \cdots x_j \cdots x_n\\
    Y&=y_1\, y_2 \cdots y_i \cdots y_j \cdots y_n.
\end{align*}

随机设定第$i,j$位为两个交叉点 (其中$i<j<n$), 两点交叉是将$X$中的$x_{i+1}$到$x_j$部分与$Y$中的$y_{i+1}$到$y_j$部分进行交换, 交叉后生成的两个新的个体是:
\begin{align*}
    X'&= x_1\, x_2 \cdots x_i\, y_{i+1} \cdots y_j\, x_{j+1} \cdots x_n\\
    Y'&= y_1\, y_2  \cdots y_i\, x_{i+1} \cdots x_j\, y_{j+1} \cdots y_n.
\end{align*}

设有两个父代的个体串$A= 0\, 0\,  1\,  1\,  0\,  1$和B=$1\,  1\,  0\,  0\,  1\,  0$, 若随机交叉点为3和5, 则交叉后的两个新的个体是:
\begin{align*}
     A'&= 0 0 1 0 1 1\\
    B'&= 1 1 0 1 0 0.
\end{align*}
\vspace{-0.5cm}
\end{example}

    \ding{175} 多点交叉: 多点交是指先随机生成多个交叉点, 然后再按这些交叉点分段地进行部分基因交换, 生成子代中的两个新的个体.

假设交叉点个数为$m$, 则可将个体串划分为$m+1$个分段, 其划分方法是:

\qquad (I) 当$m$为偶数时, 对全部交叉点依次进行两两配对, 构成$m/2$个交叉段.

\qquad (II) 当$m$为奇数时, 对前$m-1$个交叉点依次进行两两配对, 构成$(m-1)/2$个交叉段, 而第$m$个交叉点则按单点交叉方法构成一个交叉段.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
以$m=3$为例进行讨论多点交叉问题. 假设两个父代的个体串分别是$$X=x_1 x_2 \cdots x_i \cdots x_j \cdots x_k \cdots x_n$$和$$Y=y_1 y_2 \cdots y_i \cdots y_j \cdots y_k \cdots y_n,$$
随机设定第$i,j,k$位为三个交叉点(其中$i<j<k<n$), 则将构成两个交叉段. 交叉后生成的两个新的个体是:
\begin{align*}
  X'&= x_1 x_2 \cdots x_i  \,\,y_{i+1} \cdots y_j \,\,x_{j+1} \cdots x_k \,\,y_{k+1} \cdots y_n\\
  Y'&= y_1 y_2 \cdots y_i\,\, x_{i+1} \cdots x_j\,\, y_{j+1} \cdots y_k \,\,x_{k+1} \cdots x_n.
\end{align*}

设有两个父代的个体串$A= 0\, 0\, 1\, 1\, 0\, 1$和$B= 1\, 1\, 0\, 0\, 1\, 0$, 若随机交叉点为1、3和5, 则交叉后的两个新的个体是:
\begin{align*}
    &A'= 0\,\, 1\, 0\,\, 1\, 0\,\, 0,\\
    &B'= 1\,\, 0\, 1\,\, 0\, 1\,\, 1.
\end{align*}
\vspace{-0.4cm}
\end{example}

\ding{176} 均匀交叉: 均匀交叉(Uniform Crossover)是先随机生成一个与父串具有相同长度, 并被称为交叉模版(或交叉掩码)的二进制串, 然后再利用该模版对两个父串进行交叉, 即\uwave{将模版中1对应的位进行交换, 而0对应的位不交换}, 依此生成子代中的两个新的个体.
事实上, 这种方法对父串中的每一位都是以相同的概率随机进行交叉的.


设有两个父代的个体串$A=001101$和$B=110010$, 若随机生成的模版$T=010011$, 则交叉后的两个新的个体是$A'=011010$和$B'=100101$. 即
\begin{center}
\begin{Verbatim}
    A:   0 0 1 1 0 1
    B:   1 1 0 0 1 0
    T:   0 1 0 0 1 1
    A':  0 1 1 1 1 0
    B':  1 0 0 0 0 1.
\end{Verbatim}
\end{center}

    \ding{177} 实值交叉

实值交叉是在实数编码情况下所采用的交叉操作, 主要包括离散交叉和算术交叉, 下面主要讨论离散交叉(部分离散交叉和整体离散交叉) .

(I) 部分离散交叉是先在两个父代个体的编码向量中随机选择一部分分量, 然后对这部分分量进行交换, 生成子代中的两个新的个体.

(II) 整体交叉则是对两个父代个体的编码向量中的所有分量, 都以1/2的概率进行交换, 从而生成子代中的两个新的个体.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
以部分离散交叉为例, 假设两个父代个体的$n$维实向量分别是 $X=x_1x_2\cdots x_i\cdots x_k\cdots x_n$和$Y=y_1 y_2\cdots y_i \cdots y_k \cdots y_n$, 若随机选择对第$k$个分量以后的所有分量进行交换, 则生成的两个新的个体向量是:
\begin{align*}
  &X'= x_1 x_2 \cdots x_k\,\,\, y_{k+1} \cdots y_n,\\
  &Y'= y_1 y_2 \cdots y_k\,\,\, x_{k+1} \cdots x_n.
\end{align*}

设有两个父代个体向量$A=20\,\,  16\,\,  19\,\,  32 \,\, 18 \,\, 26$和$B=36\,\,  25\,\,  38\,\,  12\,\,  21\,\,  30$, 若随机选择对第3个分量以后的所有分量进行交叉, 则交叉后两个新的个体向量是:
\begin{align*}
  &A'= 20\,\, 16\,\,  19\,\,\,\,  12\,\,  21\,\,  30,\\
  &B'= 36\,\,  25\,\,  38\,\,\,\,  32\,\,  18\,\,  26.
\end{align*}
\vspace{-0.4cm}
\end{example}

 (3) 变异操作

 变异(Mutation)是指对选中个体的染色体中的某些基因进行变动, 以形成新的个体.
 变异也是生物遗传和自然进化中的一种基本现象, 它可增强种群的多样性.

 遗传算法中的变异操作增强了算法的局部随机搜索能力, 从而可以维持种群的多样性. 根据个体编码方式的不同, 变异操作可分为\uwave{二进制变异和实值变异}两种类型.

 \ding{172} 二进制变异

 当个体的染色体采用二进制编码表示时, 其变异操作应采用二进制变异方法. 该变异方法是先随机地产生一个变异位, 然后将该变异位置上的基因值由“0”变为“1”, 或由“1”变为“0”, 产生一个新的个体.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
设变异前的个体为$A=0\, 0 \, 1\,  1\,  0 \, 1$, 若随机产生的变异位置是2, 则该个体的第2位由“0”变为“1”.
变异后的新的个体是$A'= 0\,  1\,  1\,  1\,  0 \, 1$.
\end{example}

\ding{173} 实值变异

 当个体的染色体采用实数编码表示时, 其变异操作应采用实值变异方法. 该方法是用另外一个在规定范围内的随机实数去替换原变异位置上的基因值, 产生一个新的个体. 最常用的实值变异操作有:

(I) 基于位置的变异方法——该方法是先随机地产生两个变异位置, 然后将第二个变异位置上的基因移动到第一个变异位置的前面.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
设选中的个体向量$C=20\,  16\,  19\,  12\,  21\,  30$, 若随机产生的两个变异位置分别是2和4, 则变异后的新的个体向量是:
\begin{align*}
    C'= 20\,  12 \, 16 \, 19\,  21\,  30.
\end{align*}
\vspace{-0.4cm}
\end{example}

(II) 基于次序的变异——该方法是先随机地产生两个变异位置, 然后交换这两个变异位置上的基因.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    设选中的个体向量$D=20\,  12 \, 16 \, 19\,  21\,  30$, 若随机产生的两个变异位置分别是2和4, 则变异后的新的个体向量是:
\begin{align*}
    D'= 20\,  19\,  16\,  12 \, 21 \, 30.
\end{align*}
\vspace{-0.4cm}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    用遗传算法求函数$f(x)=x_2$的最大值, 其中$x\in \mathbb Z, x\in [0,31]$间的整数.
\end{example}

解: 这个问题本身比较简单, 其最大值很显然是在$x=31$处. 但作为一个例子, 它有着较好的示范性和可理解性.

按照遗传算法, 其求解过程如下:

    (1) 编码

    由于$x$的定义域是区间$[0,31]$上的整数, 由5位二进制数即可全部表示. 因此, 可采用二进制编码方法, 其编码串的长度为5.

    \begin{example}
        用二进制串00000来表示$x=0$, 11111来表示$x=31$等. 其中的0和1为基因值.
    \end{example}

    (2) 生成初始种群

    若假设给定的种群规模$N=4$, 则可用4个随机生成的长度为5的二进制串作为初始种群. 再假设随机生成的初始种群(即第0代种群)为:
\begin{align*}
  s_{01}=0\,  1\,  1\,  0\,  1,\, s_{02}=1\,  1\,  0\,  0\,  1,\,  s_{03}=0\,  1\,  0\,  0\,  0,\,  s_{04}=1\,  0\,  0\,  1\,  0.
\end{align*}

 (3) 计算适应度值

要计算个体的适应度, 首先应该定义适应度函数. 由于本例是求$f(x)$的最大值, 因此可直接用$f(x)$来作为适应度函数. 即:
\begin{align*}
    f(s)=f(x),
\end{align*}
其中的二进制串$s$对应着变量$x$的值. 根据此函数, 初始种群中各个个体的适应值及其所占比例如表 \ref{AI_table2019112802} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[H]
\caption{初始种群情况表}
\vspace{-0.6cm}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
编号&个体串(染色体)&	$x$	&适应值&	百分比\%&	累计百分比\%&	选中次数\\
\hline
$S_{01}$&     01101&	13&	169&	14.44&	14.44&	1\\
$S_{02}$&	11001&	25&	625&	52.88&	67.18&	2\\
$S_{03}$&	01000&	8&	64&	5.41&	72.59&	0\\
$S_{04}$&	10010&	18&	324&	27.41&	100&	1\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112802}\vspace{-0.4cm}
\end{table}
可以看出, 在4个个体中$S_{02}$的适应值最大, 是当前最佳个体.

(4) 选择操作

假设采用轮盘赌方式选择个体, 且依次生成的4个随机数(相当于轮盘上指针所指的数)为0.85、0.32、0.12和0.46, 经选择后得到的新的种群为:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{初始种群情况表}
\vspace{-0.6cm}
\begin{center}
\begin{tabular} {lccccccccc}
\hline
    $S_{01}$=01101\\
    $S_{02}$=11001\\
    $S_{03}$=11001\\
    $S_{04}$=10010\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112803}\vspace{-0.4cm}
\end{table}
其中, 染色体11001在种群中出现了2次 ($S_{02}$和$S_{04}$), 而原染色体01000则因适应值太小而被淘汰.

(5) 交叉

假设交叉概率$\pi$为50\%, 则种群中只有1/2的染色体参与交叉.
若规定种群中的染色体按顺序两两配对交叉, 且有$S_{01}$与$S_{02}$交叉, $S_{03}$与$S_{04}$不交叉, 则交叉情况如表 \ref{AI_table2019112804} 所示.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{初始种群的交叉情况表}
\vspace{-0.6cm}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
编号&	个体串(染色体)&	交叉对象	&交叉位&	  子代	& 适应值\\
$S_{01}$	&10010&	$S_{02}$&	3	&10001	&289\\
$S_{02}$	&11001&	$S_{01}$&	3	&11010	&676\\
$S_{03}$	&11001&	$S_{03}$&	N	&11001	&625\\
$S_{04}$	&01101&	$S_{04}$&	N	&01101	&169\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112804}\vspace{-0.4cm}
\end{table}

可见, 经交叉后得到的新的种群为:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{种群情况表}
\vspace{-0.6cm}
\begin{center}
\begin{tabular} {lccccccccc}
\hline
    $S_{01}$=10001\\
    $S_{02}$=11010\\
    $S_{03}$=01101\\
    $S_{04}$=11001\\
\hline
\end{tabular}
\end{center}\vspace{-0.4cm}
\end{table}

 (6) 变异

变异概率$P_m$一般都很小, 假设本次循环中没有发生变异, 则变异前的种群即为进化后所得到的第1代种群. 即:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{种群情况表}
\vspace{-0.6cm}
\begin{center}
\begin{tabular} {lccccccccc}
\hline
    $S_{11}$=10001\\
    $S_{12}$=11010\\
    $S_{13}$=01101\\
    $S_{14}$=11001\\
\hline
\end{tabular}
\end{center}\vspace{-0.4cm}
\end{table}

然后, 对第1代种群重复上述(4)-(6)的操作.

下一步, 对第1代种群, 同样重复上述(4)-(6)的操作. 其选择情况如表 \ref{AI_table2019112807} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{第1代种群的选择情况表}
\vspace{-0.6cm}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
编号	&个体串(染色体)&	 $x$	&适应值	&百分比\%	&累计百分比\%	&选中次数\\
  \hline
$S_{11}$	&10001	&27	&289	&16.43	&16.437	&1\\
$S_{12}$&	11010	&26	&676&38.43	&54.86	&2\\
$S_{13}$&	01101	&13	&169	&9.61	&64.47	&0\\
$S_{14}$	&11001	&25	&625	&35.53	&100	&1\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112807}
\vspace{-0.4cm}
\end{table}
其中若假设按轮盘赌选择时依次生成的4个随机数为0.14、0.51、0.24和0.82, 经选择后得到的新的种群为:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{种群情况表}
\vspace{-0.6cm}
\begin{center}
    \begin{tabular} {lccccccccc}
    \hline
        $S_{11}$=10001\\
        $S_{12}$=11010\\
        $S_{13}$=11010\\
        $S_{14}$=11001\\
    \hline
    \end{tabular}
    \end{center}
    \vspace{-0.4cm}
\end{table}
可以看出, 染色体11010被选择了2次, 而原染色体01101则因适应值太小而被淘汰.

对第1代种群, 其交叉情况如表 \ref{AI_table2019112808} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{第1代种群的交叉情况表}
\vspace{-0.4cm}
\begin{center}
    \begin{tabular} {lccccccccc}
      \hline
    编号	&个体串(染色体)&	交叉对象&	交叉位&	子代&	适应值\\
      \hline
    $S_{11}$	&10001	&$S_{12}$	&3	&10010&	324\\
    $S_{12}$	&11010	&$S_{11}$	&3	&11001&	625\\
    $S_{13}$	&11010	&$S_{14}$	&2	&11001&	625\\
    $S_{14}$	&11001	&$S_{13}$	&2	&11010&	675\\
    \hline
    \end{tabular}
    \end{center}
    \label{AI_table2019112808}
    \vspace{-0.4cm}
\end{table}
可见, 经杂交后得到的新的种群为:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{种群情况表}
    \vspace{-0.4cm}
    \begin{center}
    \begin{tabular} {lccccccccc}
    \hline
    $S_{11}$=10010\\
    $S_{12}$=11001\\
    $S_{13}$=11001\\
    $S_{14}$=11010\\
    \hline
    \end{tabular}
    \end{center}
    \vspace{-0.4cm}
\end{table}
可以看出, 第3位基因均为0, 已经不可能通过交配达到最优解. 这种过早陷入局部最优解的现象称为早熟.
为解决这一问题, 需要采用变异操作.

对第1代种群, 其变异情况如表 \ref{AI_table2019112809} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{第1代种群的变异情况表}
\vspace{-0.4cm}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
编号&	个体串(染色体)	&是否变异&	变异位	&子代&	适应值\\
  \hline
$S_{11}$&	10010&	N&	-	&10010&	324\\
$S_{12}$&	11001&	N&	-	&11001&	625\\
$S_{13}$&	11001&	N&  -	&11001&	625\\
$S_{14}$&	11010&	Y&  -	&11110&	900\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112809}
\vspace{-0.4cm}
\end{table}
它是通过对$S_{14}$的第3位的变异来实现的. 变异后所得到的第2代种群为:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{种群情况表}
\vspace{-0.4cm}
\begin{center}
\begin{tabular} {lccccccccc}
\hline
$S_{21}$=10010\\
$S_{22}$=11001\\
$S_{23}$=11001\\
$S_{24}$=11110\\
\hline
\end{tabular}
\end{center}\vspace{-0.4cm}
\end{table}
接着, 再对第2代种群同样重复上述(4)-(6)的操作:
对第2代种群, 同样重复上述(4)-(6)的操作. 其选择情况如表 \ref{AI_table2019112810} 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{第2代种群的选择情况表}
\vspace{-0.4cm}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
编号	&个体串(染色体)&	$x$	&适应值	&百分比\%	&累计百分比\%	&选中次数\\
  \hline
$S_{21}$&	10010&	18	&324	&23.92	&23.92	&1\\
$S_{22}$&11001&	25	&625	&22.12	&46.04	&1\\
$S_{23}$	&11001&	25	&625	&22.12	&68.16	&1\\
$S_{24}$ &11110 &30	&900	&31.84&	100&1\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112810}
\vspace{-0.4cm}
\end{table}
对于第2代种群, 其交叉情况如表\ref{AI_table2019112810}所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{第2代种群的选择情况表}
\vspace{-0.4cm}
\begin{center}
\begin{tabular} {lccccccccc}
  \hline
编号&	个体串(染色体)&	交叉对象&	交叉位&子代&适应值\\
  \hline
$S_{21}$&	11001	    &$S_{22}$	&3	&11010&	676\\
$S_{22}$&	10010	    &$S_{21}$   &3	&10001&	289\\
$S_{23}$&	11001	    &$S_{24}$	&4	&11000&	576\\
$S_{24}$&	11110	    &$S_{23}$	&4	&11111&	961\\
\hline
\end{tabular}
\end{center}
\label{AI_table2019112810}
\vspace{-0.4cm}
\end{table}
这时, 函数的最大值已经出现, 其对应的染色体为11111, 经解码后可知问题的最优解是在点$x=31$处. 求解过程结束.
其中若假设按轮盘赌选择时依次生成的4个随机数为0.42、0.15、0.59和0.91, 经选择后得到的新的种群为:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} [H]
\caption{种群情况表}
\vspace{-0.4cm}
\begin{center}
\begin{tabular} {lccccccccc}
\hline
$S_{21}$=11001\\
$S_{22}$=10010\\
$S_{23}$=11001\\
$S_{24}$=11110\\
\hline
\end{tabular}
\end{center}
\vspace{-1.4cm}
\end{table}
%%%%%%%%%%----------------------------------------------------------
\section{神经进化}
    神经进化首先需要初始化一组基因组, 然后将它们应用于具体的问题环境中, 然后根据神经网络, 解决应用问题的能力由分配给每个基因组的适应度分数得到.
%%%%%----------------------------------------------------------
\begin{newexam}
    适应度分数可以是图像识别任务中实现的准确度、机械臂移动实际轨迹和预期轨迹的差别等等.
\end{newexam}

%%%%%----------------------------------------------------------
\begin{remark}
    一旦初始种群被创建, 优化循环开始, 种群不断地变异、重组、评估和经历自然选择.
如果这些步骤是迭代进行的, 而整个种群一次只进行一个步骤, 那么所进行的就是代际神经进化(generational neuroevolution).
\end{remark}
%%%%%%%%%%----------------------------------------------------------
\subsection{竞争性共同进化}
竞争性共同进化意味着神经进化算法的设计允许异步性, 并且在每个基因组的基础上执行优化循环.
在代际神经进化和竞争性共同进化两种情况下, 其优化过程都是不间断的进行创新引入、创新评估, 然后对创新进行分类, 直到产生一个最佳实用性神经网络.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{GenerationElvovlilng2020020401.png}
    \caption{典型的代际神经进化过程图解}
    \label{GenerationElvovlilng2020020401}
    \vspace{-0.4cm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
神经进化过程也是一个“黑盒”, 虽然它自己的进化过程需要参数, 但却不为生成的神经网络规定任何特定超参数, 而是根据要解决的实际问题来设计神经网络.
这便为神经网络的权重、超参数等的选择提供了范围, 此范围也称为搜索空间.
虽然“黑盒”性提供了非常广泛的搜索空间, 但是为了提高遍历搜索空间的速度, 明智的做法是限制搜索空间的粒度.
\end{remark}

\begin{mydef}{遗传编码}{1}
    通过限制基因组编码的复杂性, 将基因组映射到搜索空间的粒度的能力也被称为遗传编码.
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
    综上所述, 为了使搜索空间具有适当的粒度, 根据实际问题的要求, 设计遗传编码和相应的神经进化算法非常重要. 先来回顾一下遗传编码的概念.
\end{remark}

有效的神经网络是能够进行有效的变异和重组人工神经网络的前提.
拥有强大表示能力的神经网络不用分析高度复杂的数据结构就能够快速的处理紧凑的遗传密码 (compact genetic codes).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
神经进化算法只在遗传编码上操作, 而不是在机器学习框架中复杂的数据结构上操作.
当然, 基因编码允许这两种表示之间存在映射关系.
基因组的这些有效的遗传表示被称为基因型 (genotypes), 而相应映射的神经网络被称为显型 (phenotypes), 术语来自遗传进化学. 这里将所有显型都限制为神经网络.
\end{remark}

基因编码一般可分为两个子类：\uwave{直接编码和间接编码}.
虽然还有第三类发展性编码, 但这种编码我们先忽略不计.

\ding{171} 直接编码表示神经网络的各个方面, 它们在遗传表示中显式编码 (如图 \ref{IndirectGencode2020020401} 所示).
直接编码直接在基因型中编码每个连接及其相应的权重, 但通过排除神经网络中的偏差和激活函数的可能性来限制搜索空间.
这种直接编码可以表示任意的前馈和递归拓扑, 也能够产生最优的拓扑. 但“拓扑”太灵活的话, 粒度的搜索空间就会变得非常庞大.
因此需要设计良好的神经进化算法才能快速遍历该搜索空间.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{IndirectGencode2020020401.png}
    \caption{表现型间接遗传编码}
    \label{IndirectGencode2020020401}
\end{figure}

\ding{171} 间接编码的功能是规定那些无法直接“翻译”成人工神经网络的自定义编码.
也就是说为了将基因型映射到神经网络, 需要一个由间接编码规定单独的“翻译”能力.
如果间接编码设计得当, 那么即使神经网络非常复杂, 也可以通过搜索空间实现有意义且快速的遍历.

虽然可以从直接编码快速创建人工神经网络, 但是缺少间接编码的翻译能力却会减慢处理速度, 并且可能导致“粗粒度”.
所以在决定使用哪种编码之前, 必须考虑两种编码的优缺点.
但是两种遗传编码都证明了遗传编码如何确定搜索空间的大小,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    通过控制激活函数或某些层类型确定搜索空间.
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
    遍历搜索空间的一种方法被称为繁殖的过程, 这种方法与神经进化所使用的遗传编码密切相关.
    通常通过突变或重组基因组来创造新的基因组, 新的基因组也继承了旧的基因组.
    突变让后代基因组探索人工神经网络新的结构、权重和超参数. 重组基因组本质上是将两个基因组及其独特的特征合并.
突变与遗传编码紧密相关, 因为神经网络的参数只能突变到以遗传编码表示的程度. 因此, 为神经进化算法定义突变有以下三种情况.

1) 遗传编码的哪一部分会发生突变? 是拓扑变化、权重增减、还是超参数调节?

2) 基因组中选定的部分会发生多大程度的突变?

3) 突变采用何种方式, 是定向还是随机?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    神经进化算法可以对低适应度基因组使用较大的突变, 对高性能基因组使用微小的突变.
\end{example}
重组并不会突变基因组, 可以创新性地将两个亲本基因组及其独特特征结合, 并产生“新颖”的后代基因组.
如果重组方法设计得当, 并且可以无损地融合两个亲本基因组的有益特征, 将其在整个群体中传播, 提高所有现有基因组的适应值.
设计重组方法的核心在于“无损融合”, 即不丢失任何基因特性的情况下融合.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    在神经进化算法之前, NEAT算法 (增强拓扑的进化神经网络, Evolving NN through Augmenting Topologies) 利用直接编码修改网络的拓扑结构, 新增节点和删除节点等操作时会产生交叉损失.
\end{example}

如图 \ref{Crossovers2020020402}, 竞合公约问题 (competing-conventions problem) 所示.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Crossovers2020020402.png}
    \caption{竞合公约问题图解}
    \label{Crossovers2020020402}
\end{figure}
NEAT算法提出了一种“历史标记”的方法, 该方法为每个突变提供了唯一的标识符, 从而最终实现了基因组的无损重组, 使其为神经进化算法提供了基准.
\end{remark}

在总体优化循环中, 基于问题来评估基因组似乎是最简单的.
这一步骤确实非常重要, 不仅能够指出潜在的改进和进步.
评估方法从根本上说是一个过程, 即将基因组映射到由其遗传编码规定的神经网络, 并将其应用于问题环境, 然后根据神经网络的表现计算适应值.
一些神经进化算法的评估过程还包括神经网络加权训练的附加步骤, 虽然这种方法非常明智, 但只有当实际环境能够清晰反映基本信息才有用.
在整个评估过程中, 尽管确定适应度的方式完全取决于实际问题的具体情况, 但可以进行合理的修改.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    在图像识别中可以将适应度设置为准确度, 游戏中可以将适应度设置为点数.
    在确定适应度计算时, 新颖性搜索也是一个需要考虑的重要概念.
    因为这个概念涉及用新的方法奖励基因组, 能够使其具有更高的适应值.
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
    电子游戏环境中的智能体如果进入一个未知的区域会获得体能提升, 尽管总体上获得的分数较少, 但也能促进了基因库的创新, 从而促进更有希望的进化.
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{PSO}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{量子细菌觅食算法}
本节将提出的自适应细菌觅食算法应用到了多维函数优化问题和实际优化问题求解. 尽管BFA存在许多自适应策略, 而这些方案大多基于经验得到, 而不是解析推导.
受此启发, 本节给出了基于改进自适应趋药阶段步长方案 的量子细菌觅食算法.
首先, 建立了关于细菌在最优值附近的行为动力学微分方程, 基于迭代次数的趋药阶段步长通过求解微分方程得到.
此外, 还给出了基于迭代次数指标和个体适应值的自适应趋药阶段步长计算方案.

提出的自适应策略能够使细菌个体在寻优初期具有大步长. 在寻优探测结束阶段, 细菌个体的步长将会一直下降直到细菌个体结束生命周期.
SDA 算法在趋药阶段结束启动. 直行阶段不再存在与QBFA算法中, 这一阶段的角色由SDA螺旋算法替代.
通过对不同的策略予以组合, 提出了 QBFA 的四种变种算法.
在八个基本测试函和七个  CEC05 测试函数 (包括偏移类型的测试函数)上验证提出的算法, 并和其他两种自适应细菌觅食算法比较.
基于实验结果, 对算法进行了 Welch t-检验和非参数 Friedman 检验以检验算法的差异性是否显著.
在此基础上, 该算法还被用来建立区间二型模糊逻辑系统和解决优化火电厂脱硝效率问题.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{BFA 模型}\label{TPMsec1}
    BFA 算法包括趋药阶段、直行阶段、蠕行阶段和再生及淘汰驱散四个阶段 .
    在一趋药阶段迭代过程中次, 第$i$个细菌从第 $j$次到  $j+1$次运动的方式能够有下面的公式描述
\begin{align}\label{AdapchemCI}
    \theta^{i}(j+1,k,l)=\theta^{i}(j,k,l)+C(i)\frac{\Delta(i)}{\sqrt{\Delta^{T}(i)\Delta(i)}},
\end{align}
其中 $\theta^{i}(j,k,l)$ 表示第$i$个细菌在第 $j$步趋药阶段、第 $k$步再生阶段 以及第 $l$步淘汰驱散阶段.
$C(i)$ 表示第$i$个细菌在一个随机指定的方向上的步长, $\Delta$ 是随机指定方向上的单位方向向量. 当细菌个体移动时, 吸细菌将会释放引聚集后或者排斥扩散等信息.

BFA 算法以下面的方式模拟这些社会行为
\begin{align}
    J_{cc}&(\theta^{i}, \theta)=\sum_{t=1}^{N_b}J_{cc}^{t}(\theta^{i},\theta)
            = \sum_{t=1}^{N_b}\Big[-d_{atract}\exp(-\omega_{atract}\sum_{m=1}^{N}(\theta_{m}^{i}-\theta_{m}^{t})^2)\Big]\notag \\
&\qquad\qquad+\sum_{t=1}^{N_b}\Big[-d_{repell}\exp(-\omega_{repell}\sum_{m=1}^{N}(\theta_{m}^{i}-\theta_{m}^{t})^2)\Big],
\end{align}
其中 $J_{cc}(\theta^i,\theta)$ 是加入到实际目标函数的一个计算值, 表示一个时变目标函数, $N_b$ 是所有细菌总数, $N$ 是解决问题的搜索空间维数, $d_{atract}$, $\omega_{atract}$, $d_{repell}$, $\omega_{repell}$ 分别是给定的设计系数.
直行阶段结束以后, 细菌个体将会被归类且且会被基于健康程度和适应值大小以一定概率选择进行下一步操作 .
需要指出的是: 种群个数应被设定为偶数. 然而, 算法扩展到技术个细菌的个体也是比较容易的.
具有较低健康程度和适应值较大的一半细菌个体被淘汰, 然后再随机地再生这一半细菌个体.
为了加快找到食物的速度, 算法引入了淘汰驱散阶段, 来加强 BFA 算法的探测和搜索能力.
使用这种策略的BFA 算法以一定概率 将细菌个体在靠近最优解或相应的食物充足的的地方使其重生.
四个阶段一次发生且可以连续循环直到细菌个体达到生命极限周期.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{BFA的自适应步长模型}
最近, Nasir et al. (2015) 提出了两种细菌觅食算法, 简称为 IBFA 并且 FIBFA. 两种算法在趋药性阶段的步长机制定义如下
\begin{align}\label{AdapchemCIBFA}
    C_{IBFA}=\frac {1.5} {0.9\times Iter^{0.9}+1},
\end{align}
\begin{align}\label{AdapchemCF}
     C_{FIBFA}= \frac {2.5} {\frac{0.9\times Iter^{0.9}}{0.2\times|J(i,j,k,l)|^{0.2}}+1},
\end{align}
其中 $Iter=i\times j\times k\times l$ 是迭代步数 $i, j, k$ 并且 $l$的乘积, ($i, j, k$ 并且 $l$ 分别是种群中细菌的编号, 趋药性阶段数, 再生并且淘汰$-$驱散数). $J(i, j, k, l)$ 是当前细菌的适应值.
IBFA 算法中的趋药阶段步长 $C_{IBFA}$ 的计算基于迭代次数, 而 FIBFA算法中的趋药阶段步长 $C_{FIBFA}$ 的计算综合了迭代次数和个体细菌的适应值.
两种算法较之BFA性能都有提高. IBFA 和 FIBFA将作为两个基准测试算法被用于 算法的比较.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{自适应量子细菌觅食算法}
量子细菌觅食算法 最早由见于文献(Huang 和 Zhao 2012). 在本节, 提出了两种混合 SDA算法的新自适应趋药步长 调节方法. 由此得到了自适应量子细菌觅食算法 (AQBFA), AQBFA 的算法流程图见图 \ref{fig:FlowchartAQBFA}.
量子计算原理的机理被用于解码和观测种群个体所在的位置. 量子算法的基本要素称作量子比特或者 Q-比特. 一个 Q-比特是复希尔伯特空间的一个二维单位长向量, 且能够用 Dirac记法表示为:
\begin{gather}
    |\phi\rangle=\alpha|0\rangle+\beta |1\rangle,
\end{gather}
其中 $|0\rangle$ 和 $|1\rangle$ 是Q-比特的基本态, $\alpha$ 和  $\beta$称作状态 $|\phi\rangle$ 的概率幅, 且满足条件 $\alpha^2+\beta^2=1$.
Q-比特定义为单位复向量 $(\alpha,\beta)^T$ 是量子计算中的最小单位, $|\alpha|^2$ 和 $|\beta|^2$ 给出的概率幅由 Q-比特的状态 '0' 和 '1' 状态.
此外, $N$-维空间中一个Q-比特位置由多个量子位组成.
对于BFA算法, 第 $j$趋药步骤、第 $k$再生阶段和第$l$淘汰驱散阶段的第$i$个细菌Q-比特表示如下
\begin{align}
\begin{bmatrix}
    \alpha_{i1}(j,k,l) &  \alpha_{i2}(j,k,l)  &  \cdots&  \alpha_{iN}(j,k,l)\\
    \beta_{i1}(j,k,l)&  \beta_{i2}(j,k,l)&  \cdots &  \beta_{iN}(j,k,l)\\
\end{bmatrix},
\end{align}
其中 $|\alpha_{in}(j,k,l)|^2+|\beta_{in}(j,k,l)|^2=1$, $n=1, 2$, $\cdots, N$, 并且 $N$ 表示搜索空间的维度.
Q-比特位置能够同时表示 $2^N$ 个状态. 3-Q-比特的表示方法如下
\begin{align*}
\begin{bmatrix}
\frac{\sqrt{3}}{2} &  \frac{1}{3}&  \frac{\sqrt{2}}{2}\\
\frac{1}{2}&  \frac{2\sqrt{2}}{3}&  -\frac{\sqrt{2}}{2}\\
\end{bmatrix}.
\end{align*}
上述状态的描述如下
\begin{align}
  \frac{1}{24}&|0 0 0\rangle-\frac{1}{24}|0 0 1\rangle+\frac{1}{3}|0 10\rangle-\frac{1}{3}|0 11\rangle
    +\frac{1}{72}|100\rangle-\frac{1}{72}|1 01\rangle+\frac{1}{9}|1 10\rangle-\frac{1}{9}|1 11\rangle.
\end{align}
这种 Q-比特 表示方式的优点在于能够以概率表示线性纠缠态, 共有8个位置且每个位置对应的有一个概率值.

对于量子计算原理, 量子门 被用来探索搜索空间. 新得到的 Q-比特位置能够通过量子门的旋转得到 . 具体的操作由下式表示.
 \begin{align}
\left [
\begin{array}{c}
\alpha_{in}^{'}\\
\beta_{in}^{'}\\
\end{array}\right]
&=\left [\begin{array}{cc}
\cos(\Delta\theta_{in}) &  -\sin(\Delta\theta_{in}) \\
\sin(\Delta\theta_{in})&  \cos(\Delta\theta_{in}) \\
\end{array}\right]
\left [\begin{array}{c}
\alpha_{in}\\
\beta_{in}\\
\end{array}\right]
=U(\Delta\theta_{in})
\left [\begin{array}{c}
\alpha_{in}\\
\beta_{in}\\
\end{array}\right],\label{rotationgate}
\end{align}
其中 $[\alpha_{in},\beta_{in}]^T$ 代表当前 Q-比特位置, $U(\Delta\theta_{in})$ 代表量子旋转们, $\Delta\theta_{in}$ 是旋转角度, 并且 $[\alpha_{in}^{'}, \beta_{in}^{'}]^T$ 是新的 Q-比特 位置, 由量子旋转门求得.
本文的  $\Delta\theta_{in}$ 由 第$i$个细菌个体的当前位置 和 种群当前最优个体的位置决定.
种群的初始 Q-比特位置具有如下形式:
\begin{equation}
    P_{Q}(t)=\{P_{Q_1}(t), P_{Q_2}(t),\cdots,P_{Q_{Nb}}(t)\}^T,\label{PQt}
\end{equation}
其中 $P_{Q_{i}}(t)$ 由下式计算
\begin{gather}
P_{Q_i}(t)=\left [\begin{array}{cccc}\label{init_Qindividual}
\cos(\theta_{i1}) &  \cos(\theta_{i2})  &  \cdots&  \cos(\theta_{iN})\\
\sin(\theta_{i1})&  \sin(\theta_{i2})&  \cdots &  \sin(\theta_{iN})\\
\end{array}\right],
\end{gather}
其中 $\theta_{in}\in[0,\pi/2]$, $n=1,2, \cdots, N$, $i=1,2,\cdots, N_b$, 并且 $N_b$ 记种群数. 种群由下式描述:
\begin{align}\label{measure_ppi}
    P_{P}(t)= \{P_{P_1}(t), P_{P_2}(t),\cdots,P_{P_{Nb}}(t)\}^T,
\end{align}
其中 $P_{P_i}(t)$ 按如下定义
\begin{align*}
&P_{P_i}(t)=
\left [
\begin{array}{cccc}
\cos^2(\theta_{i1})\\ \cos^2(\theta_{i2})\\ \vdots\\ \cos^2(\theta_{iN})\\
\end{array}
\right]^T\otimes UB^T
+\left [
\begin{array}{cccc}
\sin^2(\theta_{i1})\\ \sin^2(\theta_{i2})\\ \vdots\\ \sin^2(\theta_{iN} \big)\\
\end{array}
\right]^T\otimes LB^T,
\end{align*}
其中 $\otimes$为 两个向量的阿达马积, LB 和 UB分表为相应决策变量的下界和上界 .
上述表示方式能够保证结果寻优的可能性和种群的多样性.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{量子BFA算法}
接下来介绍 QBFA 算法的流程(2012).

[步骤 0] 初始化参数 $N$, $N_b$, $\theta_{i\cdot}$, $N_c$, $N_s$, $N_{re}$, $N_{ed}$, $P_{ed}$, 和 $C_{QBFA}$.
表 \ref{Para_QBFASys20150225} 列出了上述符号的具体含义.

[步骤 1] 对Q-比特位位置观测结果是 $P_Q (t)$, 得到的初始种群为 $P_P(t)$.

[步骤 2] 淘汰-再生阶段循环: 对每一个 $l=1$,$\cdots$, $N_{ed}$.

[步骤 3] 再生阶段循环: 对每一个 $k=1,\cdots,N_{re}$.

[步骤 4] 趋药阶段循环:  对每一个 $j=1,\cdots,N_{c}$.

\quad\quad (a) \textcolor[rgb]{0,0,1}{For} 第$i$个细菌 (初始值$i=1$), 根据当前位置, 由量子门的顺时针或逆时针方向旋转更新 Q-比特编码的个体 .

\quad\quad (b) 计算适应值 $J(i,j,k,l)$, 保存当期最优解 $J_{last}$.

\quad\quad (c) 计算趋药阶段步长 $C_{QBFA}$ 并按如下方式生成新的种群个体位置
\begin{align}
    \theta^i(j+1,k,l)=&\theta^i(j,k,l)+C_{QBFA} \times(\theta^b(j,k,l)-\theta^i(j,k,l)),
\end{align}
其中 $\theta^i(j,k,l)$ 表示第$i$个细菌在$j$趋药阶段第 $k$个再生阶段及第 $l$个淘汰驱散阶段 的位置, $\theta^b(j,k,l)$ 表示种群在当前找到的问题全局最优值.

\quad\quad (d) Swim:

\quad\quad \quad i) 令$m=0$ 并且计算 $J(i,j+1,k,l)$.

\quad\quad \quad ii) 若 $m<N_s$ 并且 $J(i, j+1, k, l)<J_{last}$, 令 $J_{last}=J(i, j+1, k, l)$, 由下式生成新的位置
\begin{align}
  \theta^i(j+1,k,l)=&\theta^i(j+1,k,l)+C_{QBFA}
  \times(\theta^b(j,k,l)-\theta^i(j,k,l)).
\end{align}
令 $m=m+1$, 并且转到步骤 ii).

\quad\quad \quad iii)\,\, \textcolor[rgb]{0,0,1}{Else, if} $J_{last}<J_{best}$, 更新 $J_{best}$ 和 $P_{best}$, 其中 $J_{best}$ 表示种群 个体在当前找到的最优适应值, $P_{best}$ 表示在当前最优适应值下的位置.

\quad\quad (e) 若 $i<N_b$, 令 $i=i+1$, 并且转到步骤 (a).

[步骤 5] 若 $j<N_c$, 令 $j=j+1$, 并且转到步骤 4.

[步骤 6] 再生阶段的步骤:

\quad\quad (a) 令 $J_{health}^i$ 表示第$i$个细菌在 $k$ 和 $l$ 阶段的健康程度, 计算第$i$个细菌 $(i=1, 2, \cdots, N_b)$的健康程度.
对细菌个体的健康程度$J_{health}$以升序方式排序 (适应值低代表细菌有更高的健康程度).

\quad\quad (b) 具有较好适应值的$S_r$ 个体被保留并且有机会产生下一代细菌个体.

[步骤 7] 若 $k<N_{re}$, 令 $k=k+1$, 并且转到步骤 3.

[步骤 8] 淘汰驱散阶段: 以概率 $P_{ed}$淘汰并且驱散每一个种群中的个体 $i=1,2,\cdots, N_b$. 也即, 若一个细菌个体被淘汰, 使用量子操作直接在优化区域内产生一个新位置给新个体实.
若 $l<N_{ed}$, 令 $l=l+1$ 并且 算法将会转到步骤 2, 否则算法结束.

对于QBFA算法, 趋药性阶段步长大小采用了 Dasgupta et al. (2009) 的方法:
\begin{align}
    C_{QBFA}=\frac{|J(i,j,k,l)|}{|J(i,j,k,l)|+\lambda},
\end{align}
其中 $\lambda$ 是给定正常数且 $\lambda=4000$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\begin{center}
\caption{QBFA参数设置}
\vspace{0.5em}
\centering
\begin{tabularx}{0.72\textwidth}{p{1.05cm}p{9.16cm}}
\hline
Vars &  Explanation\\
\hline
$N$&    问题的搜索空间维度\\
$N_b$&  种群中细菌的总数\\
$\theta_{i\cdot}$&  细菌个体在生命周期内趋向最优解过程中的旋转角度\\
$N_{c}$&  趋药阶段数\\
$N_s$&  Swimming length\\
$N_{re}$&  再生阶段数\\
$N_{ed}$&  淘汰-驱散阶段数\\
$P_{ed}$&  淘汰-驱散度量值\\
$C_{QBFA}$&   QBFA算法在趋药阶段的步长\\
\hline
\end{tabularx}
\label{Para_QBFASys20150225}
\end{center}
\vspace{-0.5cm}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{迭代自适应步长设计}
受到趋药阶段的自适应步长应该在初始阶段较大而在结束阶段较小这一规律的启示. 本节给出一个自适应步长的统一设计框架.
首先, 引入了基于迭代次数的自适应趋药阶段自适应步长函数. 对于函数 $c: X=[1, +\infty) \rightarrow (0,1)$, 其中 $X$ 表所有迭代次数值所在区间.
自适应趋药阶段自适应步长定义如下
\begin{align}\label{CIAQBFA1}
    C_{IAQBFA}=\frac {c_{\min}}{c(x)},
\end{align}
其中 $c_{\min}$ 算法中 最小步长. 令 $\lim_{x\rightarrow +\infty}c(x)=1$,
对于同一 $x$, $\Delta c$ 与 $\Delta x$ 成比例. 显然, 当$c\approx 1$时, $\Delta c$ 应该变小.
新加入一个阶次参数 $\alpha$来调整$(1-c)$, 由此得到 $(1-c)^\alpha$. 则有下面的微分方程:
\begin{eqnarray}
    \Delta c = k_1\, (1-c)^\alpha\,\Delta x/ x,\,\alpha \in \mathbb R^+,
\end{eqnarray}
其中 $k_1$ 是一个正常数. 令 $\Delta x \rightarrow 0$, 方程改写为
\begin{eqnarray}\label{QBFA_diffSys1}
    \frac{d c}{d x} = k_1\, (1-c)^\alpha/x.
\end{eqnarray}
求解方程 \eqref{QBFA_diffSys1} 得到
\begin{align}\label{QBFA_diffSys_alphax}
  c(x)= \left\{
  \begin{array}{ll}
    1- \left[c_1-k_1(1-\alpha)\ln x \right]^{\frac 1 {1-\alpha}},&  \alpha\neq 1,\\
    1-\frac {c_1} {x^{k_{1}}},&  \alpha= 1.
  \end{array}
  \right.
\end{align}
初始条件为 $c(1)=c_{\min}$ ($0<c_{\min}\ll 1$), 下是成立
\begin{align}
  c_1=\left\{
  \begin{array}{ll}
  (1-c_{\min})^ {1-\alpha},&  \alpha\neq 1,\\
  1-c_{\min},&  \alpha= 1.
  \end{array}
  \right.
\end{align}
则方程 \eqref{QBFA_diffSys_alphax} 具有如下形式
\begin{align}
c(x)=
\left\{
  \begin{array}{ll}
  1- \left[(1-c_{\min})^{{1-\alpha}}-k_1(1-\alpha)\ln x \right]^{\frac 1 {1-\alpha}},&  \alpha\neq 1,\\
  1-\frac{1-c_{\min}} {x^{k_1}},&  \alpha= 1,
  \end{array}
  \right.
\end{align}
其中变量 $x$ 是迭代次数 $Iter=i\times j\times k \times l$. 注意到 $c(x)$ 是关于$x$的非线性递增函数, $c(1)=c_{min}$ 并且 $\lim_{x\rightarrow+\infty}c(x)=1$.
此时的趋药阶段步长 $C_{IAQBFA}$ 由\eqref{CIAQBFA1}定义, 它是关于$x$的 非线性递减函数, 当 $x=1$, $C_{IAQBFA}=1$,且 $\lim_{x\rightarrow+\infty}C_{IAQBFA}=c_{min}$.
这即趋药阶段步长在开始迭代阶段 是 1,  当迭代次数增加, 趋药阶段步长逐渐逼近 $c_{\min}$. 算法中的趋药阶段步长为
\begin{align}\label{IAQBFA0}
    C_{IAQBFA}=\frac{c_{\min}}{1-\frac{1-c_{min}}{x^{k_{1}}}},
\end{align}
当 $\alpha=1$, 类似于 (\ref{AdapchemCI}) 的趋药阶段步长公式(Nasir et al. 2015). 当 $\alpha> 1$, 算法中的趋药阶段步长定义为
\begin{align}\label{IAQBFA}
    C_{IAQBFA}=\frac{c_{\min}}{1- \left[(1-c_{\min})^{{1-\alpha}}-k_1(1-\alpha)\ln x \right]^{\frac 1 {1-\alpha}}}.
\end{align}
图 \ref{AdaptQBFA:fig1} 给出了自适应趋药阶段步长函数 $C_{IAQBFA}$, 其中 $\alpha=2$, $c_{\min}=6\textup{E-}3$, $k_1=1\textup{E-}2$, 且 趋药阶段步长公式为 \eqref{AdapchemCI}.
从图中能够看出, 曲线的形状较为类似且两种自适应步长在迭代初始阶段的下降速度快于后期, 而 \eqref{IAQBFA} 式的自适应步长在迭代初期的下降速度快于式 \eqref{AdapchemCI} 的速度, 后期正好相反.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{适应值自适应步长设计}
文 (Dasgupta et al. 2009)的分析揭示出 将趋药阶段的适应值作为一个因素加以考虑, 以细菌的适应值定义的自适应步长函数表现好于固定步长情形.
当细菌靠近最优解, 趋药阶段的自适应步长函数应该较小. 基于式(\ref{IAQBFA0}) 和(\ref{IAQBFA})定义的趋药阶段自适应步长, 引入了基于适应值的趋药阶段自适应步长函数.
趋药阶段的自适应步长函数定义如下
\begin{align}\label{FAQBFA}
C_{FAQBFA}
 =
\left\{
  \begin{array}{ll}
  \frac{c_{\min}}{1-\left[(1-c_{\min})^{1-\alpha}-k_1(1-\alpha)\frac{\ln x}{|J(i,j,k,l)|^{\beta}}\right]^{\frac{1}{1-\alpha}}},&  \alpha\neq 1,\\
  \frac{c_{\min}}{1-(1-c_{\min})\frac{|J(i,j,k,l)|^{\beta}}{x^{k_1}}},&  \alpha= 1,
  \end{array}
  \right.
\end{align}
其中变量$x$ 来自迭代数 $Iter=i\times j\times k \times l$, $k_1$, $\alpha$, $\beta$ 并且 $c_{\min}$ 是正常数, $c_{\min}$ 是 算法中趋药阶段步长的下界.
$C_{FAQBFA}$ 的设计要满足 $C_{FAQBFA}$ 是递减的且 当 $|J(i,j,k,l)|$ 变小或者 $x$ 很大时趋于 $c_{\min}$;
当 $\alpha\neq 1$时, $C_{FAQBFA}$ 在初始状态为 1. $C_{FAQBFA}$ 的设计依赖于 细菌 个体的适应值和当前迭代次数.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!tbp]
\begin{center}
    \includegraphics[width=0.6\textwidth]{AdaptQBFA.pdf}
    \caption{迭代次数下的趋药阶段步长}
    \label{AdaptQBFA:fig1}
    \vspace{-0.5em}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[tb]
    \begin{center}
     \includegraphics[height=0.75\textwidth]{AQBFA_SDAFlowChart}
      \caption{AQBFA流程图}
      \label{fig:FlowchartAQBFA}
      \vspace{-0.4cm}
    \end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[!htb]
\begin{center}
\caption{比较用基准测试函数}
\label{tab:test2}
\vspace{0.5em}\centering
\begin{tabularx}{0.98\textwidth}{p{1.5cm}p{8.5cm}p{2.45cm}p{0.5cm}}
\hline
Name&   Objective function   	&   Range of search    &   $f_{min}$ 	 \\
\midrule
Sphere&  $f_1(x)=\sum^{N}_{i=1}x^2_i$    &   $[-100,100]^N$&  $0$\\
Quadric&  $f_2(x)=\sum^{N}_{i=1}(\sum^i_{j=1}x_j)^2$&  $[-100,100]^N$&  $0$\\
Rosenbrock&  $f_3(x)=\sum_{i=1}^{N-1} [100(x_{i+1}-x_i^2)^2 + (x_i-1)^2]$  &  $[-100,100]^N$ &   $0$\\
Quartic &  $f_4(x)=\sum_{i=1}^{N}ix_i^4+rand[0, 1]$&  $[-1.28,1.28]^N$&  $0$\\
Rastrigin&  $f_5(x)=\sum_{i=1}^{N}(x_i^2-10\cos(2\pi x_i))+10$&  $[-5.12,5.12]^N$&  $0$\\
Ackley&  $f_{6}(x) = -20 \exp(-0.2 \sqrt{ \frac{1}{N} \sum_{i=1}^{N}{x_i^2}}+ 20 + e - \exp( \frac{1}{30} \sum_{i=1}^{N} \cos{(2\pi x_i)})  $ &   $[-32,32]^N$ &   $0$\\
Griewank&  $f_{7}(x)=\frac{1}{4000}\sum_{i=1}^{N}{x_i^2}-\prod_{i=1}^{N}\cos(\frac{x_i}{\sqrt{i}})+1$
&  $[-600,600]^N$&  $0$\\
\hline
\end{tabularx}
\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[!htb]
\renewcommand\arraystretch{.2}
\begin{center}
\caption{比较用CEC05基准测试函数}
\label{Tab:CEC05}
\vspace{0.5em}\centering
\begin{tabularx}{1.\textwidth}{p{4.5cm} p{12.5cm}}
\hline
Name &  Objective function \\
\hline
Shifted Sphere &  $f_8(x)=\sum_{i=1}^N z_i^2+f_{bias}, z=x-\bm o$\\
Shifted Schwefel's Problem 1.2 &  $f_{9}(x)=\sum_{i=1}^N \sum _{j=1}^i z_j^2+f_{bias}, z=x-\bm o$\\
Shifted Rosenbrock's Function &  $f_{10}(x)=\sum_{i=1}^{N-1} (100(z_i^2-z_{i+1})^2+(z_i^2-1)^2 )+f_{bias}$\\
Shifted Rotated Griewank&   $f_{11}(x)=\frac{1}{4000}\sum_{i=1}^{N}{z_i^2}-\prod_{i=1}^{N}\cos(\frac{z_i}{\sqrt{i}})+1+f_{bias}$, $z=(x-\bm o)\ast M$\\
Shifted Rotated Ackley &   $f_{12}(x)=-20\exp(-0.2 \sqrt{\frac{1}{N}\sum_{i=1}^{N}{z_i^2}})+20$\\
                         &\qquad +$e+f_{bias} -\exp(\sum_{i=1}^N\cos(2\pi z_i))$, $z=(x-\bm o)\ast M$\\
Shifted Rastrigin &  $f_{13}(x)=\sum_{i=1}^{N}(z_i^2-10\cos(2\pi z_i)+10)+f_{bias}, z=x-\bm o$\\
Shifted Rotated  Rastrigin&  $f_{14}(x)=(1+0.1\times rand[-1,1])\sum_{i=1}^N x_i^2$ $+f_{bias}$\\
\hline
\end{tabularx}
\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{螺旋动力学算法}
受到自然界自螺旋现象的启发, Tamura 和 Yasuda (2011) 提出了一种新的 称作是 SDA 的启发式算法 .
算法首先界定一个中心点 , 其他点以一定角度绕着该中心旋转且与到中心的距离成比例. 以中心点在圆点的二维优化问题为例, 旋转动力学方程定义如下
\begin{align}\label{SDAEquation120150221}
  \begin{bmatrix}
    x_1^{s+1}\\
    x_2^{s+1}
  \end{bmatrix}
  &  =
    \begin{bmatrix}
    \alpha_1&  -\beta_1\\
    \beta_1& \alpha_1
  \end{bmatrix}
    \begin{bmatrix}
        x_1^{s}\\
        x_2^{s}
  \end{bmatrix}\notag
  = r\begin{bmatrix}
    \cos\theta &  -\sin\theta\\
    \sin\theta&  \cos\theta
  \end{bmatrix}\cdot
    \begin{bmatrix}
        x_1^{s}\\
        x_2^{s}
  \end{bmatrix}\\
  &:=A_{spiral}\,\bm x^s,\quad s=0,1,\cdots.
\end{align}
其中 $r=\sqrt{\alpha_1^2+\beta_1^2}$, 并且 $\theta=\arctan(\beta_1/\alpha_1)$.
应保证条件 $\beta_1\neq 0$ 和 $r < 1$成立, 其中条件 $\beta_1\neq 0$ 保证解在$x_1 - x_2$平面的螺旋旋转路径上, 并且条件 $r < 1$ 能够保证由 (\ref{SDAEquation120150221}) 生成的序列从任一点收敛到圆点.
对任意$r$ 和$\theta$, 在搜索阶段的后期, 所有的候选搜索点将会聚集在螺旋轨道上.

对于 $n$-维优化问题和任给初始点 $\bm x=[x_1 ,x_2,\cdots, x_n]$, 其中 $n$ 设定为偶数, $\bm x$ 写成 $ 2\times n/2$ 以便 \eqref{SDAEquation120150221} 运算, 运算后得到向量形式的结果. SDA 模型描述如下
\begin{align}
    \bm x^{s+1}=A_{spiral}\bm x^s-(A_{spiral}-I_n)\bm x^*,
\end{align}
其中 $\bm x^{*}$ 是螺旋线的中心.

SDA 以其强大的寻优能力得到广泛的应用. 对于搜索空间, 所要搜索的位置使用了螺旋运动轨迹来执行探索和搜寻策略.
这种策略能够保证搜索点对应的搜索位置的广度和密度. 该模型中的所有搜索点在经过若干次迭代后都将向最优点移动.
SDA能够模拟了个体的直行行为. 受到SDA这些性质的启发, BFA 算法中的传统直行行为采用了 SDA 算法的模式.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{自适应量子细菌觅食算法}
基于量子计算原理, 本节提出了两种带SDA 算法的自适应趋药性自适应算法, 称为 AQBFA和AQBFA, 量子原理被用来 编码量子运算原理的细菌, 采用自适应趋药步长且运动步长 使用SDA计算.
AQBFA 的流程图见图 \ref{fig:FlowchartAQBFA}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[tb]
\begin{center}
\caption{比较用CEC05基准测试函数参数($\bm o=o_1 \cdot I+o_2\cdot \textup{rand}(1,n)$)}
    \vspace{0.5em}\centering
\begin{tabularx}{0.9\textwidth}{p{6.0cm} p{1.05cm}p{0.75cm}p{1.05cm}p{4.2cm}}
\hline
Name &  $o_1$&  $o_2$&   $f_{bias}$& Range of search\\
\hline
Shifted Sphere &  -100&  200&  -450 &  $x\in [-100,100]^N$\\
Shifted Schwefel's Problem 1.2 &  -100&  200&  -450 &   $x\in [-100,100]^N$\\
Shifted Rosenbrock's Function &  -100&  200&  390 &   $x\in [-100,100]^N$\\
Shifted Rotated Griewank &  -&  -&  -180 &  No bounds\\
Shifted Rotated Ackley&  -5&  10&  -140 &  $x\in [-32,32]^N$\\
Shifted Rastrigin &  -100&  200&  -450 &  $x\in [-5,5]^N$\\
Shifted Rotated  Rastrigin&  -&  -&  -330 &  $x\in [-5,5]^N$\\
\hline
\end{tabularx}
\label{Tab:CEC05ofbias}
\end{center}
\vspace{-0.4cm}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{经典函数仿真实例}
基于两种新自适应趋药阶段步长方案和第三节 AQBFA, 提出了两种新细菌觅食算法的改进AQBFA模型(IAQBFA 和 FAQBFA). IAQBFA 和 FAQBFA 是使用式 \eqref{IAQBFA} 和 \eqref{FAQBFA}自适应趋药阶段步长方案的改进型 AQBFA. 本节给出了 IAQBFA、FAQBFA、 QBFA、 IBFA 以及FIBFA (Nasir et al. 2015)的比较结果. 算法中使用的参数设置为 $N_b=50, N_c=100, N_s=N_{re}=4, N_{ed}=2$和 $P_{ed}=0.25$. 自适应趋药阶段步长采用式 \eqref{IAQBFA} 和式 \eqref{FAQBFA}, 参数设定为 $\alpha=2$, $c_{\min}=6E-3$, $k_1=1E-2$和 $\beta=0.2$. SDA 的参数为$\alpha_1=\beta_1=0.67$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{经典测试函数}
为了测试五种算法的性能, 本节挑选了两类测试函数集, 基本测试函数集和  CEC05 测试函数集.基本测试函数集 (Yao et al. 1999) 和  CEC05 基本测试函数集 (Suganthan et al. 2005) 具体内容见表 \ref{tab:test2} 和表 \ref{Tab:CEC05}. 这两类测试函数及被广泛用于进化计算领域算法的适应值精度和收敛速度 (Chen et al. 2014; Xu 并且 Chen 2014; Nasir et al. 2015).

对于表\ref{tab:test2}, 第一个函数是 Sphere 函数 ($f_1$), 它是一个可微可分单峰函数. 二次函数 ($f_2$) 和 Rosenbrock 函数 ($f_3$) 是可微不可分单峰函数. 二次函数 ($f_4$) 是一个不连续可分带一个随机项的单峰函数. Rastrigin 函数 ($f_5$) 和 Ackley 函数 ($f_6$) 是可微多峰函数, 其中 $f_5$ 有许多局部极值, 而 $f_6$ 有许多局部极小值和一个较小的全局最值区域 . Griewank 函数 ($f_7$) 是一个可微不可分多峰函数. 对于这七个测试函数, $f_1$ 易于求解而 $f_7$ 难于找到全局最值点, 多峰函数较之单峰函数更难找到最值点. 在本节的验中, 使用30, 45和 60维的测试函数. 这些关于函数路 $f_1-f_7$的定义域和全局最值点 $f_{min}$ 也列于表中, $t$函数$f_1-f_7$ 的最值点全为0.

CEC05 测试函数集$f_8$-$f_{14}$ 见表 \ref{Tab:CEC05}, 这些函数带有偏移和旋转, 其中的 $f_8$-$f_{10}$, $f_{13}$ 是偏移类型而 $f_{11}$, $f_{12}$, $f_{14}$ 是对经典测试函数做了偏移和旋转操作. 函数$f_8$-$f_{12}$ 是多峰函数, $f_{13}$ 和 $f_{14}$ 是单峰函数. 这类测试函数比基本测试函数更难于求解. 表 \ref{Tab:CEC05} 给出的参数 $M$ 是一个给定条件数的正交矩阵, $\bm o$ 记偏移全局极值, $\bm o$的值和偏移 $f_{bias}$ 列于表 \ref{Tab:CEC05ofbias}. 旋转问题由偏移变量 $(x-\bm o)$ 左乘矩阵 $M$得到.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{测试函数结果}
五种算法均针对给定维数独立运行 30 次 . 最优适应值的均值和标准差(Std) 有计算结果平均得到. 双边 t-检验 用于比较 IAQBFA、 FAQBFA 和其它算法显著性差异. 实验使用了 Garc\'{\i}a et al. (2009) 提出的非参数统计技术, 分别用了Wilcoxon 符号秩检验和 Friedman 检验. 上述三种检验方法的参数显著性水平 $\alpha=0.05$.

对于七个测试函数, 五种 算法的30 次结果的均值, 标准差和 t-检验值列于表 \ref{TableDifBFA}. 对于均值, 五种算法针对各种维度情况作了排序. 五种算法的均值和标准差的最优值使用粗体标出. 从表 \ref{TableDifBFA} 中可以看出, 算法 FAQBFA 在函数 $f_1$, $f_2$, $f_3$, $f_5$上有最好均值, 且在 $f_7$ 测试函数的三个维度问题中都是最好, 算法 QBFA 在$f_4$上最好均值, 且在  $f_6$ 测试函数的三个维度问题中都是最好. 算法 FAQBFA, IAQBFA 和 QBFA 表现好于 IBFA 和 FIBFA. 表中数据还显示: 除了 QBFA 在 60-维$f_6$ 函数的测试结果, 有最优均值的算法也有最优标准差. 对于60-维函数 $f_6$, IAQBFA 得到了最好的标准差. 对于表 \ref{TableDifBFA}, t-值表示 算法的t-检验值和IAQBFA 和 FAQBFA 最优值比较结果, 表 \ref{TableDifBFA} 还给出了对应的 95\% 置信区间和 $p$ 值. $p$ 值能够提供统计检验是否显著的信息. 从表 \ref{TableDifBFA} 可以看出, FAQBFA 的均值小于IAQBFA的均值. 对于双边 t-检验, FAQBFA和IAQBFA 的最优值和其余四种算法做了比较, 算法 FAQBFA和其余四种算法与 IAQBFA 做了 t-检验. 对于 65样本的 95\% 置信水平及 p-值列于表 \ref{TableDifBFA} 中, 其中的60 结果显示出 FAQBFA 相较其余四种算法有显著不同.

五种算法的适应度值也做了非参数 Wilcoxon 符号秩检验和 Friedman 检验, 两种检验的显著性水平 $\alpha=0.05$. 类似于 t-检验, Wilcoxon 符号秩检验用于检验五种算法是否存在显著差异. 表 \ref{TableDifBFA} 给出了 Wilcoxon 符号秩检验的结果, 算法 IAQBFA和 FAQBFA和其他算法分别做了比较. 表 \ref{TableDifBFA} 的 $p$ 值均小于 0.05. 这意味着 FAQBFA和其他四中算法有显著性的差异; IAQBFA 和其他四中算法有显著性的差异. 基于表 \ref{TableDifBFA} 数据的Friedman 检验结果见表 \ref{Friedman_test}. 值得一提的是,由黑体字代表的排名靠前的算法是 FAQBFA, 升序排名结果为 QBFA, IAQBFA, FIBFA 和 IBFA. 总而言之, 对于七个基本测试函数, FAQBFA 是精度最好的算法. 这也说明与适应值和迭代次数有关的自适应机制 \eqref{FAQBFA} 很有效.

下只针对 30-维 CEC05 测试函数集进行试验. 表 \ref{tab:CEC05results} 列出了五种算法 30次结果的均值和方差. 就适应值精度而言, IAQBFA 在 一个函数中的$f_8-f_{11}$和 $f_{14}$ 上达到最优, FAQBFA 在$f_{12}$上达到最优. 显然IAQBFA 算法 在 CEC05 测试函数集上好于基本测试函数集. 对七个 CEC05 测试函数集的非参数 Wilcoxon 符号秩检验和 Friedman 检验结果见表 \ref{tab:CEC05results}. Wilcoxon 符号秩检验和 Friedman检验见表 \ref{TableDifBFA} 和表 \ref{Friedman_test} 中括号数据. 表 \ref{TableDifBFA} 给出的 $p$ 值小于 0.05. FAQBFA 和其他四种算法有显著区别, IAQBFA 和其他四种算法的有显著区别. 对于表 \ref{Friedman_test}, IAQBFA 排序最低, 其他依次为 QBFA, FAQBFA, FIBFA 和 IBFA. 显然, 对于测试函数集, 就适应值函数而言, IAQBFA 是最好的算法, 算法 FAQBFA 在CEC05的表现不如基本测试函数集. 部分缘于 CEC05 测试函数的全局最优点不再是0, 基于自适应步长 \eqref{IAQBFA} 和迭代次数的算法优于单独使用一种策略的算法.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\onecolumn
\vspace{0.5em}
\begin{longtable}[H]{p{0.3cm}p{1.2cm}p{0.3cm}p{2.45cm}p{2.45cm}p{0.5cm}p{1.2cm}p{4.5cm}p{1.5cm}}
\caption{$f_1-f_{7}$ 函数在不同维度下平均运行30次下均值和标准差\label{TableDifBFA}}
\vspace{0.5em}\\
        \hline
        函数  &  算法 & 维数	&   均值    &   标准差 & 排序&  $t$-值 &  95\% 置信区间 &  $p$   \\
        \hline
    \endfirsthead
        \caption{(续表)}\\
        \hline
         函数  &  算法 & 维数	&   均值    &   标准差 & 排序&  t-值 &  95\% 置信区间 &  $p$   \\
        \hline
    \endhead
        \hline
    \endfoot
        \hline
    \endlastfoot
$f_{1}$ &IBFA &30 &3.5413e-4 &6.6200e-5&5 &29.300 &(3.2941e-4, 3.7885e-4) & $<0.05$ \\
             & &45 &3.7595e-4 &7.0705e-5 &5 &29.123 &(3.4955e-4, 4.0235e-4) &$<0.05$  \\
             & &60 &3.7358e-4 &6.9194e-5 & 5&29.572 &(3.4774e-4, 3.9942e-4) &$<0.05$ \\
 &FIBFA &30 &8.9029e-5 &1.6503e-6& 4 &295.480 &(8.8413e-5, 8.9645e-5) &$<0.05$  \\6
             & &45 &8.9102e-5 &5.3927e-7 &4 &9049.05 &(8.9082e-5, 8.9122e-5) &$<0.05$  \\
             & &60 &8.9099e-5 &5.3997e-7 &4 &9036.61 &(8.9065e-5, 8.9105e-5) &$<0.05$  \\
 &QBFA &30 & 2.3175e-7 &3.7625e-7 &2  &3.373 &(9.1234e-8, 3.7223e-7) &$<0.05$\\
             & &45 &5.1032e-7  &8.6408e-7  &2  &3.235 &(1.8766e-7, 8.3297e-7) &$<0.05$\\
             & &60 &8.2765e-7 & 1.7586e-6 & 3 &2.578  &(1.7096e-7, 1.4843e-6) &$<0.05$\\
 &IAQBFA &30 &3.6095e-7 &7.3809e-7& 3&2.678 &(8.5329e-8, 6.3655e-7) &$<0.05$  \\
             & &45 &5.7802e-7 &1.0229e-7 &3&3.095 &(1.9607e-7, 9.5995e-7) &$<0.05$  \\
             & &60 &7.1193e-7 &1.2472e-7 & 2&3.126 &(2.4621e-7, 1.1776e-6) &$<0.05$  \\
 &FAQBFA &30 & $\bm{1.5866e-11}$ &$\bm{3.7492e-11}$ & $\bm{1}$&-- &-- &--  \\
             & &45 & $\bm{1.0399e-11}$ &$\bm{1.9515e-11}$  &$\bm{1}$ &-- &-- &--  \\
             & &60 & $\bm{1.0264e-11}$ & $\bm{1.4813e-11}$ & $\bm{1}$&-- &-- &--  \\
%\midrule
$f_{2}$ &IBFA &30 &2.0624e-3 &4.6954e-4& 4&24.059 &(1.8871e-3, 2.3778e-3) &$<0.05$  \\
             & &45 &2.1890e-3 &5.1702e-4 & 4&23.190 &(1.9959e-3, 2.3821e-3) &$<0.05$  \\
             & &60 &2.1848e-3 &3.5932e-4 & 4&33.299 &(2.0506e-3, 2.3190e-3) &$<0.05$  \\
 &FIBFA &30 &9.8003e-3 &5.8860e-4&5 &91.197 &(9.5805e-3, 1.0020e-2) &$<0.05$  \\
             & &45 &9.6411e-3 &5.1859e-4 & 5&101.828 &(9.4475e-3, 9.8347e-3) &$<0.05$  \\
             & &60 &9.9143e-2 &7.9466e-4 &5 &68.333 &(9.6175e-3, 1.0211e-2) &$<0.05$  \\
  &QBFA &30 & 8.1484e-5 &1.3448e-4 & 2 &3.319  &(3.1265e-5, 1.3170e-4) &$<0.05$\\
             & &45 & 7.0798e-4 &1.5044e-3  &3  &2.578  &(1.4622e-4, 1.2697e-3) &$<0.05$\\
             & &60 & 7.4818e-4 &1.1024e-3  &3  &3.717  &(3.3651e-4, 1.1598e-3) &$<0.05$\\
 &IAQBFA &30 &9.3802e-5 &3.0022e-4 & 3&1.711 &(-1.8304e-5, 2.0590e-4) &0.098  \\
             & &45 &2.9594e-4 &7.0099e-4  &2 &2.312 &(3.4172e-5, 5.5768e-4) &$<0.05$ \\
             & &60 &6.1194e-4 & 1.1555e-3 & 2&2.900 &(1.8043e-4, 1.0434e-3) &$<0.05$  \\
 &FAQBFA &30 &$\bm{1.9573e-09}$  &$\bm{2.4119e-9}$  &$\bm{1}$ &-- &-- & -- \\
             & &45 &$\bm{1.1519e-8}$  &$\bm{1.6307e-8}$  &$\bm{1}$ &-- &-- &--  \\
             & &60 &$\bm{3.9203e-8}$  & $\bm{1.0969e-7}$ &$\bm{1}$ &-- &-- &--  \\
%\midrule
$f_{3}$ &IBFA &30 &2.9123 &7.4076e-3 &4 &2153.348  &(2.9095 , 2.9150 ) &$<0.05$  \\
             & &45 &2.9089   &6.8679e-3  &4 &2319.845 &(2.9063 , 2.9115 ) &$<0.05$  \\
             & &60 &2.1848e-3  &6.8679e-4  & 4&33.304 &(2.0506e-3, 2.3190e-3) &$<0.05$  \\
 &FIBFA &30 &3.3996   &1.1144e-01 &5 &167.089 &(3.3579 , 3.4412 ) &$<0.05$  \\
             & &45 &3.4173   &1.5301e-01  &5 &122.326 &(3.3602 , 3.4745 ) &$<0.05$  \\
             & &60 &3.3795   &1.0354e-01  &5 &178.774 &(3.3409 , 3.4182 ) &$<0.05$  \\
 &QBFA &30 & 3.3631e-6 &5.7352e-6 &3  &3.212  &(1.2215e-6, 5.5045e-6) &$<0.05$\\
             & &45 &5.2958e-6  & 1.7822e-5 & 3 &1.628  &(-1.3565e-6, 1.1953e-5) &0.114\\
             & &60 &4.9209e-6  & 1.4274e-5 &2  &1.888  &(-4.0932e-7, 1.0251e-5) &0.069\\
 &IAQBFA &30 &1.5292e-6 &1.4311e-6 &2 &5.852 &(9.9473e-7, 2.0635e-6) & $<0.05$ \\
             & &45 &2.1302e-6 &2.5689e-6  &2 &4.542 &(1.1708e-6, 3.0892e-6) &$<0.05$ \\
             & &60 &5.2821e-6 &9.1432e-6  &3&3.164 &(1.8678e-6, 8.6961e-6) &$<0.05$ \\
 &FAQBFA &30 & $\bm{7.9035e-11}$ & $\bm{1.5299e-10}$ &$\bm{1}$ &-- &-- &--  \\
             & &45 &$\bm{2.3508e-10}$  & $\bm{6.2157e-11}$  &$\bm{1}$ &-- &-- &--  \\
             & &60 &$\bm{1.2115e-10}$ &$\bm{2.7715e-10}$ &$\bm{1}$ &-- &-- &--  \\
%\midrule
$f_{4}$ &IBFA &30 &3.3103e-2  &2.2559e-2 &5 &8.028 &(2.4602e-2, 4.1425e-2) &$<0.05$  \\
             & &45 &2.7716e-2  &2.3251e-2  &5 &6.508 &(1.8928e-2, 3.6278e-2) &$<0.05$  \\
             & &60 &2.6459e-2  &1.5084e-2  &4 &9.593 &(2.0747e-2, 3.1991e-2) &$<0.05$ \\
 &FIBFA &30 &2.6220e-2  &2.0288e-2 & 4&7.070 &(1.8572e-2, 3.3690e-2) &$<0.05$ \\
             & &45 &2.4181e-2 & 2.1077e-2 &4 &6.253 &(1.6196e-2, 3.1940e-2) &$<0.05$  \\
             & &60 & 2.9481e-2 &2.1111e-2  &5&7.628 &(2.1509e-2, 3.7270e-2) &$<0.05$ \\
  &QBFA &30 &$\bm{4.3413e-5}$ &$\bm{4.6293e-5}$ &$\bm{1}$  &-2.230  &(-8.8632e-5, -3.8403e-6) &$<0.05$ \\
             & &45 &$\bm{3.2872e-5}$  &$\bm{2.3298e-5}$  &$\bm{1}$ &-3.318  &(-1.2976e-4, -3.0799e-5) &$<0.05$\\
             & &60 &$\bm{4.1447e-5}$  &$\bm{3.1898e-5}$ &$\bm{1}$  &-2.235  &(-9.5008e-5, -4.2188e-6) &$<0.05$ \\
 &IAQBFA &30 &9.3498e-5 &7.6371e-4 &3 &0.185 &(-3.8663e-5, 4.6360e-5) &0.854  \\
             & &45 &1.1315e-4 & 1.3038e-4& 2&-- &-- &--  \\
             & &60 &2.7219e-4 & 3.5023e-4 &3 &2.571 &(3.7051e-5, 3.2520e-4) &$<0.05$ \\
 &FAQBFA &30 &8.9649e-5 & 9.4650e-5 & 2&-- &-- &--  \\
             & &45 &1.3110e-4 & 1.4823e-4  &3 &0.496 &(-5.6020e-5, 9.1922e-5) &0.623  \\
             & &60 & 9.1061e-5 &1.1655e-4   & 2&-- &-- & --  \\
%\midrule
$f_{5}$ &IBFA &30 &7.1197e-2  &1.3745e-2 &5&28.372 &(6.6064e-2, 7.6329e-2) &$<0.05$  \\
             & &45 & 7.5477e-2 &1.3621e-2  &5 &30.350 &(7.0391e-2, 8.0563e-2) &$<0.05$  \\
             & &60 &7.1408e-2  & 1.3332e-2 &5&29.336 &(6.6429e-2, 7.6386e-2) &$<0.05$  \\
 &FIBFA &30 &1.2425e-2  & 4.2583e-4&4&159.810 &(1.2266e-2, 1.2584e-2) &$<0.05$ \\
             & &45 &1.2520e-2  &4.4689e-4  &4 &153.452 &(1.2353e-2, 1.2687e-2) &$<0.05$  \\
             & &60 &1.2465e-2  & 4.4736e-4 &4&152.610 &(1.2298e-2, 1.2632e-2) &$<0.05$  \\
  &QBFA &30 & 2.6115e-7 & 4.8446e-7& 2 &2.952  &(8.0246e-8, 4.4205e-7) &$<0.05$ \\
             & &45 &1.5156e-7  &2.2865e-7  & 2 &3.630  &(6.6173e-8, 2.3693e-7) &$<0.05$ \\
             & &60 &1.1075e-7  &1.4497e-7  & 2 &4.184  &(5.6612e-8, 1.6488e-7) &$<0.05$\\
 &IAQBFA &30 &7.8065e-7 & 1.3103e-6&3 &3.263 &(2.9139e-7, 1.2699e-6) &$<0.05$  \\
             & &45 &6.4217e-7 & 1.0582e-6 & 3&3.327 &(2.4756e-7, 1.0378e-6) &$<0.05$  \\
             & &60 &5.7578e-7 &1.1905e-6  & 3&2.649 &(1.3123e-7, 1.0203e-6) &$<0.05$ \\
 &FAQBFA &30 & $\bm{3.7961e-12}$ &$\bm{7.6552e-12}$  &$\bm{1}$&-- &-- &--  \\
             & &45 &$\bm{5.7661e-12}$  &$\bm{8.0501e-12}$   &$\bm{1}$ &-- &-- &--  \\
             & &60 &$\bm{1.0303e-11}$  &$\bm{2.6130e-11}$   &$\bm{1}$ &-- &-- &--  \\
%\midrule
$f_{6}$ &IBFA &30 &4.9306e-3 &4.8296e-4& 5&54.045 &(4.6265e-3, 4.9905e-3) &$<0.05$ \\
             & &45 &4.9734e-3 &5.6882e-4 &5&47.224 &(4.6768e-3, 5.1002e-3) &$<0.05$ \\
             & &60 &5.0100e-3 &5.0475e-4 &5 &52.385 &(4.7251e-3, 5.1091e-3) &$<0.05$  \\
 &FIBFA &30 &2.2437e-3 &1.4948e-5&4 &105.882 &(2.0805e-3, 2.1625e-3) &$<0.05$ \\
             & &45 &2.2489e-3 &1.1776e-5 &4 &143.584 &(2.1332e-3, 2.1949e-3) &$<0.05$ \\
             & &60 &2.2480e-3 &1.4413e-5 &4&163.006 &(2.1280e-3, 2.1821e-3) &$<0.05$  \\
 &QBFA &30 & $\bm{8.1851e-5}$ & $\bm{7.1877e-5}$&$\bm{1}$ &-2.318  &(-7.5860e-5, -4.7460e-6) &$<0.05$\\
             & &45 &$\bm{5.6487e-5}$  &$\bm{7.2439e-5}$  & $\bm{1}$ &-1.326  &(-7.2238e-5, 1.5423e-5) &0.195 \\
             & &60 & $\bm{7.9332e-5}$ & 8.4479e-5 & $\bm{1}$ &-0.829  &(-4.7155e-5, 1.9945e-5) &0.414 \\
 &IAQBFA &30 &1.2215e-4 & 1.0597e-4&2 &-- &-- &--  \\
             & &45 &8.4895e-5 & 8.3114e-5 &2&-- &-- &--  \\
             & &60 &9.2937e-5 &$\bm{6.7836e-5}$ & 2&-- &-- &--  \\
 &FAQBFA &30 &9.2354e-01  &1.4084  &5 &3.591 &(3.9751e-1, 1.4493 ) &$<0.05$ \\
             & &45 &1.5493   &1.6228   &5 &5.229 &(9.4329e-1, 2.1552 ) &$<0.05$ \\
             & &60 &1.9786   &1.7065   & 5&6.350 &(1.3413 , 2.6157 ) &$<0.05$  \\
%\midrule
$f_{7}$ &IBFA &30 &2.4732e-5 &4.0951e-6& 5&33.078 &(2.3202e-5, 2.6261e-5) &$<0.05$  \\
             & &45 &2.5503e-5 &3.6765e-6 &5 &37.994 &(2.4130e-5, 2.6876e-5) &$<0.05$  \\
             & &60 &2.5941e-5 &2.9350e-6 &5 &48.410 &(2.4845e-5, 2.7037e-5) &$<0.05$ \\
 &FIBFA &30 &5.2693e-6 &4.4101e-10&4 &failed &failed &failed  \\
             & &45 &5.2693e-6 &4.9711e-10 & 4&failed &failed &failed  \\
             & &60 &5.2693e-6 &3.8092e-10 & 4&failed &failed &failed  \\
  &QBFA &30 & 7.3604e-7 &1.4991e-6 & 2 &2.689  &(1.7624e-7, 1.2958e-6) &$<0.05$ \\
             & &45 &1.1326e-6  &1.1604e-6  & 3 &3.782  &(5.2011e-7, 1.7451e-6) &$<0.05$ \\
             & &60 &1.0036e-6  &2.1783e-6  & 3 &2.523  &(1.9017e-7, 1.8169e-6) &$<0.05$\\
 &IAQBFA &30 &8.6690e-7 & 1.9908e-6&3 &2.385 &(1.2348e-7, 1.6103e-6) &$<0.05$  \\
             & &45 &6.4657e-7 & 1.8035e-6 &2 &1.964 &(-2.6892e-8, 1.3199e-6) &0.059  \\
             & &60 &7.1698e-7 & 1.3082e-6  &2 &3.002 &(2.2846e-7, 1.2055e-6) &$<0.05$ \\
 &FAQBFA &30 &$\bm{2.5345e-11}$  &$\bm{4.8562e-11}$  & $\bm{1}$&-- &-- &--  \\
             & &45 &$\bm{1.6706e-11}$ &$\bm{4.2225e-11}$   &$\bm{1}$ &-- &-- &--  \\
             & &60 &$\bm{1.5648e-11}$ &$\bm{2.8642e-11}$  & $\bm{1}$&-- &-- &--
%\midrule
\end{longtable}
%\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!htb]
\begin{center}
\caption{表 \ref{TableDifBFA} 并且表 \ref{tab:CEC05results} 的Wilcoxon秩检验结果(基于表 \ref{tab:CEC05results} 的结果列于括号内)}
\vspace{0.5em}\centering
\begin{tabularx}{\textwidth}{p{3.9cm}p{1.8cm}p{1.8cm}p{2.8cm}p{3.9cm}}
\hline
算法&   $R^{+}$   	&   $R^{-}$   &   $p$ 值 &是否改进\\
\midrule
IBFA vs. IAQBFA & 626 (210)   & 4 (0)   & $<0.05$ ($<0.05$)  &YES (YES)\\
FIBFA vs. IAQBFA &626 (210)     & 4  (0)  & $<0.05$ ($<0.05$) &YES (YES) \\
QBFA vs. IAQBFA & 274 (174)   & 356  (36)  &$<0.05$  ($<0.05$)  &YES (YES)\\
IBFA vs. FAQBFA &552 (210)     & 78  (0)  &$<0.05$ ($<0.05$)  &YES (YES)\\
FIBFA vs. FAQBFA &549  (210)    & 81  (0)  &$<0.05$  ($<0.05$) &YES (YES)\\
QBFA vs. FAQBFA & 480 (102)    & 150 (108)   &$<0.05$ ($<0.05$)  &YES  (YES)\\
IAQBFA vs. FAQBFA &507 (11)     &123 (199)   &$<0.05$ ($<0.05$)  &YES (YES)\\
\hline
\end{tabularx}
\label{Friedman_test}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[tb]
\begin{center}
\caption{基于表 \ref{TableDifBFA} 并且表  \ref{tab:CEC05results} 的Friedman检验结果(基于表 \ref{tab:CEC05results} 的结果列于括号内)}
\vspace{0.5em}\centering
\begin{tabularx}{0.62\textwidth}{p{2.cm}p{2.5cm}p{2.5cm}p{2.5cm}}
\hline
算法    & 均值(排序) &$\chi^2$   &$p$ 值\\
\midrule
IBFA &4.50 (4.98 )
&\multirow{6}{2.5cm}{1627.91 (735.02) }
&\multirow{6}{2.5cm}{$<0.05$ ($<0.05$)}\\
  FIBFA&4.21 (3.98 ) \\
  QBFA& 2.21 (2.35 )  \\
  IAQBFA &2.38 ($\bm{1.22}$) \\
FAQBFA& $\bm{1.69}$ (2.46 ) \\
\hline
\end{tabularx}
\label{Friedman_test2}
\end{center}
\vspace{-0.4cm}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure*}[htpb]
%\begin{center}
%\includegraphics[width=0.4\columnwidth]{AlgoCompareFitness1}
%\includegraphics[width=0.4\columnwidth]{AlgoCompareFitness3}
%\includegraphics[width=0.4\columnwidth]{AlgoCompareFitness5}
%\includegraphics[width=0.4\columnwidth]{AlgoCompareFitness7}
%\includegraphics[width=0.4\columnwidth]{AlgoCompareFitness9}
%\includegraphics[width=0.4\columnwidth]{AlgoCompareFitness10}
%\includegraphics[width=0.5\columnwidth]{AlgoCompareFitness11}
%  \bicaption[DifBFA：fig4-3]{}{$f_1-f_7$ 测试函数在算法 IBFA, FIBFA, QBFA, IAQBFA 和FAQBFA下的收敛速度}{Fig$.\!$}{Comparison of convergence plots of $f_1-f_7$ for IBFA, FIBFA, QBFA, IAQBFA and FAQBFA}
%\end{center}
%\label{fig4}
%\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htb]
\caption{在CEC05基准测试函数上30算法结果}
\vspace{0.5em}\centering
\begin{tabularx}{0.96\textwidth}{p{1cm}p{1cm}p{1.6cm}p{1.5cm}p{1.75cm}p{2.15cm}p{1.5cm}}
\hline
函数&  &  IBFA &  FIBFA&  QBFA&  IAQBFA &FAQBFA\\
\midrule
$f_8$&  Mean &8909.4279       &8650.6850   & -449.4366  &$\bm{-449.5195}$   & -232.4032\\
 &  Std&  3.1271      &41.8928     &  0.1476     & $\bm{0.1374}$  &50.7970\\
 &  Rank&5    &4   & 2  & $\bm{1}$&3 \\
$f_{9}$&  Mean   &8909.6041   &8653.9868  &-449.3659  &$\bm{-449.4842}$  &-225.0580\\
 &  Std&2.9858   &46.9612  & 0.1701   & $\bm{0.1517}$  &62.7650\\
 &  Rank&5   &4  &2   &$\bm{1}$  &3   \\
$f_{10}$&  Mean&8992.9453   &8721.3704   & 390.5751    &$\bm{390.4915}$   &650.1049\\
 &  Std&3.3611   &50.0396  & 0.1969 &$\bm{0.1305}$   &75.5439\\
 &  Rank&5   & 4  &2& $\bm{1}$ &3  \\
$f_{11}$&  Mean&8935.4329   &8674.7632   &6689.7910   &$\bm{4656.6614}$   &5911.8439\\
 &  Std&2.3743   &41.3210  &6524.5539   &$\bm{43.0837}$   &483.1981\\
 &  Rank&5  &4   & 3   & $\bm{1}$  &2   \\
$f_{12}$&  Mean&8939.5689&8681.5180   & 6656.4715   &-119.0426   &$\bm{-119.0299}$\\
 &  Std&2.7076   &46.1616  &9923.6429  &$\bm{0.0447}$   &0.0953\\
 &  Rank&5   &4  &3   &2   &$\bm{1}$    \\
$f_{13}$&  Mean&8920.2839   &8676.8111   &10283.4198   &$\bm{-309.3002}$    &-166.5421\\
 &  Std&3.5596   &52.9493  & 13693.1746  &$\bm{0.0959}$   &27.6014\\
 &  Rank&4   & 3  & 5   &$\bm{1}$  &2   \\
$f_{14}$&  Mean&8919.5089   &8663.0473   &9677.1638  &$\bm{-309.2905}$  &-164.1773\\
 &  Std&3.0473  &45.7693  &11164.5167 &$\bm{0.0815}$   &25.2951\\
 &  Rank&4   &3  &5   &$\bm{1}$   &2 \\
\hline
\end{tabularx}
\label{tab:CEC05results}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

提出了一种新的基于量子编码原理和自适应趋药性步长并混合 SDA算法的细菌觅食算法. 首先利用微分方程建立细菌个体在近最优点附近的行为模式, 通过求解方程得到基于迭代次数的自适应趋药阶段步长. 此外, 提出了基于迭代次数和当前适应值的自适应趋药阶段步长设计方案. 自适应步长设计方案会根据适应值动态调整, 使得细菌个体的寻解能力得到提高. BFA 的直行行为被 SDA算法取代. 通过组合这些不同的寻优策略, 提出了两种 QBFA改进算法. 提出的 QBFA 变种算法和 QBFA, IBFA 和 FIBFA 等在七个标准测试函数和 CEC05测试集中的 七个标准测试函数上做了验证测试. 算法的比较了适应度精度和 收敛速度. 使用双边 t-检验和非参数 Wilcoxon 符号秩检验来检验算法之间的是否存在显著差异. Friedman 检验用于给五种算法排序. 数值结果和图形均显示, 对于基本测试函数, FAQBFA 在适应值寻优方面和收敛速度方面要显著好于 QBFA, IAQBFA, IBFA 和 FIBFA. 对于 CEC05 测试函数集, IAQBFA 是最好的算法. 关于 IAQBFA 和 FAQBFA的研究将会集中在多目标离散优化问题及复杂实际优化约束问题上.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{其它算法*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{鱼群算法}\index{鱼群算法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{鸟群算法}\index{鸟群算法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{象群算法}\index{象群算法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{蚁群算法}\index{蚁群算法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{狼群算法}\index{狼群算法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{蝙蝠算法}\index{蝙蝠算法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{随机森林}\index{随机森林}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\begin{center}
%\tikzset{marrow/.style={midway,red,sloped,fill, minimum height=3cm, single arrow, single arrow
%    head extend=.5cm, single arrow head indent=.25cm,xscale=0.15,yscale=0.15,
%    allow upside down}}
%\scalebox{.45}{
%\begin{forest}
%for tree={l sep=3em, s sep=3em, anchor=center, inner sep=0.7em, fill=blue!50,
%circle, font=\Large\sffamily,where level=1{no edge}{}}
%  [Training Data, draw, rectangle, rounded corners, orange, text=white,alias=TD
%    [,red!70,alias=a1[[,alias=a2][]][,red!70,edge label={node[above=1ex,marrow]{}}[[][]][,red!70,edge label={node[above=1ex,marrow]{}}[,red!70,edge label={node[below=1ex,marrow]{}}][,alias=a3]]]]
%    [,red!70,alias=b1[,red!70,edge label={node[below=1ex,marrow]{}}[[,alias=b2][]][,red!70,edge label={node[above=1ex,marrow]{}}]][[][[][,alias=b3]]]]
%    [~$\cdots$~,scale=4,no edge,fill=none,yshift=-1em]
%    [,red!70,alias=c1[[,alias=c2][]][,red!70,edge label={node[above=1ex,marrow]{}}[,red!70,edge label={node[above=1ex,marrow]{}}[,alias=c3][,red!70,edge label={node[above=1ex,marrow]{}}]][,alias=c4]]]
%  ]
%\node[draw,fit=(a1)(a2)(a3)](f1){};
%\node[draw,fit=(b1)(b2)(b3)](f2){};
%\node[draw,fit=(c1)(c2)(c3)(c4)](f3){};
%\path (f1.south west)--(f3.south east) node[midway,below=4em] (David) {mean};
%\node[below=2em of David] (pred){prediction};
%\foreach \X in {1,2,3}{\draw[-stealth] (TD) -- (f\X.north);
%\draw[-stealth] (f\X.south) -- (David);}
%\draw[-stealth] (David) -- (pred);
%\end{forest}
%}
%\caption{随机森林}
%\label{AIRForestfig105}
%\end{center}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{ADP聚类算法}
%\href{https://www.sciencedirect.com/science/article/pii/S0020025518303967}{A method for autonomous data partitioning}, 2018.8 \cite{GuAngelov2018-5667}
%
%\href{https://github.com/Gu-X/Autonomous-Data-Partitioning-Algorithm}{ADP python}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{sidewaystable}[!ht]
%\caption{Training and testing error on Auto-Mpg, Bank, Diabetes and Triazines.}
%\begin{center}
% \begin{tabular}{lp{10cm}lcccccccc}
%\hline
%%\multirow{2}{*}{Data set}&\multirow{2}{*}{Algorithm}&\multicolumn{2}{c}{Training (RMSE)}&\multicolumn{2}{c}{Testing (RMSE)}\\
%%\cline{3-4}\cline{5-6}
% &  &  Web Link  \\
%\hline
%Codes &ADP Algorithm &\small{\url{http://empiricaldataanalytics.org/downloads.html}}\\
%&Pre-trained VGG-VD-16 convolutional neural network [46]&\small{\url{http://www.vlfeat.org/matconvnet/pretrained/}}\\
%Datasets &PIMA [47] &\small{\url{https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes}}\\
%&Banknote Authentication [37] &\small{\url{https://archive.ics.uci.edu/ml/datasets/banknote+authentication}}\\
%&S1 [23] &\small{\url{http://cs.joensuu.fi/sipu/datasets/}}\\
%&S2 [23] &\small{\url{http://cs.joensuu.fi/sipu/datasets/}}\\
%&Cardiotocography [9] &\small{\url{https://archive.ics.uci.edu/ml/datasets/cardiotocography}}\\
%&Pen-Based Handwritten Digits Recognition [2] &\small{\url{https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+
%Handwritten+Digits}}\\
%&Steel Plates Faults [14] &\small{\url{http://archive.ics.uci.edu/ml/datasets/steel+plates+faults}}\\
%&Multiple Features [30] &\small{\url{https://archive.ics.uci.edu/ml/datasets/Multiple+Features}}\\
%&Occupancy Detection [16] &\small{\url{https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+}}\\
%&MAGIC Gamma Telescope [13] &\small{\url{https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope}}\\
%&Letter Recognition [24] &\small{\url{https://archive.ics.uci.edu/ml/datasets/letter+recognition}}\\
%&Singapore [26] &\small{\url{http://icn.bjtu.edu.cn/Visint/resources/Scenesig.aspx}}\\
%&Caltech 101 [22] &\small{\url{http://www.vision.caltech.edu/Image_Datasets/Caltech101/}}\\
%&MNIST [35] &\small{\url{http://yann.lecun.com/exdb/mnist/}}\\
%\hline
%\end{tabular}
%\end{center}
%\label{TT2-ELM170610:Sec5-3-1}
%\end{sidewaystable}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\href{https://ww2.mathworks.cn/matlabcentral/fileexchange/67463-autonomous-data-partitioning-algorithm?s_tid=prof_contriblnk}{autonomous data partitioning algorithm (ADP)}
%宏包包括: 1) 最近发表的ADP算法; 2) 离线数据划分演示; 3) 在线数据划分演示; 4) 离线和在线版本算法的混合示例.
%
%Reference: X. Gu, P. Angelov and J. Principe, “A Method for Autonomous Data Partitioning,” Information Sciences, 2018, DOI:10.1016/j.ins.2018.05.030.
%
%For any queries about the codes, please contact Prof. Plamen P. Angelov (p.angelov@lancaster.ac.uk) and Mr. Xiaowei Gu (x.gu3@lancaster.ac.uk)
%
%The proposed ADP algorithm has an offline version and an evolving version. Its offline version is based on the ranks of the data samples in terms of their densities and mutual distances instead of the commonly used means and variances. Ranks are very important but other approaches avoid them because they are nonlinear and discrete operators. Therefore,
%the offline version is more stable and effective in partitioning static datasets. The evolving version is also generic and fully
%data-driven. It can continue the offline partitioning process with the newly arrived data, but can also start without initial
%conditions.
%
%Lifting these assumptions is possible with information theoretic \cite{JenssenErdogmus2007-5673} and spectral clustering \cite{vonLuxburg2007-5674}, but at the expense of much higher computational cost.
%
%The proposed ADP approach is proposed within the recently introduced Empirical Data Analysis (EDA)\index{Empirical Data Analysis} framework \cite{Angelov2014Outside,Angelov2016-7844219,Angelov2017Empirical}.
%EDA resembles statistical learning in its nature but is free from the range of assumptions made in traditional probability theory and statistical learning approaches \cite{Angelov2014Outside,Angelov2016-7844219,Angelov2017Empirical}. EDA measures play an instrumental role in the proposed ADP approach for extracting the ensemble properties from the observed data (see Fig. 1(b)), and frees the ADP algorithm from the requirement to make prior assumptions on the data generation model and user- and problem-specific parameters (see Fig. 1(a)). Most importantly, they guarantee objectiveness of the partitioning results.
%
%Firstly, we define the data set/stream in the Euclidean data space $\mathbb R^N$ as $\{\bm x\}_K = \{\bm x_1, \bm x_2, \cdots, \bm x_K\}$, $\bm x_k =[x_{k, 1}, x_{k, 2}, \cdots, x_{k, N}]^T \in \mathbb R^N$, where $k = 1, 2, \cdots, K$ is an order index. We, further, consider that some data samples within the data set/stream can have the same value, i.e. $\exists \bm x_k = \bm x_j, k\neq j$. The set of sorted unique data samples is denoted as $\{\bm u\}_{L_K} =\{\bm u_1, \bm u_2, \cdots, \bm u_{L_K}\},\bm u_j = [u_{j,1}, u_{j,2}, \cdots, u_{j,N}]^T ( j = 1, 2, \cdots, L_K), \{\bm u\}_{L_K} \subseteq \{\bm x\}_K , L_K \leq K$ with the corresponding occurrence frequencies
%$\{f\}_{L_K} = \{ f_1, f_2, \cdots, f_{L_K}\},\sum\limits^{L_K}_{k=1} f_k = 1$.
%
%In this paper, we use the natural metric of the space of samples (i.e. Euclidean distance) for clarity, however, various types of distances within the data space, $\mathbb R^N$ can be considered as well. In the remainder of this paper, all the derivations are conducted at the $K$th time instance except when specifically mentioned.
%
%In this section, we will summarize the nonparametric EDA measures that we use in ADP:
%
%I) local density, $D$ \cite{Angelov2016-7844219,Angelov2017Empirical};
%
%II) global density, $D^G$ \cite{Angelov2016-7844219,Angelov2017Empirical};
%
%and their recursive implementations to make the paper self-contained.
%
%
%3.1. Local density
%
%Local density \cite{Angelov2016-7844219} is a measure within EDA framework identifying the main local mode of the data distribution and is derived empirically from all the observed data samples in a parameter-free way. The local density, $D$ of the data sample $\bm x_k$ is expressed as follows $(k = 1, 2, \cdots, K; L_K > 1)$ \cite{Angelov2016-7844219,Angelov2017Empirical}:
%\begin{align}\label{EDAlocaldensity18072401}
%    D_K(\bm x_k) =\frac{\sum^K_{j=1}\sum^K_{l=1} d^2(\bm x_j,\bm  x_l)} {2K \sum^K_{l=1} d^2(\bm x_k,\bm  x_l)}
%\end{align}
%where $d(\bm x_k, \bm x_l)$ is the distance between data samples $\bm x_k$ and $\bm x_l$; the coefficient 2 is used in the denominator for normalization (because each distance is counted twice in the numerator).
%
%For the case of Euclidean distance, the calculation of $\sum^K_{l=1} d^2(\bm x_k,\bm  x_l)=\sum^K_{l=1}\|\bm x_k-\bm  x_l\|^2$ and $\sum^K_{j=1}\sum^K_{l=1} d^2(\bm x_j,\bm  x_l)=\sum^K_{j=1}\sum^K_{l=1}\|\bm x_j-\bm  x_l\|^2 $ can be simplified by using the mean of $\{\bm x\}_K$, $\bm \mu_K$ and the average scalar product, $X_K$ 定义为 \cite{Angelov2016-7844219,Angelov2017Empirical}:
%\begin{align}
%  &\sum^K_{l=1}\|\bm x_k-\bm  x_l\|^2=K(\|\bm x_K-\bm \mu_K\|^2+X_K-\|\bm \mu_K\|^2)\label{EDAlocaldensity18072402}\\
%  &\sum^K_{j=1}\sum^K_{l=1}\|\bm x_j-\bm  x_l\|^2 =2K^2(X_K-\|\bm \mu_K\|^2)\label{EDAlocaldensity18072403}
%\end{align}
%$\bm \mu_K$ 和 $X_K$ 分别按如下更新律\cite{Angelov2012Autonomous}更新:
%\begin{align}
%  &\bm \mu_k=\frac {k-1} k \bm \mu_{k-1}+\frac {1} k \bm  x_k, \bm \mu_1=\bm x_1, k=1,2,\cdots, K\label{EDAlocaldensity18072404}\\
%  &X_k=\frac {k-1} k X_{k-1}+\frac {1} k \|\bm x_k\|^2, X_1=\|\bm  x_1\|^2, k=1,2,\cdots, K\label{EDAlocaldensity18072405}
%\end{align}
%The recursive calculation of $\bm \mu_K$ and $X_K$ allows for “one pass” calculation, thus, ensures computation-efficiency because only the key aggregated/summarized information has to be kept in memory.
%
%Combining \eqref{EDAlocaldensity18072401}-\eqref{EDAlocaldensity18072405}, we can re-formulate the local density, $D$ in a recursive form:
%\begin{align}\label{EDAlocaldensity18072406}
% D_K(\bm x_k) =\frac 1 {1 + \frac{\|\bm x_k-\bm\mu_K\|^2}{X_K-\|\bm \mu_K\|^2}}
%\end{align}
%From \eqref{EDAlocaldensity18072406} we can see that when the Euclidean distance is used, the local density, $D$ behaves as a \textbf{Cauchy function}\index{Cauchy function}, while there is \textbf{no prior assumption about the type of the distribution involved}. Note that $0 < D_K(\bm x_k) \leq 1$ and the closer $\bm x_k$ is to the main local mode, the higher the value of $D_K(\bm x_k)$ is.
%
%3.2. Global density
%
%The global density, $D^G$ estimated at the unique data sample $\bm u_k$ is a weighted sum of its local density by the corresponding occurrence, $f_k (k = 1, 2, \cdots, L_K;L_K > 1)$, expressed as follows \cite{Angelov2016-7844219,Angelov2017Empirical}:
%\begin{align}\label{EDAlocaldensity18072407}
% D^G_K(\bm u_k) = \frac {f_k} {\sum_{j=1}^{L_K} f_j} D_K(\bm u_k) =  \frac {f_k}{1 + \frac{\|\bm u_k-\bm\mu_K\|^2}{X_K-\|\bm \mu_K\|^2}}
%\end{align}
%It is clear that it behaves locally as a Cauchy function, but has multiple local modes/peaks which depend on $f_k$ (if $L_K = K$, global density reduces to the local density but with a smaller amplitude). The largest of the peaks is called the global peak. The global density can directly disclose the data pattern without any further pre-processing. This property can be very useful in real cases where the units of measurement are fixed and data samples measured at different indices are likely to share the same values. The reason we do not stop with $D^G$ but move further with partitioning is that although it is fully automatic and objective, $D^G$ often reveals too many local peaks.
%
%4. The autonomous data partitioning algorithm
%
%In this section, we will describe the proposed ADP algorithm for further refining the local modes of the data set/stream. The local modes are defined as the local maxima of the global data density and are constructed directly from samples. These local modes play a key role in partitioning the data space into shape-free data clouds \cite{Angelov2012Autonomous,Angelov2014Outside} by aggregating data samples around them and forming naturally a Voronoi tessellation \cite{OkabeBoots2000-5677}. We will present two independent algorithms for the two versions, i) 离线方式; ii) 进化方式.
%
%4.1. Offline ADP algorithm
%
%The offline ADP algorithm works with the global density, $D^G$ of the observed data samples. It is based on the ranks of the data samples in terms of their global densities and mutual distributions. Its main procedure is described as follows:
%
%步骤1: \textbf{Rank order the samples with regards to the distance to the global mode}
%
%We start by organizing the unique data samples $\{\bm u\}_K$ into an indexing list, denoted by $\{\bm u^*\}_K$ based on their mutual distances and values of the global density, $D^G$.
%Firstly, the global densities of the unique data samples, $D^G_K(\bm u_i) (i = 1, 2, \cdots, L_K)$ are calculated using \eqref{EDAlocaldensity18072407}. The unique data sample with the highest global density is then selected as the first element of the indexing list $\{\bm u^*\}_{L_K}$:
%\begin{align}\label{EDAlocaldensity18072408}
% \bm u^{*1} = {\arg \max}_{j=1,2,\cdots,L_K} \left(D^G_K(\bm u_j)\right)
%\end{align}
%We set $\bm u^{*1}$ as the first reference point: $\bm u^{*r} \leftarrow \bm u^{*1}$ and remove $ \bm u^{*1}$ from $\{\bm u\}_{L_K}$. Then, by selecting out the unique data
%sample which is nearest to $\bm u^{*r}$, the second element of $\{u^*\}_{L_K}$ denoted by $u^{*2}$ is identified and it is set as the new reference point $\bm u^{*r} \leftarrow \bm u^{*2}$ and, is also removed from $\{\bm u\}_{L_K}$. The process is repeated until $\{\bm u\}_{L_K}$ becomes empty, and the rank ordered list $\{\bm u^*\}_{L_K}$ is finally derived. Based on this list, we can rank the global density of the unique data samples as:
%\begin{align}\label{EDAlocaldensity18072409}
% \left\{D^G_K(\bm u^*)\right\} =\{D^G_K(\bm u^{*1}),D^G_K(\bm u^{*2}),\cdots, D^G_K(\bm u^{*L_K}) )\} .
%\end{align}
%An example using the wine dataset \cite{AeberhardWinedata1992} is shown in Fig. 2. Fig. 2(a) shows the global density of the wine dataset, Fig. \ref{RankedDGlocalMaxima0726}
%shows the ranked global density.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%2018-7-26
%\begin{figure}[H]
%\centering
%  \includegraphics[width=.45\textwidth]{RankedDGlocalMaxima.eps}
%  \caption{The ranked $D^G$ and the identified local maxima.}
%  \label{RankedDGlocalMaxima0726}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[H]
%\centering
%  \includegraphics[width=.45\textwidth]{DGlocalMaxima3D.eps}
%  \caption{The location of local maxima in the data space.}
%  \label{DGlocalMaxima3D0726}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%步骤 2: \textbf{Detecting local maxima (local modes)}
%
%Based on the list $\{\bm u^*\}_{L_K}$ and the ranked global density $\{D^G_K(\bm u^*)\}$, we can identify all the data samples with the local maxima of $D^G$ directly as follows:
%\begin{align}\label{EDAlocaldensity18072410}
%\textup{IF}\,\, \textup{sgn} D^G_K(\bm u^{*j})- &D^G_K(\bm u^{*j+1}) = 1\,  \textup{AND}\, \textup{sgn} D^G_K(\bm u^{*j-1}) - D^G_K(\bm u^{*j}) =-1 \notag\\
%                                             &\textup{THEN}\, \bm u^{*j}\,\textup{is a local maximum of}\, D^G
%\end{align}
%where
%\begin{align*}
%\textup{sgn}(x)
%\left\{
%\begin{array}{ll}
%1,& x > 0\\
%0,& x = 0\\
%−1,& x < 0
%\end{array}
%\right.
%\end{align*}
%is the sign function; we denote the collection of data samples with the local maxima of $D^G$ as $\{u^{**}\}_{L_K^*}= {u^{** j}|j = 1, 2, \cdots, L_K^*} (L_K^* < L_K)$. The local maxima (peaks) identified from the wine dataset \cite{AeberhardWinedata1992} are marked by red circles in Fig. 2(b). The locations of the local maxima in the data space are depicted in Fig. \ref{DGlocalMaxima3D0726}.
%
%步骤 3: \textbf{Forming data clouds}
%
%The local peaks identified from the indexing list, namely, $\{u^{**}\}L_K^*$, are then used to attract the data samples $\{\bm x\}_K$ that are closer to them using a min operator:
%\begin{align}\label{EDAlocaldensity18072411}
%\textup{winning cloud} = {\arg \min}_{ j=1,2,\cdots,L_ K^*} (\|\bm x_i-u^{**j}\|)\, i=1,2,\cdots, K; L_K^*>1
%\end{align}
%
%By assigning all the data samples to the nearest local maxima, a number of Voronoi tessellations \cite{OkabeBoots2000-5677} are naturally formed and data clouds are built around the local maxima \cite{Angelov2012Autonomous,Angelov2014Outside}. More importantly, the process is free from any threshold.
%
%After the data clouds are formed, the actual centre (mean) $\mu_j$ and the standard deviation $\sigma_j ( j = 1, 2, \cdots, L_K^*)$ per data cloud and the support, $S^j$, namely, the number of data samples within the data cloud can be calculated easily. Note that, all this procedure is post factum, i.e. it is determined bottom up from the data without a prior assumption, except the selection of the metric.
%
%步骤 4: \textbf{Filtering local modes}
%
%The data clouds formed in the previous stage may contain some less representative ones; therefore, in this stage, we filter the initial Voronoi tessellations and combine them into larger, more meaningful data clouds.
%The global densities at the data clouds centres $\{\bm\mu\}_{L_K^*}$ are firstly calculated as follows $( j = 1, 2, \cdots, L_K^*)$:
%\begin{align}\label{EDAlocaldensity18072412}
% D^G_K(\bm\mu^j ) = \frac{S^j}{1 +\frac{\|\bm\mu^j-\bm\mu_K\|^2}{X_K-\|\bm \mu_K\|^2} },
%\end{align}
%where $S^j$ is the support of the data cloud.
%
%In order to identify the centres with the local maxima of the global density, we introduce the following three objectively derived quantifiers of the data pattern:
%\begin{align}
%&\eta_K^c = 2\sum^{L^*_K-1}_{ p=1}\sum^{L^*_K}_{q=p+1} \|\bm \mu^p - \bm \mu^q\|  /L^*_K({L^*_K-1}),\label{EDAlocaldensity18072413}\\
%&\gamma_K^c =\frac{\sum\limits_{\bm x\neq \bm y, \|\bm x-\bm y\|\leq \eta_K^c,\bm x,\bm y\in \{\bm\mu\}_{L_K^*}}\|\bm x-\bm y\|}{M_\eta},\label{EDAlocaldensity18072414}\\
%&\lambda_K^c =\frac{\sum\limits_{\bm x\neq \bm y, \|\bm x-\bm y\|\leq \gamma_K^c,\bm x,\bm y\in \{\bm\mu\}_{L_K^*}}\|\bm x-\bm y\|}{M_\gamma},\label{EDAlocaldensity18072415}
%\end{align}
%$\eta_K^c$ is the average Euclidean distance between any pair of existing local modes. $\gamma_K^c$ is the average Euclidean distance between any pair of existing local modes within a distance less than $\eta_K^c$, and $M_\eta$ in \eqref{EDAlocaldensity18072413} is the number of such local mode pairs. $\lambda_K^c$ is the average Euclidean distance between any pair of existing local modes within a distance less than $\gamma_K^c$, $M_\gamma$ in \eqref{EDAlocaldensity18072415} is the number of such local mode pairs.
%
%Note that $\eta_K^c, \gamma_K^c$ and $\lambda_K^c$ are not problem- specific and are parameter-free. The relationship between $\eta_K^c, \gamma_K^c$ and $\lambda_K^c$ is depicted in Fig. 3 using the wine dataset \cite{AeberhardWinedata1992} again, where one can see that $\lambda_K^c$ is a smaller value compared with $\eta_K^c$ and $\gamma_K^c$ which are obtained as the average distance of two very close data cloud centres.
%The quantifier $\lambda_K^c$ can be viewed as the estimation of the distances between the strongly connected data clouds condensing the local information from the whole dataset. Moreover, instead of relying on a fixed threshold, which may frequently fail, $\eta_K^c, \gamma_K^c$ and $\lambda_K^c$ are derived from the dataset objectively and conforming with the concept of a mode. Experience has shown that they provide meaningful tessellations, regardless of the distribution of the data.
%
%Each centre $\bm\mu^j(j = 1, 2, \cdots, L_K^*)$ is compared with the centres of neighbouring data clouds in terms of the global density:
%\begin{align}\label{EDAlocaldensity18072416}
%\textup{IF}\,  D^G_K(\bm u^{j})=\max \left\{ \left\{D^G_K(\bm \mu)\right\}_n^j,D^G_K(\bm \mu^j)\right\}\, \textup{THEN}\, \bm u^{j}\,\textup{is a local maximum},
%\end{align}
%where $\left\{ D^G(\bm u^{*j}) \right\}_n^j$ is the collection of global densities of the neighbouring centres, which satisfy the following condition:
%\begin{align}\label{EDAlocaldensity18072417}
%\textup{IF}\,  \left(\|\bm \mu^i-\bm \mu^j\|\leq \frac{\lambda_K^c}{2}\right)\, \textup{THEN}\, \bm u^{i}\,\textup{is neighbouring }\bm u^{j}.
%\end{align}
%The criterion of neighbouring range is defined in this way because two centres with the distance smaller than $\gamma_K^c$ can be considered to be potentially relevant in the sense of spatial distance; $\lambda_K^c$ is the average distance between the centres of any two potentially relevant data clouds. Therefore, when the condition (16) is satisfied, both $\bm \mu^i$ and $\bm \mu^j$ are highly influencing each other and, the data samples within the two corresponding data clouds are strongly connected. Therefore, the two data clouds are considered as neighbours. This criterion also guarantees that only small-size (less important) data clouds that
%significantly overlap with large-size (more important) ones will be removed during the filtering operation.
%
%After the filtering operation (stage 4), the data cloud centres with local maximum global densities denoted by
%$\{\bm \mu^{*}\}_{L_K^{**}}={\bm \mu^{*j}| j = 1, 2, \cdots, L_K^{**} , L_K^{**} \leq L_K^{*}}$ are obtained. Then, $\{\bm \mu^*\}L_K^{**}$ are used as local modes for forming data clouds in stage 3 and are filtered in 步骤 4.
%
%步骤 3 and 4 are repeated until all the distances between the existing local modes exceed $\frac{\lambda_K^c} 2$. Finally, we obtain the remaining centres with the local maxima of $D^G$, denoted by $\{\bm\mu^o\}$, and use them as the local modes to form data clouds
%using \eqref{EDAlocaldensity18072410}.
%
%After the data clouds are formed, the corresponding centres, standard deviations, supports, members and other parameters of the formed data clouds can be extracted post factum. The final partitioning result of the wine dataset \cite{AeberhardWinedata1992} is depicted in Fig. 4, where the dots in different colours stand for data samples from different data clouds, the black asterisks stand for
%the centres of the data clouds.
%
%4.2. 进化 ADP算法
%
%The proposed evolving ADP algorithm works with the local density, D of the streaming data. This algorithm is able to start “from scratch”, i.e. with a single sample. In addition, a hybrid between the evolving and the offline versions is also possible.
%
%The main procedure of the evolving algorithm is as follows.
%
%Stage 1: Initialization
%
%We select the first data sample within the data stream as the first local mode. The proposed algorithm then starts to self-evolve its structure and update the parameters based on the arriving data samples.
%
%Stage 2: System structure and meta-parameters update
%
%For each newly arrived data sample at the current time instance $k \leftarrow k + 1$, denoted as $\bm x_k$, the global meta-parameters $\bm \mu_k$ and $X_k$ are updated with $\bm x_k$ firstly using \eqref{EDAlocaldensity18072404} and \eqref{EDAlocaldensity18072405}. The local density at $\bm x_k$ and the centres of all the existing data clouds, $D_k(\bm x_k)$ and $D_k(\bm \mu^i _k) (i = 1, 2, \cdots ,C_k)$ are calculated using \eqref{EDAlocaldensity18072406}; here, we use $C_k$ as the number of existing local modes at the $k$th
%time instance.
%
%Then, the following condition \cite{Angelov2012Autonomous} is checked to decide whether $\bm x_k$ will form a new data cloud:
%\begin{align}\label{EDAlocaldensity18072417}
%  \textup{IF}\, D_k(\bm x_k) > &\max_{i=1}^{C_k } (D_k(\bm \mu^i _k))\, \textup{OR}\, D_k(\bm x_k) < \min_{i=1}^ {C_k} (D_k(\bm \mu^i _k))\notag\\
%    &\textup{THEN}\, (\bm x_k\,  \textup{becomes a new f ocal point})
%\end{align}
%If the condition is met, a new data cloud is added with $\bm x_k$ as its local mode $(C_k \leftarrow C_k + 1,\mu_k^{C_k} \leftarrow x_k$ and $S_k^{C_k} \leftarrow 1)$.
%Otherwise, the existing local mode closest to $\bm x_k$ is found, denoted as $\bm \mu^n_k$. Then, the following condition is checked before $\bm x_k$ is assigned to the data cloud formed around $\bm \mu^n_k$:
%\begin{align}\label{EDAlocaldensity18072418}
%  \textup{IF}\, \||\bm x_k - \bm\mu^n_k\| \leq \frac{\eta^k_c} 2, \textup{THEN}\, (\bm x_k\, \textup{is assigned to}\, \bm\mu^n_k)
%\end{align}
%
%However, it is not computationally efficient to calculate $\eta_k^c$ at each time when a new data sample arrives. Since the average distance between all the data samples $\eta_k^d$ is approximately equal to $\eta_k^c$, $\eta_k^c\approx \eta_k^d$, $\eta_k^c$ can be replaced as:
%\begin{align}\label{EDAlocaldensity18072419}
%  \eta_k^c\approx \eta_k^d=\sqrt{\frac{\sum_{i=1}^k\sum_{l=1}^k \|\bm x _i-\bm x _l\|^2 }{k^2}}=\sqrt{2(X_k-\|\bm\mu_k\|^2)}
%\end{align}
%If the condition \eqref{EDAlocaldensity18072418} is satisfied, then $\bm x_k$ is associated with the nearest existing local mode $\bm \mu^n_k$ and the meta-parameters of $\bm \mu^n_k$ are updated as follows:
%\begin{align}
%  &S^n_k \leftarrow S^n_k + 1 \label{EDAlocaldensity18072420}\\
%  &\bm\mu^n_ k \leftarrow \frac{S^n_k  - 1}{S^n_k}\bm \mu^n_k + \frac 1 {S^n_k}\bm x_k\label{EDAlocaldensity18072421}
%\end{align}
%If the condition \eqref{EDAlocaldensity18072418} is not satisfied, then $\bm x_k$ starts a new data cloud: $(C_k \leftarrow C_k + 1,\mu_k^{C_k} \leftarrow \bm x_k$ and $S_k^{C_k} \leftarrow 1)$.
%The local modes and supports of other data clouds that do not get the new data sample stay the same for the next processing cycle. After the update of the system structure and the meta-parameters, the algorithm is ready for the next data sample.
%
%Stage 3: Forming data clouds
%
%When there are no more data samples, the identified local modes (renamed as $\{\bm \mu^o\}$) are used to build data clouds using \eqref{EDAlocaldensity18072411}. The parameters of these data clouds can be extracted post factum.
%The main procedure of the proposed ADP algorithm (evolving version) is presented in the following pseudo code.
%
%In this subsection, a number of benchmark datasets are used in the performance evaluation as tabulated in Table 1. During the experiments, we assume that we do not have any prior knowledge about the benchmark datasets. The following well-known algorithms are used for comparison: \index{Clustering Algorithms}
%\begin{enumerate}[label=\Roman*,align=left]
%  \item MS: Mean-shift clustering algorithm 2002 \cite{DPComaniciu2002-5678};
%  \item SUB: Subtractive clustering algorithm 1994 \cite{Chiu1994-5679};  subclust--Find cluster centers using subtractive clustering-matlab 2006a up
%  \item DBS: DBScan clustering algorithm 1996 \cite{EsterKriegel1996-5680};
%
%  \url{http://yarpiz.com/255/ypml110-dbscan-clustering}
%  \item SOM: Self-organizing map algorithm 1997 \cite{Kohonen1997-5691};
%  \item ELM: Evolving local means clustering algorithm 2012 \cite{RPDutta2012-5681};
%  \item DP: Density peaks clustering algorithm 2014 Science \cite{RodriguezLaio2014-5691};
%  \item NMM: Nonparametric mixture model based clustering algorithm 2006\cite{Blei2006-5683};
%  \item NMI: Nonparametric mode identification based clustering algorithm 2007 \cite{LiRay2007-5690};
%  \item CEDS: Clustering of evolving data streams algorithm 2017 \cite{HydeAngelov2017-5682}.
%\end{enumerate}
%
%Fully online clustering of evolving data streams into arbitrarily shaped clusters, 2017 \cite{HydeAngelov2017-5682} \index{Clusters-Arbitrarily Shaped }
%
%In recent times there has been an increase in data availability in continuous data streams and clustering of this data has many advantages in data analysis. It is often the case that these data streams are not stationary, but evolve over time, and also that the clusters are not regular shapes but form arbitrary shapes in the data space. Previous techniques for clustering such data streams are either hybrid online / offline methods, windowed offline methods, or find only hyper-elliptical clusters. In this paper we present a fully online technique for clustering evolving data streams into arbitrary shaped clusters. It is a two stage technique that is accurate, robust to noise, computationally and memory efficient, with a low time penalty as the number of data dimensions increases. The first stage of the technique produces micro-clusters and the second stage combines these micro-clusters into macro-clusters. Dimensional stability and high speed is achieved through keeping the calculations both simple and minimal using hyper-spherical micro-clusters. By maintaining a graph structure, where the micro-clusters are the nodes and the edges are its pairs with intersecting micro-clusters, we minimise the calculations required for macro-cluster maintenance. The micro-clusters themselves are described in such a way that there is no calculation required for the core and shell regions and no separate definition of outer micro-clusters necessary. We demonstrate the ability of the proposed technique to join and separate macro-clusters as they evolve in a fully online manner. There are no other fully online techniques that the authors are aware of and so we compare the technique with popular online / offline hybrid alternatives for accuracy, purity and speed. The technique is then applied to real atmospheric science data streams and used to discover short term, long term and seasonal drift and their effects on anomaly detection. As well as having favourable computational characteristics, the technique can add analytic value over hyper-elliptical methods by characterising the cluster hyper-shape using Euclidean or fractal shape factors. Because the technique records macro-clusters as graphs, further analytic value accrues from characterising the order, degree, and completeness of the cluster-graphs as they evolve over time.
%
%
%Rodriguez, Alex and Laio, Alessandro, \cite{RodriguezLaio2014-5691}, Clustering by fast search and find of density peaks, Science 2014 \index{Science}\index{Local Density Maxima}\index{Local Density Maxima}
%
%Cluster analysis is used in many disciplines to group objects according to a defined measure of distance. Numerous algorithms exist, some based on the analysis of the local density of data points, and others on predefined probability distributions. Rodriguez and Laio devised a method in which the cluster centers are recognized as local density maxima that are far away from any points of higher density. The algorithm depends only on the relative densities rather than their absolute values. The authors tested the method on a series of data sets, and its performance compared favorably to that of established techniques. Cluster analysis is aimed at classifying elements into categories on the basis of their similarity. Its applications range from astronomy to bioinformatics, bibliometrics, and pattern recognition. We propose an approach based on the idea that cluster centers are characterized by a higher density than their neighbors and by a relatively large distance from points with higher densities. This idea forms the basis of a clustering procedure in which the number of clusters arises intuitively, outliers are automatically spotted and excluded from the analysis, and clusters are recognized regardless of their shape and of the dimensionality of the space in which they are embedded. We demonstrate the power of the algorithm on several test cases.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table*}[!ht]
%\caption{Details of the benchmark datasets for evaluation.}
%\begin{center}
% \begin{tabular}{lp{8cm}lcccccccc}
%\hline
%%\multirow{2}{*}{Data set}&\multirow{2}{*}{Algorithm}&\multicolumn{2}{c}{Training (RMSE)}&\multicolumn{2}{c}{Testing (RMSE)}\\
%%\cline{3-4}\cline{5-6}
%Abbreviation& Dataset& $N_A$b &$N_S$c &$N_C$d \\
%\hline
%PI& PIMA \cite{SmithEverhart1988-5692} &8& 768& 2\\
%BA& Banknote Authentication \cite{LohwegHoffmann2013-5693}& 4 &1372 &2\\
%S1& S1 \cite{FrantiVirmajoki2006-5694}& 2 &5000 &15\\
%S2& S2 \cite{FrantiVirmajoki2006-5694}&2 &5000 &15\\
%CA& Cardiotocography \cite{AyreseCamposBernardes2000-5695}& 22& 2126& 3\\
%PB& Pen-Based Handwritten Digits Recognition \cite{AlimogluAlpaydin1996-5696}& 16 &10,992& 10\\
%ST& Steel Plates Faults \cite{Buscema1998-5698}& 27 &1941 &7\\
%MU& Multiple Features \cite{ARJain2000-5703} &649 &2000& 10\\
%OD& Occupancy Detection \cite{CandanedoFeldheim2016-5699}a& 5 &20,560& 2\\
%MA& MAGIC Gamma Telescope \cite{BockChilingarian2004-5701}& 10 &19,020& 2\\
%LE& Letter Recognition \cite{FreySlate1991-5702} &16 &20,000 &26\\
%\hline
%\end{tabular}
%\end{center}
%\label{TT2-ELM170610:Sec5-3-1}
%\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%a The time stamps in the original dataset have been removed.
%b Number of attributes.
%c Number of samples.
%d Number of classes.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{模糊计算}
美国加州大学扎德(Zadeh)教授于1965年提出了模糊集合与模糊逻辑理论是模糊计算的数学基础.
它主要用来处理现实世界中因模糊而引起的不确定性.
目前, 模糊理论已经在推理、控制、决策等方面得到了非常广泛的应用. 本节主要讨论模糊理论的基础.

20世纪70年代中期, 中国开始引进模糊数学.
80年代在理论研究和应用研究方面都非常活跃并取得了大量研究成果, 主要代表人物有著名数学家汪培庄、张文修、蒲保明、刘应明、王国俊等教授.
1988年, 汪培庄教授及其指导的几名博士生研制成功一台模糊推理机——分立元件样机, 它的推理速度为1500万次/秒; 表明中国在突破模糊信息处理难关方面迈出了重要的一步, 是一项了不起的研究成果.
2005年, 中国科学院院士刘应明教授(1940.10.8-2016.7.15, 福建福州人, 数学家, 中国科学院院士.
主要从事拓扑学与不确定性(主要是模糊性)数学处理等方面的教学与科学研究. 被誉为“中国的查德") 获国际模糊系统协会 (IFSA) 授予 ``Fuzzy Fellow" 称号, 他是作为首位非发达国家学者获此殊荣的.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \vspace{-0.4cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{WPZ20191218122443.jpg}
    \caption{汪培庄教授}
    \label{WPZ20191218122443}
    \vspace{-0.4cm}
\end{figure}
\vspace{-0.8cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{Liuyingmingyuanshi.JPG}
    \caption{刘应明院士}
    \label{WPZ20191218122443}
    \vspace{-0.4cm}
\end{figure}
1983年国际模糊系统协会筹备会在法国马xi举行, 参加会议的我国学者有, 刘应明(四川大学), 汪培庄(北师大), 王震源(河北大学), 邓聚龙(华中科大)和张文修共五人. 图为在会议组织委员会主席Sanchez家中合影.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{IFSA1983.JPG}
    \caption{1983年国际模糊系统协会筹备会在法国马xi举行, 参加会议的我国学者有, 刘应明(四川大学), 汪培庄(北师大), 王震源(河北大学), 邓聚㔫(华中科大)和张文修(西交大)}
    \label{WPZ20191218122443}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
    中国著名学者周海中教授曾指出：“模糊数学的诞生, 是科学技术发展的必然结果, 更是现代数学发展的必然产物. 但就目前现状而言, 模糊数学的理论尚未成熟、体系还未形成, 对它也还存在不同看法和意见; 这些都有待日后完善和实践检验. ”
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊集}{1}
设$U$是给定论域, $\mu_F$是把任意$u\in U$映射为$[0, 1]$上某个实值的函数, 即
\begin{align}
    U\rightarrow [0, 1], u\rightarrow \mu_F(u),
\end{align}
则称 $\mu_F(x)$为定义在$U$上的一个隶属函数, 由   (对所有) 所构成的集合$F$称为$U$上的一个模糊集, 称为$u$对$F$的隶属度.
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}模糊集的二点注记

\ding{172} 模糊集$F$完全是由隶属函数$\mu_{F}$来刻画的, 把$U$中的每一个元素$u$都映射为$[0, 1]$上的一个值$\mu_F(x)$ .

\ding{173} \,$\mu_F(u)$的值表示$u$隶属于$F$的程度, 其值越大, 表示$u$隶属于$F$的程度越高. 当$\mu_F(u)$仅取0和1时, 模糊集$F$便退化为一个普通集合.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{模糊集及其运算}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}\label{AIC5Fuzzyexam551}
    设论域$U=\{20, 30, 40, 50, 60\}$给出的是年龄, 请确定一个刻画模糊概念“年轻”的模糊集$F$.
\end{example}

解: 由于模糊集是用其隶属函数来刻画的, 因此需要先求出描述模糊概念“青年”的隶属函数. 假设对论域$U$中的元素, 其隶属函数值分别为:
\begin{align}
    \mu_{F}(20)=1, \mu_{F}(30)=0.8, \mu_{F}(40)=0.4, \mu_{F}(50)=0.1, \mu_{F}(60)=0,
\end{align}
则可得到刻画模糊概念“年轻”的模糊集
\begin{align}
    F=\{ 1, 0.8, 0.4, 0.1, 0\}.
\end{align}

\textbf{模糊集的含义}:

(1) 离散且为有限论域的表示方法

设论域 $U=\{u_1, u_2, \ldots, u_n\}$为离散论域, 则其模糊集可表示为:
\begin{align}
    F=\left\{\mu_{F}\left(u_{1}\right), \mu_{F}\left(u_{2}\right), \ldots, \mu_{F}\left(u_{n}\right)\right\}.
\end{align}
为了能够表示出论域中的元素与其隶属度之间的对应关系, 扎德引入了一种模糊集的表示方式: 先为论域中的每个元素都标上其隶属度, 然后再用“+”号把它们连接起来, 即
\begin{align}
    F=\mu_{F}\left(u_{1}\right) / u_{1}+\mu_{F}\left(u_{2}\right) / u_{2}+\cdots+\mu_{F}\left(u_{n}\right) / u_{n};
\end{align}
也可写成
\begin{align}
    F=\sum_{i=1}^{n} \mu_{F}\left(u_{i}\right) / u,
\end{align}
其中, $\mu_{F}\left(u_{i}\right)$为$u_i$对$F$的隶属度; “$\mu_{F}\left(u_{i}\right) / u_{i}$”不是相除关系, 只是一个记号; “+”也不是算术意义上的加, 只是一个连接符号.
\begin{align}
    \begin{array}{ll}
        F=\left\{\mu_{F}\left(u_{1}\right) / u_{1}, \mu_{F}\left(u_{2}\right) / u_{2}, \cdots, \mu_{F}\left(u_{n}\right) / u_{n}\right\},\\
        F=\left\{\left(\mu_{F}\left(u_{1}\right), u_{1}\right),\left(\mu_{F}\left(u_{2}\right), u_{2}\right), \cdots,\left(\mu_{F}\left(u_{n}\right), u_{n}\right)\right\}.
    \end{array}
\end{align}
在这种表示方法中, 当某个$u_i$对$F$的隶属度等于$0$时, 可省略不写.
\begin{example}
    前面例 \ref{AIC5Fuzzyexam551} 的模糊集$F$可表示为:
    \begin{align*}
        F=1 / 20+0.8 / 30+0.6 / 40+0.2 / 50.
    \end{align*}
    有时, 模糊集也可写成如下两种形式:
    \begin{align*}
        \begin{array}{l}
            F=\left\{\mu_{F}\left(u_{1}\right) / u_{1}, \mu_{F}\left(u_{2}\right) / u_{2}, \cdots, \mu_{F}\left(u_{n}\right) / u_{n}\right\},\\
            F=\left\{\left(\mu_{F}\left(u_{1}\right), u_{1}\right),\left(\mu_{F}\left(u_{2}\right), u_{2}\right), \cdots,\left(\mu_{F}\left(u_{n}\right), u_{n}\right)\right\},
        \end{array}
    \end{align*}
其中, 前一种称为单点形式, 后一种称为序偶形式. \index{序偶形式}
\end{example}

(2) 连续论域的表示方法

如果论域是连续的, 则其模糊集可用一个实函数来表示.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
扎德以年龄为论域, 取$U=[0, 100]$, 给出了“年轻”与“年老”这两的模糊概念的隶属函数
\begin{align*}
\begin{array}{ll}
{\mu_{\textup{老}}(u)=\left\{\begin{array}{ll}{0,} & { 0 \leq u \leq 50} \\
{\left[1+\left(\frac{5}{u-50}\right)^{2}\right]^{-1},} & {50<u \leq 100}\end{array}\right.} \\
 {\mu_{\textup{年轻}}(u)=\left\{\begin{array}{ll}{1,} & {0 \leq u \leq 25} \\
 {\left[1+\left(\frac{u-25}{5}\right)^{2}\right]^{-1},} & { 25<4 \leq 100}\end{array}\right.}
 \end{array}
\end{align*}
\end{example}

(3) 一般表示方法

不管论域$U$是有限的还是无限的, 是连续的还是离散的, 扎德又给出了一种类似于积分的一般表示形式:
\begin{align}
    F=\int_{u \in U} \mu_{F}(u) / u.
\end{align}

\begin{remark}
    这里的记号不是数学中的积分符号, 也不是求和, 只是表示论域中各元素与其隶属度对应关系的总括.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊集相等}{1}
    设$F,G$分别是$U$上的两个模糊集, 对任意$u\in U$, 都有$\mu_{F}(u)=\mu_{G}(u)$ 成立, 则称$F$等于$G$, 记为$F=G$.
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊集包含}{1}
    设$F,G$分别是$U$上的两个模糊集, 对任意$u\in U$, 都有$\mu_{F}(u)\leq \mu_{G}(u)$ 成立, 则称$F$等于$G$, 记为$F\subseteq G$.
\end{mydef}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊集并集交集}{1}
设$F,G$分别是$U$上的两个模糊集, 则$F\cup G$、$F\cap  G$分别称为$F$与$G$的并集、交集, 它们的隶属函数分别为:
 \begin{align}
   \begin{array}{l}
   F \cup G: \mu_{F \cup G}(u)=\max _{u \in U}\left\{\mu_{F}(u), \mu_{G}(u)\right\},\\
   F \cap G: \mu_{F \cap G}(u)=\min _{u \in \mathbb{U}}\left\{\mu_{F}(u), \mu_{G}(u)\right\}.
   \end{array}
 \end{align}
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊集补集}{1}
设$F$为$U$上的模糊集, 称$\neg F$为$F$的补集, 其隶属函数为:
\vspace{-0.1cm}
 \begin{align}
    \neg F: \mu_{\neg F}(u)=1-\mu_{F}(u).
 \end{align}
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
设$U=\{1,2,3\}$, $F$和$G$分别是$U$上的两个模糊集, 即
 \begin{align*}
     F&=\{\textup{小}\}=1/1+0.6/2+0.1/3,\\
     G&=\{\textup{大}\}=0.1/1+0.6/2+1/3.
 \end{align*}
则
 \begin{align*}
     F\cup G&=(1\vee 0.1)/1+(0.6\vee 0.6)/2+(0.1\vee  1)=1/1+0.6/1+1/1,\\
     F\wedge G&=(1\wedge 0.1)/1+(0.6\wedge 0.6)/2+(0.1\wedge 1)=0.1/1+0.6/1+0.1/1,\\
     F&=(1-1)/1+(1-0.6)/2+(1-0.1)=0.4/2+0.9/3.
 \end{align*}
  \vspace{-0.6cm}
\end{example}
从这个例子可以看出, 两个模糊集之间的运算实际上就是逐点对隶属函数作相应的运算.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{模糊关系及其运算}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{模糊关系的定义}
    设$V$与$W$是两个普通集合, $V$与$W$的笛卡尔乘积为
 \begin{align}
        \mathbf{V} \times \mathbf{W}=\{(\mathbf{v}, \mathbf{w}) | v \in V, w \in W\}.
 \end{align}
所谓从$V$到$W$的关系$R$, 是指$V\times W$上的一个子集, 即
 \begin{align}
     \mathbf{R} \subseteq \mathbf{V} \times \mathbf{W}.
 \end{align}
记为
 \begin{align}
        V \stackrel{R}{\longrightarrow} W.
 \end{align}
对于$V\times W$中的元素$(v,w)$, 若$(v,w)\in\mathbb R$, 则称$v$与$w$有关系$R$;
若$(v,w)\notin\mathbb R$, 则称$v$与$w$没有关系.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
设$V=\{1\textup{班}, 2\textup{班}, 3\textup{班}\}$, $W=\{\textup{男队},\textup{女队}\}$,
则$V\times W$中有6个元素, 即
 \begin{align*}
    V\times W =\{(1\textup{班}, \textup{男队}), (2\textup{班}, \textup{男队}), (3\textup{班}, \textup{男队}), (1\textup{班}, \textup{女队}), (2\textup{班}, \textup{女队}), (3\textup{班}, \textup{女队})\},
    \vspace{-0.3cm}
 \end{align*}
 其中, 每个元素是一代表队. 假设要进行一种双方对垒的循环赛, 则每一个赛局都是$V\times W$中的一个子集, 它构成了$V\times W$上的一个关系.
\vspace{-0.4cm}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊集}{1}
设$F_i$是$U_i,\,(i=1,2,\cdots,n)$上的模糊集, 则称
 \begin{align}
   F_{1} \times F_{2} \times \cdots \times F_{n}=
    \int_{u_{1} \times u_{2} \times \cdots \times u_{n}}\left(\mu_{F_{1}}(u_{1}) \wedge \mu_{F_{2}}\left(u_{2}\right)
    \wedge \cdots \wedge \mu_{F_{n}}\left(u_{n}\right)\right) /\left(u_{1}, u_{2}, \cdots, u_{n}\right).
 \end{align}
为$F_1,F_2,\cdots,F_n$的笛卡尔乘积, 它是$U=U_1\times U_2\times\cdots\times U_n$上的一个模糊集.
\end{mydef}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊关系}{1}
    在$U=\prod_{i=1}^n U_i$上的一个$n$元模糊关系$R$是指以$\prod_{i=1}^n U_i$上为论域的一个模糊集, 记为
\begin{align}
    R=\int_{U_1\times U_2\times\cdots\times U_n} \mu_{R}\left(u_{1}, u_{2}, \cdots, u_{n}\right) /\left(u_{1}, u_{2}, \cdots, u_{n}\right).
\end{align}
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
设有一组学生$U=\{u_1,u_2\}=$\{秦学, 郝玩\}, 计算机上的活动$V={v_1,v_2,v_3}=$\{编程, 上网, 玩游戏\},
并设每个学生对各种活动的爱好程度分别为 $\mu_{F}\left(u_{i}, v_{j}\right),\, i=1,2; j=1,2,3$, 即
\begin{align*}
  \mu_{R}(u_i,v_j)=
  \mu_{R}
  \left\{
  \begin{array}{llll}
    (u_1,v_1),&(u_1,v_2),&(u_1,v_3)\\
    (u_2,v_1),&(u_2,v_2),&(u_2,v_3),
  \end{array}
  \right.
\end{align*}
则$U\times V$上的模糊关系R为
\begin{align*}
    R=\left[\begin{array}{ccc}
    {0.9} & {0.6} & {0} \\
    {0.2} & {0.3} & {0.8}
    \end{array}\right].
\end{align*}
\vspace{-0.4cm}
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{模糊关系的合成}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊关系}{1}
    设$R_1$与$R_2$分别是$U\times V$与$V\times W$上的两个模糊关系, 则$R_1$与$R_2$的合成是从$U$到$W$的一个模糊关系, 记为   $R_1\circ R_2$. 其隶属函数为
\begin{align}
  \mu_{R_{1}\circ R_{2}}(u, w)=\vee\left\{\mu_{R_{1}}(u, v) \wedge \mu_{R_{2}}(v, w)\right\},
\end{align}
其中, $\wedge$和$\vee$分别表示取最小和取最大.
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
设有以下两个模糊关系
\begin{align*}
    R_{1}=\left[\begin{array}{ccc}{0.4} & {0.5} & {0.6} \\
    {0.8} & {0.3} & {0.7}
    \end{array}\right],
    R_{2}=\left[\begin{array}{cc}{0.7} & {0.9} \\
    {0.2} & {0.8} \\
    {0.5} & {0.3}
    \end{array}\right].
\end{align*}
则$R_1$与$R_2$的合成是
\begin{align}
    R=R_{1} \circ R_{2}=\left[
    \begin{array}{ll}
    {0.5} & {0.5} \\
    {0.7} & {0.8}
    \end{array}\right].
\end{align}
其方法是把$R_1$的第$i$行元素分别与$R_2$的第$j$列的对应元素相比较, 两个数中取最小者, 然后再在所得的一组最小数中取最大的一个, 并以此数作为$R_1\circ R_2$的元素$R(i,j)$.
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mydef}{模糊变换}{1}
    设$F=\{\mu_F(u_1),\,\mu_F(u_2),\,\cdots,\, \mu_F(u_n)\}$, 是论域$U$上的模糊集, $R$是$U\times V$上的模糊关系, 则$F\circ R=G$称为模糊变换.
\end{mydef}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}
设$F=(1, 0.6, 0.2)$
\begin{align}
  R=\left[
  \begin{array}{cccc}
  {1} & {0.5} & {0} & {0} \\
  {0.5} & {1} & {0.5} & {0} \\
  {0} & {0.5} & {1} & {0.5}
  \end{array}
  \right].
\end{align}
则
\begin{align*}
  G=F\circ R&=\{(1\wedge 1)\vee (0.6\wedge 0.5)\vee (0.2\wedge 0), (1\wedge 0.5)\vee (0.6\wedge 1)\vee (0.2\wedge 0.5),\\
            &\qquad\quad  (1\wedge 0)\vee (0.6\wedge 0.5)\vee (0.2\wedge 1), (1\wedge 0)\vee (0.6\wedge 0)\vee (0.2\wedge 0.5)\}\\
            & =\{1, 0.6, 0.5, 0.2\}.
\end{align*}
\vspace{-0.55cm}
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{FNN}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{DFNN}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{神经模糊系统和模糊神经系统}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{参考书}

1. 神经网络的综合基础, 第二版,  国际知名大学原版教材——信息科学学科与电气工程学科系列,
Simon Haykin (S. Haykin 是一位物理、数学、信号处理和通信的科学家, 编著的著作有50多本), 2001, 清华大学出版社/培生教育出版集团, 1-836.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{作      业}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
    用遗传算法求$f(x)=x\sin(10πx)+1.0$的最大值, 其中$x\in [-1,2]$.
\end{custom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
    用遗传算法及其变种求表 \ref{tab:test2} 和 \ref{Tab:CEC05} 的最值.
\end{custom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
设有论域$U=\{u_1, u_2, u_3, u_4, u_5\}$, 并设$F,\, G$是$U$上的两个模糊集, 且有
\begin{align*}
    F&=0.9/u_1+0.7/u_2+0.5/u_3+0.3/u_4\\
    G&=0.6/u_3+0.8/u_4+1/u_5
\end{align*}
请分别计算 $F\cap G, F\cup G,\neg F$.
\end{custom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
    试用M-P模型构造一个实现逻辑``OR''运算的神经元, 给出模型结构和各个 $w_{i}$和 $\theta $值.
\end{custom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
    试证明感知机学习算法收敛定理.
\end{custom}
%%%%%%---------------------------------
\begin{think}
    描述支持向量机的基本思想, 给出伪代码.
\end{think}

%%%%%%---------------------------------
\begin{think}
    试解释强化学习模型及与其他机器学习方法的异同.
\end{think}
%%%%%%---------------------------------
\begin{think}
    什么是Q学习? 它的基本原理是什么?
\end{think}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
\begin{enumerate}
\item 试将感知机学习算法用C语言编成程序, 并做下述的 $n$维随机矢量$\mbox{X}=[x_{1} ,x_{2} ,\cdots ,x_{n} ]^{T}$的二值分类的模拟实验：
	\begin{enumerate}
	\item 用程序产生M个均值为0, 方差为1的正态随机矢量(取维数$n=3$, 即 $\mbox{X}(k)=[x_{1} (k),x_{2} (k),x_{3} (k)]^{T}$, $k=1,2,\cdots, M$;
每个$x_{i} (k)$为服从$N(0,1)$分布的随机变量).
要求产生三组矢量 (分别取M$=$10,20,30), 分别用每组矢量训练一个感知机模型. 对于每个训练矢量 $\mbox{X}(k)$, 给定其理想输出为
$d(k)=
\left\{
\begin{array}{*{20}c}
 1\;,\;x_{2}(k)\ge 0 \\
 0\;,\;x_{2}(k)<0  \\
\end{array}
\right.$
在每组训练收敛后, 再产生30个新矢量, 用来检验所得到的感知机的分类性能.
对每一组结果要给出收敛时所用的迭代次数$K_{0}$, 收敛时的权矢量值$\mbox{W}(K_{0})$, 和检验时所达到的正确分类率$R$.
	\item 取维数$n=5$, 做与(1)相同的模拟实验 (仍产生$M=10,20,30$的三组正态随机矢量, 给出相应的结果.
在本题中, 对于每个5维训练矢量$\mbox{X}(k)$, 其理想输出$d(k)$为 $d(k)=
\left\{
\begin{array}{*{20}c}
 1\;,\;\;x_{3}(k)\ge 0\\
 0\;,\;\;x_{3}(k)<0\\
\end{array} \right.$.
	\end{enumerate}
\item[\textbullet] \textbf{提示 }: 产生正态随机数 $X\sim N(0,1)$的方法:
	\begin{enumerate}
	\item 先产生在$(0, 1)$区间均匀分布的两个随机数 $U_{1}$ 和 $U_{2}$;
	\item 令 $X_{1} =(-2\ln U_{1} )^{1/2}\sin (2\pi \,U_{2} )$, $X_{2} =(-2\ln U_{1} )^{1/2}\cos (2\pi \,U_{2})$.
	\end{enumerate}
\end{enumerate}
则$X_{1} $ 和 $X_{2} $ 均为服从$N(0,1)$分布的正态随机数.
\end{custom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
设有如下两个模糊关系:
\begin{align*}
R_{1}=\left[
\begin{array}{ccc}{0.3} & {0.7} & {0.2} \\ {1} & {0} & {0.4} \\ {0} & {0.5} & {1}\end{array}\right],
R_{2}=\left[\begin{array}{cc}{0.2} & {0.8} \\ {0.6} & {0.4} \\ {0.9} & {0.1}\end{array}\right]
\end{align*}请写出$R_1$与$R_2$的合成$R_1\circ R_2$.
\end{custom}
%%%%%%---------------------------------
\begin{think}
    说明遗传算法的构成要素, 给出遗传算法流程图.
\end{think}

%%%%%%---------------------------------
\begin{think}
    简述蚁群算法的原理, 用蚁群算法求解旅行商问题.
\end{think}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%教材(Comprehensive Foundation of Neural Networks) p.152, the Problem 3.4 is as follows:
%The correlation matrix $R_{X} $ of the input vector $\mbox{X(n)}$ in the LMS
%algorithm is given by $R_{X} =\left[ {{\begin{array}{*{20}c}
% 1 \hfill & {0.5} \hfill \\
% {0.5} \hfill & 1 \hfill \\
%\end{array} }} \right]$. Define the range of values for the learning-rate
%parameter $\alpha$ of the LMS algorithm for it to be convergent in the mean
%square.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
\begin{enumerate}
\item 在BP学习算法中, 若各单元均取为 Sigmoid函数, 则网络输出值必处于(0,1)之间. 为使输出值不限于(0,1)之间, 可选输出单元为线性函数(即$\phi(s)=s$), 其它单元仍取为 Sigmoid函数. 试导出此时的BP学习算法公式.
	\begin{enumerate}
	\item (1) 已知用一个2层的NN可实现逻辑XOR运算. 试构造一个实现XOR运算的NN, 给出模型结构和各个 $w_{ji} $ 和 $\theta_{j}$值. (注: 在该NN中, 输入X $=[x_{1} ,\;x_{2} ]^{T}$中各$x_{i} $取值为$+$1或-1, 各单元的非线性函数取为符号函数, 即$\phi(s)=\textup{Sgn}(s)$).
	\end{enumerate}
\end{enumerate}
(2) 奇偶检验问题可视为XOR问题的推广(由2输入到$n$输入的推广): 若$n$个输入中有奇数个1, 则输出为1；若$n$个输入中有偶数个1, 则输出为0. 一个2层的NN可实现奇偶检验运算. 试构造一个实现这种运算的NN, 给出模型结构和各个
$w_{ji} $ 和 $\theta_{j}$ 值. (注: 在该NN中, $n$为任意整数, 输入$X=[x_{1} , x_{2} ,\cdots, x_{n} ]^{T}$中各$x_{i}$取值为1或0, 各单元的非线性函数取为单位阶跃函数, 即$\phi(s)=U(s)$).
\end{custom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
\begin{enumerate}
\item 试用C语言编程实现多层前向NN的BP算法. 要求：输入、输出结点数目, 隐层数目, 及各隐层中结点的数目应为任意整数.
\item 试用所编出的BP算法程序训练出一个实现XOR运算的2层前向网络.
\item 用所编出的BP算法程序训练出输入矢量的维数分别为$n=7$和$n=8$的两个实现奇偶检验运算的2层前向NN.
\item[\textbullet] \textbf{注： }要求: 列表给出训练收敛后的NN权值和所用的迭代次数;
\item 给出训练收敛后的训练误差和检验误差, 及用训练集和检验集做输入时所得到的正确输出率;
	\begin{enumerate}
	   \item 给出NN的学习曲线(即$E(W(k))$随迭代次数k的变化曲线, 该结果应是用计算程序计算和打印出来的曲线, 要是用手画出的曲线).
	\end{enumerate}
\end{enumerate}
\end{custom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{custom}[explorecolor]{探索}
基于剪枝技术的一字棋博弈系统

    1. 实验目的

    理解和掌握博弈树的启发式搜索过程, 能够用某种程序语言建立一个简单的博弈系统.

    2. 实验环境: 微机上的编程语言(任选).

    3. 实验要求

    (1) 规定棋盘大小为5行5列, 要求自行设计估价函数, 按极大极小搜索方法, 并采用$\alpha-\beta$剪枝技术.

    (2) 采用人机对弈方式, 一方走完一步后, 等待对方走步, 对弈过程每一时刻的棋局都在屏幕上显示出来.

    (3) 提交完整的软件系统和相关文档, 包括源程序和可执行程序.
\end{custom}

